[["setup.html", "Statistics in R: An Introduction for Phoneticians 1 Setup 1.1 Installation 1.2 R Projects 1.3 Packages and R Version 1.4 Sessions 1.5 Types of Documents 1.6 Help 1.7 Acknowledgements", " Statistics in R: An Introduction for Phoneticians Johanna Cronenberg 2025-12-10 1 Setup 1.1 Installation Download and install the Software R. The latest version is currently 4.5.x (December 2025). Also download and install RStudio. Watch this short introduction to RStudio. Create a directory on your hard drive to be used for this course. 1.2 R Projects This course will make use of an R project which will make working with R easier in the long run. Open RStudio and click on the (very small) triangle in the top right hand corner labelled Project: (None). Then click on New Project and then Existing Directory and use Browse to select the directory that you just created on your hard drive. Finish by clicking on Create Project. RStudio will then automatically open this project that has the same name as your course directory (see the triangle on the top right). The project can be closed via this triangle and also re-opened. The project can also be opened by clicking on the file that has been created in your course directory with the extension .Rproj. If you have any difficulty with project installation watch this introductory video. More information on R projects R projects have many advantages especially if R is being used in several courses and there is a separate R project for each course. The project remembers which files were displayed and makes these available when the project is opened again – so that you can carry on working where you left off. Moreover, the working directory of the project is your course directory. As a result, all files that you put into this directory are accessible via the Files toolbar on the bottom right. You can also check what your course directory is by entering getwd() into the console and hitting enter. 1.3 Packages and R Version There are many thousands of packages i.e. libraries which make working in R easier. Please install the following (takes a few minutes): install.packages(c(&quot;Rcpp&quot;, &quot;remotes&quot;, &quot;knitr&quot;, &quot;tidyverse&quot;, &quot;magrittr&quot;, &quot;rmarkdown&quot;, &quot;emuR&quot;, &quot;gridExtra&quot;, &quot;emmeans&quot;, &quot;broom&quot;, &quot;lmerTest&quot;, &quot;pbkrtest&quot;, &quot;MuMIn&quot;)) More information: Installation of R packages Should the above result in the error message installation of package had non-zero exit status, then it means that the installation failed. For Windows, you might then additionally have to install Rtools. For MacOS you might need to install/reset the XCode command-line tools. For this purpose, open a Mac terminal window and execute the following: xcode-select --install # Should the R packages installation still not work: xcode-select --reset If you are unsure about how to solve problems during the installation of R packages, please ask me! While some base packages are automatically loaded when you start up RStudio, most have to be loaded using the library() function, thus: library(tidyverse) Further Information: Updates Please check regularly whether your packages need to be updated – updates in R are not automatic! For this purpose, click in the main toolbar on Tools &gt; Check for Package Updates. You can also check whether there are any updates to RStudio via Help &gt; Check for Updates. R also has to be regularly updated. Check what version you have with getRversion(). Check the R Webpage regularly to see whether a newer, stable version is available. 1.4 Sessions A session starts when either R or RStudio or a project is opened. A session is terminated with Session &gt; Quit Session in the main toolbar or with Strg+Q or Ctrl+Q or q(). The session also ends automatically when you quit RStudio. You will then be asked whether you want to save the workspace image. Should you want to keep the objects that you have created in any one session so that they are available again in the next session, click save. The workspace is then saved in your course directory with the extension .RData. If you don’t want to save the workspace, click Don't save. If you decide you don’t want to end the session after all, click Cancel. For this course, please never save the workspace i.e. Don't save. 1.5 Types of Documents 1.5.1 R Scripts The console in RStudio is the direct connection to R i.e. that’s where R code can be executed. But storing R code in a document is important in order to be able to replicate previously executed code. You could do this with an R script which can be created with File &gt; New File &gt; R Script (or via Strg + Shift + N) and should always be saved with a .R extension. An R script only contains executable code. When an R script is run, all lines are executed except those that begin with # that are commented out. There are various ways of executing an R script. Select the required lines of code (or if it’s just one line of code, put the cursor on the line) and then click Run or enter Strg + enter or Ctrl + enter. The output from running the command is then immediately visible in the console. 1.5.2 R Markdown In the last few years R Markdown has become another popular way for creating documents for R code that can be especially useful for creating reports and teaching material. R Markdown is a document containing snippets of R code that can then be executed in the way described above. R Markdown documents often have more text than code. You can create an R Markdown document with File &gt; New File &gt; R Markdown in the main toolbar. It’s conventional to save the document with the extension .Rmd. An R markdown file is usually converted or knitted into a different format such as HTML or PDF or even Word. This is done via the button with a ball of wool marked Knit or via: library(rmarkdown) render(&quot;document.Rmd&quot;) R Markdown was also used to create the HTML document that you are currently reading. Special symbols are used in the markdown document that are converted as follows when knitted: # Heading: A hashtag gives the largest heading size; the more hashtags used, the smaller the size of the heading. **bold**: A double asterisk before and after the text marks it as bold. *italics*: A single asterisk italicises text. `code`: The backslash quotes highlight the text: this is often used when marking code or variables outside of a code snippet (but note that any such code cannot be executed). ```: Three backslash quotes in sequence mark the beginning and the end of a code snippet (also known as a code block). All R code must be entered between the sets of three backslash quotes; any text inside a snippet has to be marked with a # in order for it to be identified as a comment. Curly brackets identify the language in which the code is written (for our purposes: {r}). More information is available in the cheatsheet for R Markdown (esp. page 2, left column). 1.6 Help 1.6.1 Introduction to Programming in R If you’re not familiar with programming in R, I suggest you work through my tutorial Programming in R: An Introduction for Phoneticians! 1.6.2 Recognizing Errors Warnings: if you make a syntax error (e.g. by forgetting a bracket), you will see a red warning sign at the beginning of the line of code that contains the error. Don’t ignore this because it shows that a mistake has been made. The warning sign disappears once the error is corrected. “Knit”: We recommend that you regularly convert your Markdown document to HTML by clicking Knit on the toolbar at the top. If all goes well, you will hopefully see the compiled HTML in a new window or in the viewer. However, if you have syntax errors or other errors in your code, the HTML will not be generated and you will instead get an error in the console where you can also see in which line the error has occurred. Execute code one line (or in blocks of a couple of lines) at a time. That way you can see the result and decide whether this is what you intended. 1.6.3 Ask the Community There is a very large and helpful R community that will make learning R easier for you. Here are a few useful links and commands in case you get stuck: Stack Overflow: A blog where you can often find an answer to your questions about R. The easiest way is to google your question in English; a stack overflow member’s answer will be included in the first search results. Hadley Wickham’s “R for Data Science”: Hadley Wickham is the chief programmer of the “tidyverse”. His books are very readable, well structured, and entertaining. Cheatsheets: those are PDFs that provide an overview of functions with explanations and examples in a very compact form. You will find some cheatsheets in the main toolbar via Help &gt; Cheatsheets. The first three are especially useful. You can also find cheatsheets by googling, see e.g. Data Transformation with dplyr or this very comprehensive Reference Card. Vignettes: For some packages there are so-called “vignettes”, which are mostly HTMLs or PDFs that have been written by the package’s authors. You can search for vignettes using the following input to the console: # for e.g. one of the libraries from tidyverse vignette(&quot;dplyr&quot;) You can get information about a function by entering its name in the RStudio Help tab on the bottom right. You’ll then get information about the function’s arguments and often some examples. You can also get help via the console, as follows (e.g. here for getwd()): ?getwd help(&quot;getwd&quot;) 1.6.4 Help with ggplot2 ggplot2 is both well-known and popular and there’s plenty of help available for ggplot2 from the R community. Here are some useful links for creating graphics: The chapter Data Visualisation in Hadley Wickham’s “R for Data Science” Cookbook for R Cheatsheet ggplot2 Stack Overflow 1.6.5 Statistics in R: Literature If you need more information about using statistics in R, the following are recommended: Bodo Winter’s “Statistics for Linguists: An Introduction using R”: A recent book with excellent explanations to all major themes in inferential statistics. It’s available online via the LMU library. Stefan Gries’ “Statistics for Linguistics with R: A Practical Introduction”: Useful for decision making about which model to use for which type of question. Although the code is not always up-to-date (given that the book was published in 2009) the statistical content is still valid. Also online via the LMU library. Harald Baayen’s “Analyzing Linguistic Data: A Practical Introduction to Statistics”: This is for more advanced readers. Although the R code is a bit out of date, the explanations and examples of statistics foundations are very helpful. Available as a hard copy from the LMU library. 1.7 Acknowledgements This teaching material is based on original materials by Jonathan Harrington and Ulrich Reubold who were my stats teachers. I am especially grateful to Prof. Jonathan Harrington who trusted me to teach programming and statistics and has always supported me unconditionally. My material benefited greatly from me reading Bodo Winter’s “Statistics for Linguists: An Introduction using R” – a truly fantastic book about stats which showed me that it is possible to teach stats in an entertaining and non-overwhelming way. I would also like to thank all students who have attended my classes and brought them to life through their active participation. "],["introduction-to-inferential-statistics.html", "2 Introduction to Inferential Statistics 2.1 Load Packages and Data 2.2 Basic Terminology 2.3 Normal Distribution", " 2 Introduction to Inferential Statistics 2.1 Load Packages and Data Open the R project which you created for this tutorial. To that end, open RStudio and use the button on the top right or navigate to your course directory and double click on the .Rproj file. Then load the tidyverse and the following data frame: library(tidyverse) ## ── Attaching core tidyverse packages ────────────────── ## ✔ dplyr 1.1.4 ✔ readr 2.1.6 ## ✔ forcats 1.0.1 ✔ stringr 1.6.0 ## ✔ ggplot2 4.0.1 ✔ tibble 3.3.0 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.2.0 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; df &lt;- read.table(file.path(url, &quot;vdata.txt&quot;)) %&gt;% as_tibble() %&gt;% rename(vowel = V, tension = Tense, consonant = Cons, rate = Rate, subject = Subj) %&gt;% mutate(duration = log(dur)) %&gt;% select(-c(X, Y)) 2.2 Basic Terminology Statistical analysis […] is a strikingly subjective process – Bodo Winter In statistics, a population is the set of all units (i.e., people, words, etc.) that meet certain identification criteria (e.g., gender, origin, grammatical function, etc.). For example, imagine you want to determine the average fundamental frequency (f0) of all women in Germany. Then your population is the set of all women in Germany, which is approximately 40 million people. In this example, you could only determine the population mean \\(\\mu\\) (pronounced as /myː/, often written mu) by traveling to every woman in Germany and measuring her fundamental frequency, which is, of course, impossible for many reasons. Instead, in science, one usually collects samples, e.g. a subset of the female population, and assumes that the resulting sample mean \\(m\\) is not too far from the true population mean \\(\\mu\\). The larger the sample, the closer its mean \\(m\\) and standard deviation \\(s\\) will approximate the true population mean \\(\\mu\\) and the population standard deviation \\(\\sigma\\) (sigma). Greek symbols are typically used for the characteristics of a population, while Roman letters are used for the characteristics of an empirically collected distribution (i.e., a sample). The methods of inferential statistics allow us to draw conclusions about the population from the sample. More precisely, inferential statistics helps us to estimate the parameters of the population. There are also measures, such as the standard error, that describe how good or bad the estimate is. Before a statistical test is conducted, it must be clear what is being tested: the so-called null hypothesis (H0). We operate according to the principle of falsification, i.e., we try to refute H0 with a statistical test. In addition, the alternative hypothesis (H1) is often mentioned, which is the exact opposite of the null hypothesis. Here is an example: We have conducted an experiment because we want to investigate whether the duration of a vowel is influenced by the vowel’s tension. The collected data is stored in the data frame df. We then formulate the following hypotheses: H0: The vowel duration is not influenced by tension. H1: The vowel duration is influenced by tension. We then perform a statistical test to falsify H0. But be careful: If we have falsified H0, that does not mean that we have verified H1! In addition, before conducting our statistical test, we must define the so-called significance level, also known as the \\(\\alpha\\)-level (alpha level). This is the probability value at which we reject our null hypothesis or consider it falsified. In science, there are three (arbitrarily defined!) \\(\\alpha\\)-levels: 0.05, 0.01, and 0.001. If the result of our statistical test falls below the defined significance level, we consider H0 refuted and describe the result as “significant”. In the case of the statistical tests presented here, the \\(p\\)-value is calculated, which provides information about the significance level. With a \\(p\\)-value of \\(p\\) &lt; 0.05, the test result is statistically significant. 2.3 Normal Distribution Above, we pointed out the difference between a population and a sample. Similarly, a distinction is also made between theoretical and empirical distributions. Theoretical distributions often have established names (normal distribution, Poisson distribution, Student’s \\(t\\)-distribution, etc.), and it is assumed that measurements taken for an entire population follow a specific theoretical distribution. However, as we have noted, we can almost never measure an entire population and instead use a sample. The measurements from a sample represent an empirical distribution because they were collected empirically. We will frequently need to test which theoretical distribution the observed empirical distribution most closely resembles. In many empirical experiments, the data follows a normal distribution (also called a Gaussian distribution). This distribution can be completely described by two parameters: its mean and standard deviation. Here you see three different normal distributions: black: mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\) blue: mean \\(\\mu = 3\\) and standard deviation \\(\\sigma = 2\\) green: mean \\(\\mu = -2\\) and standard deviation \\(\\sigma = 1.5\\) As you can see, the mean shifts the normal distribution along the x-axis, while the standard deviation leads to changes in the width of the distribution: the larger the standard deviation, the wider the normal distribution. Furthermore, the plot shows that the distribution is continuous, meaning it covers the range of values from minus to plus infinity. 2.3.1 Testing for Normal Distribution The duration of vowels was recorded in the data frame df. We want to test whether the empirical distribution of the duration corresponds to the normal distribution. First, here is the empirically measured duration in a probability density distribution: ggplot(df) + aes(x = duration) + geom_density() + xlab(&quot;Log. Duration&quot;) + ylab(&quot;Probability Density&quot;) + xlim(3.0, 7.0) Further information: Taking the logarithm In the figure, you see the vowel duration not in milliseconds, but in logarithmic form, i.e., we have applied the natural logarithm to the data (see code snippet where df was loaded). For certain measurements, it can be useful to logarithmically transform the data, namely when the empirical distribution is skewed. With reaction times, for example, it is not possible to have values below zero; very low values (below 100ms) are also very unlikely, but very long reaction times do occur. The same goes for duration values. The actual duration values in milliseconds look like this: ggplot(df) + aes(x = dur) + geom_density() + xlab(&quot;Duration (ms)&quot;) + ylab(&quot;Probability Density&quot;) This distribution has a strong right-hand side skew, precisely because long durations occur while very short durations are rare. 2.3.1.1 Superimposing the Normal Distribution In order to test whether data is normally distributed, visual methods are more popular amongst statisticians than statistical tests. The first method is to superimpose a normal distribution with the same mean and standard deviation as the empirical distribution onto the latter. We can use the function dnorm() for this, which receives the arguments mean and sd. In ggplot2, we cannot simply add “foreign” functions such as dnorm() to our ggplot() code. Instead, we use the function stat_function() from the ggplot2 package. This function takes the following arguments: fun: the function with which a new curve shall be created (here: the curve of a normal distribution). args: a list of arguments belonging to the function given to the argument fun. In our case, the function dnorm() needs the arguments mean and sd. ggplot(df) + aes(x = duration) + geom_density() + xlim(3.0, 7.0) + xlab(&quot;Log. Duration&quot;) + ylab(&quot;Probability Density&quot;) + stat_function(fun = dnorm, args = list(mean = mean(df$duration), sd = sd(df$duration)), color = &quot;blue&quot;) There are small differences between the blue normal distribution and the black empirical one. Our data may not be perfectly normally distributed, but at least approximately so. 2.3.1.2 Q-Q Plot In addition to superimposing a normal distribution onto the empirical distribution, so-called Q-Q plots are frequently used, where Q stands for quantile. Watch the following YouTube video to understand how this plot is computed. And don’t get confused: Although the video shows “sample quantiles” on the y-axis, these are simply the data points ordered in ascending order! ggplot(df) + aes(sample = duration) + stat_qq() + stat_qq_line() + ylab(&quot;samples&quot;) + xlab(&quot;theoretical quantiles&quot;) In ggplot2, this plot can be created using stat_qq(). Additionally, we use stat_qq_line() to plot a straight line that serves as a reference. If the points deviate from the line, the data are not normally distributed (although slight deviations at the upper and lower ends of the line are quite common). In this case, a slight deviation from the normal distribution is also visible. 2.3.1.3 Practice Your Interpretation Initially, it can be difficult to determine from one or two plots whether the plotted data are normally distributed. Therefore, we show four examples of clearly non-normally distributed data so that you can hone your ability to recognize what Q-Q plots and overlaid probability distributions should not look like if your data are normally distributed. Below, you will see four probability distributions: for bimodal data (there are two peaks), for left- and right-skewed data, and for uniformly distributed data (where each value theoretically occurs with equal frequency). Here are the same probability distributions with the respective parametrically fitted normal distribution (i.e., for each normal distribution, the mean and standard deviation corresponding to the depicted distribution were used): Finally, we also create Q-Q plots for all four distributions, which deviate significantly from the straight line: 2.3.1.4 Shapiro-Wilk Test Finally, we take a look at the Shapiro-Wilk test because it is the most frequently used test for normal distributions. In R, the function shapiro.test() conducts that test and takes the data as a vector as its only argument: shapiro.test(df$duration) ## ## Shapiro-Wilk normality test ## ## data: df$duration ## W = 0.99, p-value = 2e-11 The null hypothesis of this test is that the data are indeed normally distributed. If the \\(p\\)-value is below the generally accepted significance level of \\(\\alpha = 0.05\\), we must reject this hypothesis and therefore assume that the data are not normally distributed. According to this test, our log-transformed duration values are therefore not normally distributed. For the Shapiro-Wilk test, your dataset should consist of fewer than 5000 observations and should not contain too many outliers or identical values, as this can strongly influence the validity of the result. Despite this test result, based on the plots created earlier, I would assume that our data are approximately normally distributed. Further information: Scientific notation of numbers Very large and very small numbers in R are often represented using scientific notation, e.g., 1e+02 instead of 100. “e” here stands for base 10, and the numbers that follow are the exponent: 1e+02 == 1 * 10^2 ## [1] TRUE 1e-02 == 1 * 10^-2 ## [1] TRUE With numbers like 100 or 0.01, you can still easily convert scientific notation in your head. But of course, R can do this for you, using the format() function, which takes the argument scientific = FALSE: format(1e+02, scientific = F) ## [1] &quot;100&quot; format(1e-02, scientific = F) ## [1] &quot;0.01&quot; # and for the p-value from our Shapiro-Wilk test above: format(2.062e-11, scientific = F) ## [1] &quot;0.00000000002062&quot; Attention! This function returns character objects, so if you want to work with the converted number you must use as.numeric() to convert the result of format() back into a number: class(format(1e+02, scientific = F)) ## [1] &quot;character&quot; as.numeric(format(1e+02, scientific = F)) ## [1] 100 2.3.2 68–95–99.7 Rule &amp; Confidence Intervals 2.3.2.1 Theoretical For normal distributions (and, approximately, also for quasi-normally distributed data), the so-called 68–95–99.7 rule applies. We illustrate this with a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\): The total area under the normal distribution is 1, meaning we can use the area to determine the probability that data points fall within a given range of values. With a normal distribution, 68% of the data falls within the blue area, 95% within the blue + red area, and 99.7% within the entire shaded area. As you can see from the x-axis of this figure, this corresponds to \\(\\mu\\pm1\\sigma\\) (blue), \\(\\mu\\pm2\\sigma\\) (blue + red), and \\(\\mu\\pm3\\sigma\\) (blue + red + yellow). The 68–95–99.7 rule is therefore a mnemonic to help remember that with (approximately) normally distributed data, 68% of the data points lie within a range of \\(\\mu\\pm1\\sigma\\), and so on. We can calculate the area under the normal distribution using pnorm(). This function takes an x-value and the mean and standard deviation that describe the normal distribution (the default is zero and one, so I’m omitting these two arguments here). Then, pnorm() calculates the area from negative infinity up to the specified x-value. If we enter a very high x-value, it should become clear that the area under the normal distribution is indeed 1: pnorm(100) ## [1] 1 For x = 0 (i.e. our mean), the area should be 0.5: pnorm(0) ## [1] 0.5 This means that 50% of all data points fall within the range from negative infinity to zero. Now we can also calculate the areas highlighted above: # blue areas: pnorm(0) - pnorm(-1) ## [1] 0.3413 pnorm(1) - pnorm(0) ## [1] 0.3413 # read areas: pnorm(-1) - pnorm(-2) ## [1] 0.1359 pnorm(2) - pnorm(1) ## [1] 0.1359 # yellow areas: pnorm(-2) - pnorm(-3) ## [1] 0.0214 pnorm(3) - pnorm(2) ## [1] 0.0214 In statistics, we are often interested in whether data falls within a confidence interval, usually referred to as the 95% confidence interval. This means that for normally distributed data, we check whether a data point falls within the range \\(\\mu \\pm 2\\sigma\\). To determine the precise range of values, we use the qnorm() function, which again takes the mean and standard deviation as arguments, and additionally the desired area under the distribution. If we want a total area of 0.95 symmetrically around the mean, the area at the left and right edges of the distribution is \\((1 - 0.95) / 2 = 0.025\\). For our normal distribution above, we can then calculate: qnorm(0.025) ## [1] -1.96 qnorm(1-0.025) ## [1] 1.96 This means that for a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\), an area of 0.95 (i.e., 95% of the data points) lies between -1.96 and 1.96. Above, we stated that the area of 0.95 corresponds to \\(\\mu \\pm2\\sigma\\) – more precisely, we should have written \\(\\mu \\pm 1.96\\sigma\\). 2.3.2.2 Empirical Let’s assume we’ve determined that the duration values in df are normally distributed. Using our knowledge of the 68–95–99.7 rule and confidence intervals, we can, for example, find out the probability that a new measurement point from the same experiment falls within a certain range of values. The following figures show the normal distribution with mean mean(df$duration) and standard deviation sd(df$duration), and not the empirical data distribution. mean_dur &lt;- mean(df$duration) sd_dur &lt;- sd(df$duration) So, if we repeat the same experiment from which df was derived, what is the probability that a new data point would lie in a range of values below 4.5 (logarithmic duration)? This is equivalent to asking how large the blue area is, because the total area under the normal distribution is always equal to 1. For this, we use the function pnorm(): pnorm(4.5, mean = mean_dur, sd = sd_dur) ## [1] 0.07422 The probability that a new data point falls within a value range from negative infinity to 4.5 is therefore 7.4%. Now the same for the following blue area: So we ask ourselves how large this area is, or rather, with what probability a new data point falls into the range above 5.1. However, we must consider two things when calculating this: First, pnorm() always calculates the area between negative infinity and a value; second, the area under the normal distribution is equal to 1. Therefore: 1 - pnorm(5.1, mean = mean_dur, sd = sd_dur) ## [1] 0.2536 The area is therefore 0.254, i.e. the probability is 25.4%. If we are now interested in the probability that the values in the experiment fall within the range of 4.9 to 5.5, we need to manipulate pnorm() as follows: pnorm(5.5, mean = mean_dur, sd = sd_dur) - pnorm(4.9, mean = mean_dur, sd = sd_dur) ## [1] 0.4965 So the probability that a new data point falls within the queried range of values is 49.7%. Now, we want to determine the 95% confidence interval for our distribution of duration values, i.e., the range of values in which 95% of the data lie. This simply means that we are trying to calculate the range of values for which the area under the normal distribution is 0.95. Since the area should be symmetrically distributed around the mean, the boundary region on each side must be calculated for which the area is \\((1 - 0.95) / 2 = 0.025\\). To this end, we once again use qnorm(): qnorm(0.025, mean = mean_dur, sd = sd_dur) ## [1] 4.353 qnorm(0.975, mean = mean_dur, sd = sd_dur) ## [1] 5.469 This means that a new value from the experiment will fall within the range of 4.35 to 5.47 with a probability of 95%, or that 95% of the data from the current experiment lie within this range (assuming that the data are normally distributed). count &lt;- df %&gt;% filter(duration &gt; 4.35 &amp; duration &lt; 5.47) %&gt;% nrow() count ## [1] 2877 count / nrow(df) ## [1] 0.9648 In fact, 96.5% of the data lies within the calculated range. This again illustrates the difference between theoretical and empirical distribution: We verified that our empirical data is approximately normally distributed and then created a normal distribution using the mean and standard deviation of the data. It is a theoretical calculation that 95% of the data lies within the range of 4.35 to 5.47 (and that 7.4% of the data lies below 4.5, etc.), because this is based on the theoretical normal distribution. If we look at the actual proportions of the values within the ranges and the confidence interval, they may differ from the theoretically calculated proportions. "],["simple-linear-regression.html", "3 Simple Linear Regression 3.1 Load Packages and Data 3.2 Introduction 3.3 Correlation 3.4 The Regression Line 3.5 Linear Regression with lm() 3.6 Residuals 3.7 Testing Assumptions 3.8 Understanding all Results of lm() 3.9 Reporting the Result 3.10 Summary", " 3 Simple Linear Regression 3.1 Load Packages and Data Load the following packages and data frame: library(broom) library(tidyverse) url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; queen &lt;- read.table(file.path(url, &quot;queen.txt&quot;)) %&gt;% as_tibble() %&gt;% rename(age = Alter) 3.2 Introduction So far, we have used descriptive statistics to examine specific measurements (variables) in more detail and learned about empirical and theoretical distributions. However, such variables are often dependent on other variables. For example, many studies have shown that our reaction time decreases with increasing sleep deprivation. This means that the variable reaction time depends on the variable sleep deprivation. Therefore, we also speak of dependent and independent variables. These dependencies can be described using simple linear regression. In other words, the value of the dependent variable \\(y\\) is predicted by the independent variable \\(x\\). Before we perform a linear regression, we will discuss regression lines and correlation. 3.3 Correlation Correlation, also known as Pearson’s correlation \\(r\\), is a measure of the association between two variables and can be calculated using the cor() function. Here, we will be working with the queen data frame. queen %&gt;% head() ## # A tibble: 6 × 6 ## age f0 F1 F2 F3 F4 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 297. 566. 1873. 2895. 4091. ## 2 28 283. 526. 1846. 2930. 4089. ## 3 29 260. 518. 1785. 2880. 4065. ## 4 30 258. 521. 1786. 2804. 4103. ## 5 31 262. 533. 1819. 2952. 4097. ## 6 32 260. 545. 1694. 2772. 4056. This data frame holds the average fundamental frequency values of Queen Elizabeth II during her annual Christmas addresses. We are interested in whether the Queen’s age has had an influence on her fundamental frequency. First, let’s get an overview of the situation. It is important that, when plotting data where we suspect a correlation, we always place the independent variable (here: age) on the x-axis and the dependent variable (here: f0) on the y-axis. ggplot(queen) + aes(x = age, y = f0) + geom_point() It looks like there might be a connection: the older the Queen got, the more her fundamental frequency decreased! We can verify our visual impression using the correlation \\(r\\): cor(queen$age, queen$f0) ## [1] -0.8346 The correlation \\(r\\) takes values exclusively between -1 and 1. The closer the value is to zero, the weaker the relationship between the two variables. A strong negative correlation of -0.84 indicates a strong negative correlation, meaning our visual impression appears to be correct. 3.4 The Regression Line 3.4.1 Theoretical Information The regression line of simple linear regression can be described by the following formula: \\(y = k + bx\\) Here, \\(k\\) is the y-intercept and \\(b\\) is the slope. Because the intercept and slope unambiguously describe a regression line, these two parameters are also called regression coefficients. Using the formula above, if the intercept \\(k\\) and the slope \\(b\\) are known, the corresponding \\(y\\) values can be predicted for all possible \\(x\\) values. The regression line is always an infinite, perfectly straight line and also passes through the mean of the distribution. In the following figure, you see three regression lines: blue and green have the same intercept but opposite slopes; blue and orange have different intercepts but the same slope. The exact value of the slope indicates by how much the \\(y\\) value increases or decreases when \\(x\\) is increased by one unit. For \\(x = 0\\) in the figure, \\(y = 1\\) (for blue and green). For \\(x = 1\\), \\(y = 1 + b\\), so for blue \\(y = 1 + 0.5 = 1.5\\) and for green \\(y = 1 + (-0.5) = 0.5\\). For the orange line, when \\(x = 0\\), \\(y = 2\\), and when \\(x = 1\\), \\(y = 2 + 0.5 = 2.5\\). ggplot() + xlim(-0.5, 1) + ylim(-0.5, 3) + xlab(&quot;x&quot;) + ylab(&quot;y&quot;) + geom_abline(slope = 0.5, intercept = 1, color = &quot;blue&quot;, linewidth = 1.2) + geom_abline(slope = -0.5, intercept = 1, color = &quot;darkgreen&quot;, linewidth = 1.2) + geom_abline(slope = 0.5, intercept = 2, color = &quot;orange&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) In summary, the blue and orange lines describe a positive correlation between \\(x\\) and \\(y\\) (the larger \\(x\\), the larger \\(y\\)), while the green line describes a negative correlation (the larger \\(x\\), the smaller \\(y\\)). Important: Correlation is not causation! Linear regression can only describe the correlation between two variables, not causality. We introduce causality through our knowledge. For example, we know that sleep deprivation causes a slower reaction time. Linear regression can only show whether a relationship exists between reaction time and sleep deprivation, but from a regression perspective, it could just as easily mean that a slower reaction time causes sleep deprivation. 3.4.2 Regression Lines with ggplot2 To fit a regression line through a ggplot2 figure, we can use either geom_abline() (see above) or geom_smooth(). The first function takes the arguments slope and intercept, as you can see in the image above. The function geom_smooth(), on the other hand, takes the argument method = \"lm\". “lm” stands for linear model, meaning that the function calculates slope and intercept for us, assuming that the data are in a linear relationship. We also specify the argument se = F because we don’t want confidence intervals displayed here. This is what the regression line looks like in the case of the Queen: ggplot(queen) + aes(x = age, y = f0) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;blue&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The difference between geom_abline() and geom_smooth() is that geom_abline() draws a theoretically infinitely long, straight line (but of course we only see a portion of it), while geom_smooth() is limited by the range of values of the data. geom_smooth() can also draw other types of regression lines. 3.5 Linear Regression with lm() Now we are ready to perform a linear regression using the lm() function. This function takes only a formula and the data frame as arguments. The formula is y ~ x, meaning we want to predict the \\(y\\) values (the fundamental frequency) as a function of the \\(x\\) values (the age). Linear regression estimates the intercept and slope so that a regression line can be fitted through the data points that has the smallest possible distance to all points (this method is also called least squares; more on that below). queen.lm &lt;- lm(f0 ~ age, data = queen) queen.lm ## ## Call: ## lm(formula = f0 ~ age, data = queen) ## ## Coefficients: ## (Intercept) age ## 288.19 -1.07 The coefficients can be displayed on their own using coef(): queen.lm %&gt;% coef() ## (Intercept) age ## 288.186 -1.074 So we see that the estimated intercept is 288.2 and the slope is -1.07. Confusingly, the slope is always referred to by the same name as the \\(x\\) variable, in this case, “age”. The coefficients mean the following: At an age of zero years (\\(x = 0\\)), the mean fundamental frequency is approximately 288 Hz, assuming a perfect linear relationship between age and the Queen’s fundamental frequency. With each additional year (\\(x\\) increases by 1), the fundamental frequency decreases by 1.07 Hz. By substituting the intercept and slope into our previous formula, we can now predict the corresponding fundamental frequency for all possible ages: x &lt;- c(0, 40, 50) f0_fitted &lt;- coef(queen.lm)[1] + coef(queen.lm)[2] * x f0_fitted ## [1] 288.2 245.2 234.5 As mentioned, the Queen’s estimated fundamental frequency at birth was 288 Hz. At age 40, it was likely only 245 Hz, and at age 50, 234.5 Hz. As you can see, our “fitted” model can also predict or estimate \\(y\\) values that were not included in the original dataset (such as the f0 value for the 50-year-old Queen). However, all these points lie precisely on the regression line, and since the regression line is infinitely long, the estimate does not necessarily make sense for all values. For example, do you consider it likely that the Queen’s fundamental frequency was 288 Hz at birth? Children typically have a fundamental frequency of 300 to 400 Hz. You must always be aware of whether the estimates are meaningful or not when working with your data. In R, you can perform the estimations using the predict() function, which takes as arguments the model queen.lm and a data frame containing the \\(x\\) values for which \\(y\\) is to be estimated. The \\(x\\) variable must have the same name as in the original data frame, in this case, “age”. predict(queen.lm, data.frame(age = seq(0, 100, by = 10))) ## 1 2 3 4 5 6 7 8 9 ## 288.2 277.4 266.7 256.0 245.2 234.5 223.8 213.0 202.3 ## 10 11 ## 191.6 180.8 3.6 Residuals Above we see the estimated \\(y\\) value for \\(x = 40\\), which is the fundamental frequency for the Queen at 40 years old, namely approximately 245 Hz. However, the actual measured value is much lower: queen %&gt;% filter(age == 40) %&gt;% pull(f0) ## [1] 228.7 The differences between the estimated and measured \\(y\\) values are called residuals. The following figure shows a section of the previous plot for the age range between 30 and 40 years. The black dots are the actually measured f0 values, while the red dots are the estimated values. They lie exactly on the blue regression line. The vertical dashed lines represent the residuals. This figure illustrates why residuals are also referred to as errors. The difference between the actual and estimated values is calculated as the sum of the squared residuals and is therefore also called the sum of squares of error (SSE). The method used to estimate the parameters of the regression line is called least squares because it attempts to keep the SSE as small as possible. This results in the regression line being positioned through the data so that all data points are as close to the line as possible. The residuals can be output using resid(): queen.lm %&gt;% resid() ## 1 2 3 4 5 6 ## 36.273 25.151 2.743 1.576 6.633 5.814 ## 7 8 9 10 11 12 ## -3.825 5.283 -7.605 -12.949 -22.552 -8.751 ## 13 14 15 16 17 18 ## -16.571 -6.042 -7.350 -11.662 -7.231 -10.822 ## 19 20 21 22 23 24 ## -1.267 -9.038 6.623 6.383 17.037 10.016 ## 25 26 27 28 29 30 ## 1.064 -9.792 11.148 2.725 -6.990 3.978 SSE can be calculated with deviance(): queen.lm %&gt;% deviance() ## [1] 4412 3.7 Testing Assumptions Statistical models such as the linear regression are based on assumptions about the data that must be met for the model’s result to be meaningful. In the case of linear regression, these assumptions relate to the residuals. 3.7.1 Normal Distribution of Residuals The first assumption is that the residuals are normally distributed. We check this with a Q-Q plot: ggplot(augment(queen.lm)) + aes(sample = .resid) + stat_qq() + stat_qq_line() + ylab(&quot;samples&quot;) + xlab(&quot;theoretical quantiles&quot;) That looks okay, but not perfect. In addition, let’s look at the probability distribution with a superimposed normal distribution: ggplot(augment(queen.lm)) + aes(x = .resid) + geom_density() + xlim(-40, 40) + xlab(&quot;residuals&quot;) + stat_function(fun = dnorm, args = list(mean = mean(augment(queen.lm)$.resid), sd = sd(augment(queen.lm)$.resid)), inherit.aes = F, color = &quot;blue&quot;) If we are still undecided, we can perform a Shapiro-Wilk test: shapiro.test(augment(queen.lm)$.resid) ## ## Shapiro-Wilk normality test ## ## data: augment(queen.lm)$.resid ## W = 0.94, p-value = 0.1 Since the \\(p\\)-value is higher than \\(\\alpha = 0.05\\), the residuals of the model seem to be approximately normally distributed. 3.7.2 Constant Variance of the Residuals The second assumption states that the variance of the residuals should be similar for all estimated values. This assumption is also known as homoscedasticity (try saying that three times fast). If the assumption is not met, we speak of heteroscedasticity. To visually represent the variance, we plot the residuals against the estimated values. Since the mean of the residuals is always approximately zero (dashed line in the figure at \\(y = 0\\)), we can recognize the constant variance by the fact that the points are evenly distributed around this mean. ggplot(augment(queen.lm)) + aes(x = .fitted, y = .resid) + geom_point() + xlab(&quot;estimated f0 values&quot;) + ylab(&quot;residuals&quot;) + geom_hline(yintercept = 0, lty = &quot;dashed&quot;) Since we have very few data points here, it’s difficult to assess whether there’s a recognizable pattern in the plot that would indicate that the errors don’t have constant variance. The two outliers in the upper right are certainly not a good sign; but the rest looks okay. For now, we’ll assume that the residuals are homoscedastic. To develop your intuition for what constitutes good and bad residual plots, I recommend Figures 6.2 and 6.3 in Winter (2020, pp. 111f.). 3.8 Understanding all Results of lm() The details mentioned in this section for calculating the various values are extremely rare in statistics books, statistics blogs, R vignettes, or other information sources. You don’t need to memorize these details; the point is that you can understand the results of lm() – and that involves more than just the \\(p\\)-value. 3.8.1 Estimated \\(y\\)-Values and Residuals As you may have noticed, I used the augment() function to utilize the results of the linear model in ggplot2. This function comes from the package broom, which we loaded at the beginning and which also provides two other helpful functions: tidy() and glance(). The result of these functions is always a tibble, an object that we can easily process further (unlike the strange regression objects): queen.lm %&gt;% class ## [1] &quot;lm&quot; queen.lm %&gt;% augment() %&gt;% class() ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Let’s first take a look at the results of augment(): queen.lm %&gt;% augment() ## # A tibble: 30 × 8 ## f0 age .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 297. 26 260. 36.3 0.0946 10.5 0.482 ## 2 283. 28 258. 25.2 0.0845 11.7 0.202 ## 3 260. 29 257. 2.74 0.0798 12.8 0.00225 ## 4 258. 30 256. 1.58 0.0753 12.8 0.000694 ## 5 262. 31 255. 6.63 0.0710 12.7 0.0115 ## 6 260. 32 254. 5.81 0.0670 12.7 0.00826 ## 7 249. 33 253. -3.82 0.0632 12.8 0.00334 ## 8 257. 34 252. 5.28 0.0596 12.7 0.00597 ## 9 243. 35 251. -7.60 0.0563 12.7 0.0116 ## 10 236. 37 248. -12.9 0.0503 12.5 0.0297 ## # ℹ 20 more rows ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; This function appends further columns to the original data (columns f0 and age), namely the fitted (i.e., the model-estimated) f0 values .fitted, the residuals .resid, and others that are not of interest to us at this time. 3.8.2 Regression Coefficients and \\(t\\)-Statistic The tidy() function returns a table of the estimated regression coefficients: queen.lm %&gt;% tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 288. 6.98 41.3 1.23e-26 ## 2 age -1.07 0.134 -8.02 9.93e- 9 The estimate column contains the estimates for intercept (first row) and slope (second row). The std.error column contains the so-called Standard Error, a measure of the accuracy of the estimate. Here, we want the smallest possible values (relative to the estimate) because this means that the model’s estimates for the regression coefficients are precise. A column with the test statistic, statistic, follows. So far, we haven’t discussed the statistical significance of the regression. Here, a \\(t\\)-test is performed to determine whether the estimated regression coefficients differ significantly from zero. If the regression coefficients are close to zero, they contribute nothing to predicting the \\(y\\) value (recall the formula for the regression line: if \\(k\\) or \\(b\\) are equal to zero, they have no effect on the regression line). The value in the statistic column, which in this case is also called the \\(t\\)-value, is calculated as estimate / std.error, e.g. for the slope: as.numeric(tidy(queen.lm)[2, &quot;estimate&quot;] / tidy(queen.lm)[2, &quot;std.error&quot;]) ## [1] -8.016 The \\(t\\)-statistic has its own probability density distribution, called the Student’s \\(t\\) distribution or simply the \\(t\\) distribution, which we can plot using the dt() function in ggplot2 (analogous to the dnorm() function for normal distributions). This function takes an argument called df, which stands for degrees of freedom. The degrees of freedom are usually the sample size minus the number of coefficients, so here nrow(queen) - 2. ggplot() + xlim(-5, 5) + stat_function(fun = dnorm) + stat_function(fun = dt, args = list(df = 28), col = &quot;orange&quot;) + stat_function(fun = dt, args = list(df = 5), col = &quot;darkgreen&quot;) + xlab(&quot;&quot;) + ylab(&quot;density&quot;) Here you see the \\(t\\) distribution in orange compared to the black normal distribution with a mean of zero and a standard deviation of one; the two distributions are very similar. With decreasing degrees of freedom (for example, 5 degrees of freedom for the dark green distribution), the normal and \\(t\\) distributions become less similar. As you know, the area under these distributions is always 1, meaning that even with the \\(t\\) distribution, we can use a function to calculate the probability of a value falling within a specific range. The \\(t\\)-value for the slope in our example is approximately -8.02. Under the orange \\(t\\) distribution, which matches our data, there is only a very, very small area under the curve for the range of values from negative infinity to -8.02 (you’ll need a bit of imagination here, as the range in the figure above only starts at -5). Following the example of pnorm(), the function for calculating the area under the \\(t\\) distribution is called pt(). Using the \\(t\\)-value and the degrees of freedom, we can calculate the \\(p\\)-value: 2 * pt(-8.016248, df = 28) ## [1] 9.929e-09 Further Information: two-tailed t-test We need to multiply the probability value calculated by pt() by 2 because the calculated \\(t\\)-test is not a one-tailed but a two-tailed \\(t\\)-test. The extreme ends of the distribution are called the tail; for the normal and \\(t\\) distributions, very high and very low values are unlikely (i.e., there is very little area under the distribution from negative infinity to a very low x-value or from a very high x-value to positive infinity). When we calculate a probability (or area) here with pt(), it only applies to the lower tail from negative infinity to the specified x-value. However, the same probability applies to the area from the positive x-value (here: abs(-8.016248)) to positive infinity (the upper tail). Therefore, we simplify things by multiplying the result above by 2. We could also have written: pt(-8.016248, df = 28) + pt(abs(-8.016248), df = 28, lower.tail = F) ## [1] 9.929e-09 You have seen something similar when calculating the 95% confidence interval for normal distributions: Our aim was to divide the area of 0.05 equally between both symmetrical halves of the distribution, i.e., we considered both tails, and not just one of them. The \\(p\\)-value can be interpreted as follows: If we assume that the actual slope is zero (this is the null hypothesis of this \\(t\\)-test), then the observed slope of -1.07 is highly unexpected. 3.8.3 Quality Criteria for the Model and \\(F\\)-Statistic Now that we understand the output of tidy(), only glance() remains to be discovered. The glance() function displays at a glance a few criteria that can be used to assess the quality of the model (also called goodness-of-fit; for better readability, we convert the data frame to the long format): queen.lm %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 6.97e-1 ## 2 adj.r.squared 6.86e-1 ## 3 sigma 1.26e+1 ## 4 statistic 6.43e+1 ## 5 p.value 9.93e-9 ## 6 df 1 e+0 ## 7 logLik -1.17e+2 ## 8 AIC 2.41e+2 ## 9 BIC 2.45e+2 ## 10 deviance 4.41e+3 ## 11 df.residual 2.8 e+1 ## 12 nobs 3 e+1 The value r.squared is exactly what the name suggests: the squared correlation value \\(r\\), which we calculated above using cor(): cor(queen$age, queen$f0)^2 ## [1] 0.6965 The \\(R^2\\) value describes the proportion of the variance in the data that is described by the fitted model. In this case, approximately 70% of the variance in the data is described by the model with a predictor (i.e., an independent variable). In linguistics, much lower \\(R^2\\) values are more common because our subject of study is often influenced by many arbitrary factors that we cannot capture. The adj.r.squared value is a form of \\(R^2\\) that is normalized for the number of independent variables. This is important because with a higher number of independent variables, \\(R^2\\) will automatically increase, even if one or more of the variables do not contribute statistically to explaining or estimating the y-values. The adjusted \\(R^2\\), on the other hand, incorporates the number of independent variables into the calculation of \\(R^2\\) and is therefore more reliable than the simple \\(R^2\\). Since there is only one variable here, \\(R^2\\) and adjusted \\(R^2\\) are very similar. The column sigma contains the Residual Standard Error, which is an estimate of the standard deviation of the error distribution. We can calculate this value using sigma() (we will return to this value shortly): queen.lm %&gt;% sigma() ## [1] 12.55 Then we see another statistic along with p.value. This time it’s the \\(F\\) statistic, meaning we read the \\(p\\)-value from the \\(F\\) distribution, which can be plotted using df() in ggplot2 (the function’s name stands for \\(F\\) distribution, not for degrees of freedom). This distribution depends on two parameters: glance(queen.lm)$df and glance(queen.lm)$df.residual. These are the degrees of freedom for the model and for the residuals. In orange, you see the \\(F\\) distribution that fits our data (namely, with one degree of freedom for the model and 28 degrees of freedom for the residuals). The distribution can only take values greater than zero. In dark green, you see an \\(F\\) distribution where the degrees of freedom for the model have been changed, and in black, for comparison, the normal distribution. ggplot(data.frame(x = 0:5)) + aes(x) + stat_function(fun = dnorm) + stat_function(fun = df, args = list(df1 = 1, df2 = 28), col = &quot;orange&quot;) + stat_function(fun = df, args = list(df1 = 7, df2 = 28), col = &quot;darkgreen&quot;) + xlab(&quot;&quot;) + ylab(&quot;density&quot;) ## Warning: Computation failed in `stat_function()`. ## Caused by error in `fun()`: ## ! could not find function &quot;fun&quot; ## Warning: Computation failed in `stat_function()`. ## Caused by error in `fun()`: ## ! could not find function &quot;fun&quot; Looking at the orange distribution here, we can already see that an \\(F\\)-value of 64.3 (see statistic) would be extremely unlikely, hence the very small \\(p\\)-value. The null hypothesis of the \\(F\\)-test performed here is that a model without predictors explains the data as well as, or better than, the model with the predictor age. That is, if the \\(p\\)-value resulting from the \\(F\\)-test is very small, we can conclude that our model with the independent variable age explains the data better than a model without predictors. We will now try to understand the values in the columns statistic, df, df.residual, and deviance. You already know the latter as SSE (sum of squared error): SSE &lt;- queen.lm %&gt;% deviance() SSE ## [1] 4412 df, as already indicated, is the number of degrees of freedom of the model and is calculated as the number of regression coefficients \\(k\\) minus 1: k &lt;- length(queen.lm$coefficients) df.SSR &lt;- k-1 df.SSR ## [1] 1 df.residual is the number of degrees of freedom of the residuals, which is calculated as the number of observations \\(N\\) minus the number of regression coefficients \\(k\\). N &lt;- queen %&gt;% nrow() df.SSE &lt;- N - k df.SSE ## [1] 28 Finally, to understand the \\(F\\)-value in the statistic column, let’s look at its formula: \\(F = \\frac{MSR}{MSE}\\). Here, MSR stands for mean squares of regression and MSE for mean squares of error. These two values describe the variance of the estimated y-values and the variance of the residuals. To calculate MSE and MSR, we first need to calculate two other values: sum of squares of Y (SSY), which describes the distance of the data points from the mean of \\(y\\), and sum of squares of regression (SSR), which describes the difference between SSY and SSR. First, SSY: SSY &lt;- sum((queen$f0 - mean(queen$f0))^2) SSY ## [1] 14537 In practice, SSY is the sum of all distances between the black (actually measured) data points and the orange line that intersects the y-axis at mean(queen$f0) and has a slope of zero (this figure again only shows a section of the entire range of values): You can already see visually that the orange line describes the data much less accurately than the blue regression line, which is why SSY is much larger than SSE. To calculate SSR, we now only need to find the difference between SSY and SSE: SSR &lt;- SSY - SSE SSR ## [1] 10125 Now we can finally calculate MSE and MSR: MSE &lt;- SSE/df.SSE MSE ## [1] 157.6 MSR &lt;- SSR/df.SSR MSR ## [1] 10125 … and the \\(F\\)-value is obtained from the division of MSE and MSR: F_wert &lt;- MSR / MSE F_wert ## [1] 64.26 Incidentally, there is a quadratic relationship between the \\(t\\)-statistic and our \\(F\\)-statistic: \\(F = t^2\\) or \\(t = \\sqrt{F}\\): sqrt(F_wert) ## [1] 8.016 Therefore, the \\(p\\)-value is exactly the same for both statistics. Finally, let’s return to the residual standard error, which we described above as an estimate of the standard deviation of the residuals. This is calculated as the square root of MSE: sqrt(MSE) ## [1] 12.55 When we determine the standard deviation of the residuals, it should be very close to the residual standard error (however, the latter is only an estimate, therefore the values do not have to be the same): queen.lm %&gt;% augment() %&gt;% pull(.resid) %&gt;% sd() ## [1] 12.33 If the residual standard error is exactly zero, then all data points lie exactly on the regression line, i.e., every y-value from the dataset can be calculated exactly by the corresponding x-value using a linear model. 3.9 Reporting the Result We have now reviewed all the (relevant) results from lm(). We did this using the broom functions. A more traditional way to summarize the results of the linear regression is provided by summary(): queen.lm %&gt;% summary() ## ## Call: ## lm(formula = f0 ~ age, data = queen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.55 -8.46 -0.10 6.24 36.27 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 288.186 6.977 41.31 &lt; 2e-16 *** ## age -1.074 0.134 -8.02 9.9e-09 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.6 on 28 degrees of freedom ## Multiple R-squared: 0.697, Adjusted R-squared: 0.686 ## F-statistic: 64.3 on 1 and 28 DF, p-value: 9.93e-09 You should recognize all the numbers here. It is extremely important that we report our results correctly. For this, we need: \\(R^2\\) (or, because it’s more stable: adjusted \\(R^2\\)): 0.69 the \\(F\\)-value: 64.3 the degrees of freedom for the model and the residuals: 1 and 28 the \\(p\\)-value, or the next higher significance level: \\(p &lt; 0.001\\) We report: There is a significant linear relationship between the Queen’s age and her fundamental frequency (\\(R^2\\) = 0.69, \\(F\\)[1, 28] = 64.3, \\(p\\) &lt; 0.001). 3.10 Summary In a linear regression, the values of the dependent variable \\(y\\) are estimated using the values of the independent variable \\(x\\), assuming a linear relationship exists between them. The regression line is the straight line to which the data points are closest (least squares method). The function lm() estimates the regression coefficients y-intercept and slope. A \\(t\\)-test is performed to determine if the regression coefficients differ from zero. If \\(p &lt; 0.05\\) in the \\(t\\)-test for \\(x\\), then the slope differs significantly from zero, meaning \\(x\\) is a good predictor of \\(y\\). The differences between the actual and estimated y-values are called residuals or errors; the residual standard error is an estimate of the standard deviation of the error distribution. \\(R^2\\) is the square of the correlation coefficient \\(r\\) and describes the proportion of the variance in the dependent variable \\(y\\) that is described by the linear model. An \\(F\\)-test is also performed to check whether the linear model successfully explains a significant proportion of the variance in the dependent variable. If \\(p &lt; 0.05\\) in the \\(F\\)-test, then the model with the chosen predictor describes the empirical data better than a model without predictors (mean model). SSE is the sum of squares of error and describes the distance of the data points from the regression line; the least squares method attempts to minimize SSE. SSR is the sum of squares of regression and describes how well the regression model performs compared to the mean model (SSY). "],["multiple-linear-regression.html", "4 Multiple Linear Regression 4.1 Load Packages and Data 4.2 Introduction 4.3 Continuous Independent Variables 4.4 Categorical Independent Variables 4.5 Mix of Continuous and Categorical Variables", " 4 Multiple Linear Regression 4.1 Load Packages and Data Please load the following packages and datasets: library(broom) library(emmeans) ## Welcome to emmeans. ## Caution: You lose important information if you filter this package&#39;s results. ## See &#39;? untidy&#39; library(tidyverse) library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; faux &lt;- read.table(file.path(url, &quot;faux.txt&quot;), stringsAsFactors = T, header = T) %&gt;% as_tibble() int &lt;- read.table(file.path(url, &quot;dbwort.df.txt&quot;), stringsAsFactors = T) %&gt;% as_tibble() %&gt;% rename(vowel = V, gender = G, subject = Vpn, word = Wort) vlax &lt;- read.table(file.path(url, &quot;vlax.txt&quot;), stringsAsFactors = T) %&gt;% as_tibble() %&gt;% filter(V %in% c(&quot;O&quot;, &quot;I&quot;) &amp; f0 != 0) %&gt;% rename(vowel = V, subject = Vpn, word = Wort, duration = Dauer) 4.2 Introduction Up to now, we have worked with simple linear regression, where one variable (e.g., fundamental frequency) was predicted by another variable (e.g., age in years). However, we more often have multiple variables that we suspect influence our measurements. For this, we need multiple linear regression. The formula for linear regression is adapted so that there is a separate slope for each independent variable \\(x_1\\), \\(x_2\\), etc.: \\(y = k + b_1x_1 + b_2x_2 + ...\\) Here again, \\(k\\) represents the y-intercept, and \\(b_1\\), \\(b_2\\), etc., are slopes. Below, we show three examples of multiple linear regressions, depending on the type of independent variables (categorical or continuous). For clarity, all examples contain only two independent variables; however, regressions can, of course, generally include more than two predictors. 4.3 Continuous Independent Variables First, we want to find out if the fundamental frequency f0 in the (artificially created) data frame faux depends on the two continuous variables dB (volume) and dur (word duration). To do this, we create a plot of the data: ggplot(faux) + aes(x = dB, y = f0, col = dur) + geom_point() Interpreting this plot isn’t entirely straightforward. If we initially focus solely on the relationship between fundamental frequency and volume, we see a clear positive correlation; that is, the louder the participants spoke, the higher their fundamental frequency became. Since duration is also a continuous variable, we can only use a color continuum to visualize it (ggplot() does this automatically with the col argument). It appears that the darker points (= low duration values) are associated with relatively high f0 values, and the lighter data points (= high duration values) with relatively low f0 values. Therefore, there could be a negative correlation between f0 and duration. We can verify this impression using cor(): cor(faux$f0, faux$dB) ## [1] 0.3107 cor(faux$f0, faux$dur) ## [1] -0.53 Since we are dealing with two independent variables, we must also consider whether there might be an interaction between them. An interaction exists when the effect of one independent variable on the dependent variable differs for different (extreme) values of the second independent variable. In our example, the effect of volume on f0 is generally positive, meaning the louder the speech, the higher f0. However, this effect is (visually) more pronounced for low than for high duration values. Imagine placing two regression lines through the figure above: a dark blue one for an example of a low duration value (e.g., 150 ms) and a light blue one for an example of a high duration (e.g., 450 ms). If there is no interaction between the variables, the two example regression lines will be parallel to each other; otherwise, they will not. For the data in the faux data frame, I have drawn the two example regression lines: in the left plot under the assumption that there is no interaction, in the right plot under the assumption that there is an interaction: The regression lines in the right-hand plot appear to describe the data better than the data in the left-hand plot; that is, we assume there is an interaction between volume and duration in the dataset. We will see later exactly how the regression lines for the two plots were calculated. Further Information: Regression lines for two continuous predictors Drawing regression lines for two continuous variables requires some computation. However, if you want to avoid this work at some point, you can install the libraries ggiraph and ggiraphExtra. This vignette provides information about the ggPredict() function from ggiraphExtra, which can draw regression lines for multiple predictors. In the following sections, we will first calculate a multiple regression without and then with interaction for many research questions, so that you can familiarize yourself with interactions. If you are faced with the decision of whether there might be an interaction between independent variables in your dataset, visualize your data and consider carefully how an interaction would be interpreted – before you run the regression. 4.3.1 Without Interaction If there are no interactions in a model, the independent variables in the lm() formula are connected with a plus sign. lm1 &lt;- lm(f0 ~ dB + dur, data = faux) lm1 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 169. ## 2 dB 2.89 ## 3 dur -0.372 Here we try to understand exactly what the estimates in the estimate column mean. The estimate for the intercept, as in the simple linear regression, is the estimated arithmetic mean of the dependent variable for \\(x_1 = 0\\) and \\(x_2 = 0\\) (and all possible additional \\(x\\) values if there are more independent variables). That is, if the volume is 0 dB and the duration is also 0 ms, the fundamental frequency should be 168.6 Hz. The slopes no longer measure the effect of each individual independent variable, but rather the effect of one independent variable, while all other variables are held constant at zero. Every increase in volume by one decibel (at constant duration) leads to an increase in the fundamental frequency of 2.9 Hz, and every increase in duration by one millisecond (at constant volume) leads to a reduction in the fundamental frequency of 0.4 Hz. This means we find the expected relationships: volume positively influences the fundamental frequency, duration negatively. Let’s recall the formula for the regression line with two independent variables: \\(y = k + b_1x_1 + b_2x_2\\) We can retrieve the values for the y-intercept \\(k\\) and the two slopes \\(b_1\\) and \\(b_2\\) from the data frame returned by tidy(): k &lt;- lm1 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) k ## [1] 168.6 b_1 &lt;- lm1 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_1 ## [1] 2.892 b_2 &lt;- lm1 %&gt;% tidy() %&gt;% filter(term == &quot;dur&quot;) %&gt;% pull(estimate) b_2 ## [1] -0.3721 Now we can choose any values for \\(x_1\\) (volume in decibels) and \\(x_2\\) (duration in milliseconds), insert them into the formula along with the regression coefficients, and thus estimate y-values. If we set both \\(x\\) values to zero, the estimated \\(y\\) value is exactly the intercept \\(k\\): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur \\\\ &amp;= 168.64 + 2.89 \\cdot 0 + (-0.37 \\cdot 0) \\\\ &amp;= 168.64 \\end{aligned} \\] If you don’t want to calculate these values manually, the predict() function can do it for you. For multiple linear regression, the function requires a data frame with as many columns as there were independent variables in the regression; the columns must also be named exactly like the independent variables. For demonstration purposes, we show that the fundamental frequency f0, according to the calculated linear model, is indeed approximately 168 Hz when the loudness is 0 dB and the duration is 0 ms. We also show the estimated f0 value for db = 1 and dur = 0 (which is 2.9 Hz higher than the intercept) and for db = 0 and dur = 1 (which is 0.4 Hz lower than the intercept). Finally, we use predict() to estimate the f0 value for the average loudness and duration. d1 &lt;- data.frame(dB = c(0, 1, 0, mean(faux$dB)), dur = c(0, 0, 1, mean(faux$dur))) d1 %&lt;&gt;% mutate(estimated_f0 = predict(lm1, d1)) d1 ## dB dur estimated_f0 ## 1 0.0 0.0 168.6 ## 2 1.0 0.0 171.5 ## 3 0.0 1.0 168.3 ## 4 60.1 298.9 231.3 Now we can also understand how the two example regression lines in the left plot above were calculated. We choose 450 ms as an example of a high duration and 150 ms as an example of a low duration (in the literature, the mean plus/minus 1.5 standard deviations of an independent variable is often used as an “extreme value”). We then insert these values, along with the regression coefficients, into our formula for the regression line (here first for a duration of 450 ms): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-0.37 \\cdot 450) \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-167.43) \\\\ &amp;= 1.21 + 2.89 \\cdot dB \\end{aligned} \\] The result is that, for a duration of 450 ms, the intercept of a regression line should be 1.21 and the slope should be 2.89. \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-0.37 \\cdot 150) \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-55.81) \\\\ &amp;= 112.83 + 2.89 \\cdot dB \\end{aligned} \\] For the regression line with a duration of 150 ms, the intercept is at 112.83 and the slope is at 2.89. Therefore, the slope is the same for both regression lines, meaning these lines are parallel to each other. The two calculated intercepts and the slope can now be used for geom_abline(): high_dur &lt;- 450 low_dur &lt;- 150 slope &lt;- b_1 intercept_high_dur &lt;- k + b_1 * 0 + b_2 * high_dur intercept_low_dur &lt;- k + b_1 * 0 + b_2 * low_dur ggplot(faux) + aes(x = dB, y = f0, col = dur) + geom_point() + xlim(0, 95) + ylim(0, 500) + geom_abline(slope = slope, intercept = intercept_high_dur, color = &quot;#56B1F7&quot;, linewidth = 1.2) + geom_abline(slope = slope, intercept = intercept_low_dur, color = &quot;#132B43&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) We could, of course, have placed dur on the x-axis and represented dB using a color continuum. Then we would have to choose two extreme example volume values, insert all the values (intercept, slopes, chosen example values for dB) into the formula, and we would arrive at the following result: high_dB &lt;- 75 low_dB &lt;- 45 intercept_high_dB &lt;- k + b_1 * high_dB + b_2 * 0 intercept_low_dB &lt;- k + b_1 * low_dB + b_2 * 0 slope &lt;- b_2 ggplot(faux) + aes(x = dur, y = f0, col = dB) + geom_point() + xlim(0, 600) + ylim(0, 500) + geom_abline(slope = slope, intercept = intercept_high_dB, color = &quot;#56B1F7&quot;, linewidth = 1.2) + geom_abline(slope = slope, intercept = intercept_low_dB, color = &quot;#132B43&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) If you have taken sufficient time to understand and interpret the regression coefficients, we can now turn to the statistics and goodness-of-fit criteria of the model. Using tidy(), we examine the \\(t\\)-statistic along with the \\(p\\)-value, which indicates whether the regression coefficients differ significantly from zero: lm1 %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 169. 3.45 48.8 0 ## 2 dB 2.89 0.0633 45.7 1.91e-306 ## 3 dur -0.372 0.00668 -55.7 0 According to the tests, the two regression coefficients for the independent variables differ significantly from zero, meaning that both the volume and the duration appear to be good predictors of the fundamental frequency. The goodness-of-fit criteria are again the \\(F\\)-statistic and \\(R^2\\): lm1 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 0.661 ## 2 adj.r.squared 0.660 ## 3 sigma 24.0 ## 4 statistic 1815. ## 5 p.value 0 ## 6 df 2 ## 7 logLik -8589. ## 8 AIC 17186. ## 9 BIC 17208. ## 10 deviance 1073556. ## 11 df.residual 1866 ## 12 nobs 1869 The two independent variables describe 66% of the variance in the fundamental frequency values. Generally speaking, this is a relatively high value. However, this value becomes even more impressive when compared to the \\(R^2\\) values from the two simple linear regressions that we can perform using the independent variables: lm(f0 ~ dB, data = faux) %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.09651 lm(f0 ~ dur, data = faux) %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.2809 This means that a model with volume as the only independent variable describes just under 10% of the variance in the fundamental frequency values, while a model with duration describes approximately 28% of this variance. However, when both predictors are combined in a single model (without interaction), this proportion increases to 66%. Finally, we should not forget to report the results of the regression: A multiple linear regression with volume and duration as independent variables showed a significant effect of volume (\\(t\\) = 45.7, \\(p\\) &lt; 0.001) and duration (\\(t\\) = 55.7, \\(p\\) &lt; 0.001) on the fundamental frequency. The chosen model describes the data better than a model without predictors (\\(R^2\\) = 0.66, \\(F\\)[2, 1866] = 1815, \\(p\\) &lt; 0.001). In a scientific publication, you should, of course, dedicate a few lines to describing the direction of the significant effects (i.e., whether they positively or negatively, strongly or weakly, influence the dependent variable) and whether this corresponds to the expectations (hypotheses) that you ideally formulated before data collection. (Of course, under realistic conditions, we would also check the assumptions about the residuals here!) 4.3.2 With Interaction Since we determined in our first plot of the data in faux that there might be an interaction between volume and duration, we now want to calculate the model with this interaction. Interactions in a linear model can be written in two ways: either with an asterisk dB * dur or in the more explicit form dB + dur + dB:dur. With three independent variables A, B, and C, interactions can occur between any two factors as well as between all three, i.e., A * B * C or A + B + C + A:B + A:C + B:C + A:B:C. It can also be useful to calculate an interaction between only two of the three factors, e.g., A * B + C or A + B + C + A:B. Here, we choose the shorthand notation and first look again at the estimates of the regression coefficients: lm2 &lt;- lm(f0 ~ dB * dur, data = faux) lm2 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 86.0 ## 2 dB 4.30 ## 3 dur -0.0921 ## 4 dB:dur -0.00466 The y-intercept for dB = 0 and dur = 0 is now at 86 Hz. The slopes describe the influence of an independent variable when all other independent variables are held constant at zero. The loudness dB also has a positive influence on the fundamental frequency in this model, with a slope of 4.3 Hz, meaning the fundamental frequency increases with loudness (when the duration is held constant at zero). The slope for dur, however, is negative at -0.09 Hz, meaning the fundamental frequency decreases for higher duration values (when the volume is held constant at zero). Now, there is another slope, namely for the interaction dB:dur. This slope is also found in the formula for the regression line adapted for interactions: \\(y = k + b_1x_1 + b_2x_2 + b_3(x_1 \\cdot x_2)\\) The slope \\(b_3(x_1 \\cdot x_2)\\) already indicates that the interaction is only important for the regression line if both \\(x_1\\) and \\(x_2\\) are non-zero: because if either of them is zero, the multiplication \\(x_1 \\cdot x_2\\) results in zero, meaning the slope \\(b_3\\) must be multiplied by zero and thus disappears from the formula. In our model, the slope for the interaction of the two predictors is negative. This can be interpreted as the fundamental frequency decreasing when both the volume and the duration increase. To understand these relationships precisely, we retrieve the intercept \\(k\\), the two slopes \\(b_1\\) (for volume) and \\(b_2\\) (for duration), and the interaction \\(b_3\\) from the data frame above: k &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) k ## [1] 86.02 b_1 &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_1 ## [1] 4.298 b_2 &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;dur&quot;) %&gt;% pull(estimate) b_2 ## [1] -0.09212 b_3 &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;dB:dur&quot;) %&gt;% pull(estimate) b_3 ## [1] -0.004656 Now you can insert these values, along with freely chosen values for \\(x_1\\) (volume) and \\(x_2\\) (duration), into the formula above to find out what the fundamental frequency should be for the chosen values of the independent variables. Let’s assume \\(x_1\\) and \\(x_2\\) are both zero; then, again, the estimated y-value corresponds exactly to the intercept \\(k\\): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot 0 + (-0.092 \\cdot 0) + (-0.0047 \\cdot 0 \\cdot 0) \\\\ &amp;= 86.02 \\end{aligned} \\] Similarly, instead of the two zeros, you can use different values for \\(x_1\\) and \\(x_2\\). As an example, we choose \\(x_1 = 1\\) and \\(x_2 = 1\\) to show that the slope then also comes into play for the interaction of the two variables: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot 1 + (-0.092 \\cdot 1) + (-0.0047 \\cdot 1 \\cdot 1) \\\\ &amp;= 90.22 \\end{aligned} \\] Or, by using the extracted regression coefficients: k + b_1 + b_2 + b_3 ## [1] 90.22 Here again we can use predict() to estimate the f0 value for different combinations of dB and dur values: d2 &lt;- data.frame(dB = c(0, 1, 0, 1, mean(faux$dB)), dur = c(0, 0, 1, 1, mean(faux$dur))) d2 %&lt;&gt;% mutate(estimated_f0 = predict(lm2, d2)) d2 ## dB dur estimated_f0 ## 1 0.0 0.0 86.02 ## 2 1.0 0.0 90.32 ## 3 0.0 1.0 85.93 ## 4 1.0 1.0 90.22 ## 5 60.1 298.9 233.19 Now we can re-create the example regression lines that we saw in the right-hand plot above. We choose 450 ms and 150 ms as example duration values: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-0.092 \\cdot 450) + (-0.0047 \\cdot dB \\cdot 450) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-41.45) + (-2.10 \\cdot dB) \\\\ &amp;= 44.57 + 2.20 \\cdot dB \\end{aligned} \\] And for 150ms: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-0.092 \\cdot 150) + (-0.0047 \\cdot dB \\cdot 150) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-13.82) + (-0.70 \\cdot dB) \\\\ &amp;= 72.20 + 3.60 \\cdot dB \\end{aligned} \\] This time, the slopes for the two extreme chosen duration values differ precisely because the interaction was included. Now we define the calculated intercepts and slopes as variables and insert them again into geom_abline() to draw the regression lines: high_dur &lt;- 450 low_dur &lt;- 150 intercept_low_dur &lt;- k + b_1 * 0 + b_2 * low_dur + b_3 * 0 * low_dur intercept_low_dur ## [1] 72.2 intercept_high_dur &lt;- k + b_1 * 0 + b_2 * high_dur + b_3 * 0 * high_dur intercept_high_dur ## [1] 44.57 slope_low_dur &lt;- b_1 + b_3 * low_dur slope_low_dur ## [1] 3.6 slope_high_dur &lt;- b_1 + b_3 * high_dur slope_high_dur ## [1] 2.203 ggplot(faux) + aes(x = dB, y = f0, col = dur) + geom_point() + xlim(0, 95) + ylim(0, 500) + geom_abline(slope = slope_low_dur, intercept = intercept_low_dur, color = &quot;#132B43&quot;, linewidth = 1.2) + geom_abline(slope = slope_high_dur, intercept = intercept_high_dur, color = &quot;#56B1F7&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) Having understood the regression coefficients, we take a look at the \\(t\\)-statistic and find that both the slope for dB (\\(t\\) = 22.6, \\(p\\) &lt; 0.001) and that for dur (\\(t\\) = 2.5, \\(p\\) &lt; 0.05) are significantly different from zero. Additionally, the slope for the interaction is also significantly different from zero (\\(t\\) = 7.8, \\(p\\) &lt; 0.001). lm2 %&gt;% tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 86.0 11.1 7.75 1.50e- 14 ## 2 dB 4.30 0.190 22.6 4.63e-100 ## 3 dur -0.0921 0.0364 -2.53 1.15e- 2 ## 4 dB:dur -0.00466 0.000595 -7.82 8.83e- 15 This means that the entire model is also likely to be better than a model without predictors. We’ll use glance() again to look at the \\(F\\)-statistic and \\(R^2\\): lm2 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 0.671 ## 2 adj.r.squared 0.671 ## 3 sigma 23.6 ## 4 statistic 1270. ## 5 p.value 0 ## 6 df 3 ## 7 logLik -8559. ## 8 AIC 17128. ## 9 BIC 17156. ## 10 deviance 1039480. ## 11 df.residual 1865 ## 12 nobs 1869 Thus, we can report: A multiple linear regression with volume and duration as independent variables, including their interaction, showed a significant effect of volume (\\(t\\) = 22.6, \\(p\\) &lt; 0.001) and duration (\\(t\\) = 2.5, \\(p\\) &lt; 0.05) on the fundamental frequency. Additionally, the interaction was also significant (\\(t\\) = 7.8, \\(p\\) &lt; 0.001). The chosen model describes the data better than a model without predictors (\\(R^2\\) = 0.67, \\(F\\)[3, 1865] = 1269.6, \\(p\\) &lt; 0.001). (Of course, under realistic conditions, we would also test the assumptions about the residuals!) 4.4 Categorical Independent Variables Now we want to use the data in int to find out if the volume in decibels db is influenced by the vowel type vowel and the gender of the test subject gender. int ## # A tibble: 120 × 5 ## db vowel gender subject word ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 100 a m S1 w1 ## 2 70 a m S2 w1 ## 3 90 a m S3 w1 ## 4 60 a m S4 w1 ## 5 80 a m S5 w1 ## 6 50 a f S6 w1 ## 7 40 a f S7 w1 ## 8 60 a f S8 w1 ## 9 30 a f S9 w1 ## 10 20 a f S10 w1 ## # ℹ 110 more rows First, we’ll look at a boxplot to see if there could be any dependency between the variables: ggplot(int) + aes(x = vowel, y = db, fill = gender) + geom_boxplot() The plot shows, generally speaking, that women speak more quietly than men. However, this effect differs for the two vowels: For /a/, the effect of gender is much more pronounced than for /i/. Therefore, there could be an interaction between vowel and gender. Similarly, we can consider the interaction of the independent variables for the effect of the vowel: The volume is generally lower for the vowel /i/ than for /a/ – but for men, the effect is stronger than for women (i.e., the difference between the two blue boxes is greater than the difference between the two red boxes). Although there is likely an interaction between vowel and gender here, for didactic reasons, let’s first look at what a multiple regression with two categorical predictors (independent variables) without interaction looks like. 4.4.1 Without Interaction lm3 &lt;- lm(db ~ vowel + gender, data = int) lm3 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 53.2 ## 2 voweli -20.1 ## 3 genderm 30.1 When dealing with continuous independent variables, the intercept is the estimated y-value for zero across all independent variables. But what exactly is “zero” for categorical variables? R determines this through treatment coding (often also referred to as dummy coding). The variable gender can take the values f (female) or m (male). R then proceeds alphabetically and determines that f should be interpreted as zero and m as one. The same applies to the vowels vowel, where a is interpreted as zero and i as one. Accordingly, the intercept in this regression is the estimated decibel mean for the female a sound. We can easily verify this mathematically: k &lt;- lm3 %&gt;% augment() %&gt;% filter(vowel == &quot;a&quot; &amp; gender == &quot;f&quot;) %&gt;% summarise(m = mean(.fitted)) %&gt;% pull(m) k ## [1] 53.23 As you can see, the two slopes in our model are called genderm and voweli. The factor (the name of the categorical variable) is always listed first, followed by the level of the factor that was not processed in the intercept (i.e., the level m for gender and the level i for vowel). The slope genderm describes the change in volume from women to men for the vowel /a/. At 30.1 dB, it is positive, meaning men produce /a/ significantly louder than women. The slope voweli describes the change in volume from /a/ to /i/ for women and is negative at -20.12 dB, meaning women produce /i/ more quietly than /a/. This aligns with our impressions from the boxplots. If we want to estimate the average volume levels for all four combinations of vowel and gender, we again need the two slopes \\(b_1\\) (for the vowel /i/) and \\(b_2\\) (for male) from the result of tidy(), as well as the intercept \\(k\\), which we already calculated above. Also, remember that the treatment coding specifies that a = 0, i = 1, female = 0, and male = 1. b_1 &lt;- lm3 %&gt;% tidy() %&gt;% filter(term == &quot;voweli&quot;) %&gt;% pull(estimate) b_1 ## [1] -20.12 b_2 &lt;- lm3 %&gt;% tidy() %&gt;% filter(term == &quot;genderm&quot;) %&gt;% pull(estimate) b_2 ## [1] 30.08 Estimated volume of /a/ (\\(x_1 = 0\\)) produced by women (\\(x_2 = 0\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 0) + 30.09 \\cdot 0 \\\\ &amp;= 53.23 \\end{aligned} \\] … for /a/ (\\(x_1 = 0\\)) produced by men (\\(x_2 = 1\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 0) + 30.09 \\cdot 1 \\\\ &amp;= 83.31 \\end{aligned} \\] … for /i/ (\\(x_1 = 1\\)) produced by women (\\(x_2 = 0\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 1) + 30.09 \\cdot 0 \\\\ &amp;= 33.11 \\end{aligned} \\] … and for /i/ (\\(x_1 = 1\\)) produced by men (\\(x_2 = 1\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 1) + 30.09 \\cdot 1 \\\\ &amp;= 63.19 \\end{aligned} \\] The predict() function can also do this calculation work for us: d3 &lt;- data.frame(gender = c(&quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;), vowel = c(&quot;a&quot;, &quot;i&quot;, &quot;a&quot;, &quot;i&quot;)) d3 %&lt;&gt;% mutate(estimated_mean_dB = predict(lm3, d3)) d3 ## gender vowel estimated_mean_dB ## 1 f a 53.23 ## 2 f i 33.11 ## 3 m a 83.31 ## 4 m i 63.19 Compare these estimated means with the boxplots we created earlier or with the actual means (which you can calculate using tidyverse functions). We naturally aim for the most accurate estimates possible, as this indicates that our chosen model fits the data very well. In this case, the estimates already appear to be quite good: int %&gt;% group_by(gender, vowel) %&gt;% summarise(m = mean(db)) ## `summarise()` has grouped output by &#39;gender&#39;. You can ## override using the `.groups` argument. ## # A tibble: 4 × 3 ## # Groups: gender [2] ## gender vowel m ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 f a 48.2 ## 2 f i 38.2 ## 3 m a 88.4 ## 4 m i 58.1 Further Information: Dummy Coding If one of the independent variables had more than two levels (i.e., if, for example, a third vowel /o/ occurred in the dataset), then there would be another slope for that level (e.g., vowelo). There are also other types of dummy coding, such as sum coding, as you can read in Chapter 7 in Winter (2020). Now let’s take a look at the statistical results of our multiple regression: lm3 %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 53.2 3.28 16.2 8.86e-32 ## 2 voweli -20.1 3.78 -5.32 5.09e- 7 ## 3 genderm 30.1 3.78 7.95 1.27e-12 This overview shows that there was a significant influence of gender (\\(t\\) = 8.0, \\(p\\) &lt; 0.001) and vowel (\\(t\\) = 5.3, \\(p\\) &lt; 0.001) on loudness; or in other words, the slopes for vowel and gender differ significantly from zero and are therefore good predictors of volume. Now let’s look at the goodness-of-fit criteria: lm3 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 4.39e- 1 ## 2 adj.r.squared 4.29e- 1 ## 3 sigma 2.07e+ 1 ## 4 statistic 4.58e+ 1 ## 5 p.value 2.06e-15 ## 6 df 2 e+ 0 ## 7 logLik -5.32e+ 2 ## 8 AIC 1.07e+ 3 ## 9 BIC 1.08e+ 3 ## 10 deviance 5.02e+ 4 ## 11 df.residual 1.17e+ 2 ## 12 nobs 1.2 e+ 2 Here we see that the two independent variables, gender and vowel, can explain approximately 43% of the variance in the volume measurements. The \\(F\\)-test shows that our regression model describes the data more successfully than a model without predictors (\\(R^2\\) = 0.43, \\(F\\)[2, 117] = 45.8, \\(p\\) &lt; 0.001). (Of course, under realistic conditions, we would also test the assumptions about the residuals here!) 4.4.2 With Interaction Since we also suspect an interaction between vowel and gender in this dataset, we include it in our regression: lm4 &lt;- lm(db ~ vowel * gender, data = int) lm4 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 48.2 ## 2 voweli -10.0 ## 3 genderm 40.2 ## 4 voweli:genderm -20.2 The intercept still describes the estimated average decibel level for /a/ for females, which is approximately 48 dB. The influence of the vowel on loudness is again negative, while the influence of gender is positive. Now, there is also the slope for the interaction voweli:genderm, which is negative at -20.2 dB. Therefore, if the vowel is /i/ and the gender is male, the loudness decreases. Below, you can see how, by including the interaction, you can estimate the loudness for the four combinations of \\(vowel \\times gender\\) (all of this is again derived from the formula for the regression line with interaction): k &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) b_1 &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;genderm&quot;) %&gt;% pull(estimate) b_2 &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;voweli&quot;) %&gt;% pull(estimate) b_3 &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;voweli:genderm&quot;) %&gt;% pull(estimate) # female-a k ## [1] 48.17 # male-a k + b_1 ## [1] 88.36 # female-i k + b_2 ## [1] 38.16 # male-i k + b_1 + b_2 + b_3 ## [1] 58.14 The predict() function can once again do the calculations for you: d4 &lt;- data.frame(gender = c(&quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;), vowel = c(&quot;a&quot;, &quot;i&quot;, &quot;a&quot;, &quot;i&quot;)) d4 %&lt;&gt;% mutate(estimated_mean_dB = predict(lm4, d4)) d4 ## gender vowel estimated_mean_dB ## 1 f a 48.17 ## 2 f i 38.16 ## 3 m a 88.36 ## 4 m i 58.14 By integrating the interaction into the model, the volume estimates have become more precise compared to the previous model! In fact, the estimates perfectly match the actual average values (this will almost never happen in “real life”): int %&gt;% group_by(gender, vowel) %&gt;% summarise(m = mean(db)) ## `summarise()` has grouped output by &#39;gender&#39;. You can ## override using the `.groups` argument. ## # A tibble: 4 × 3 ## # Groups: gender [2] ## gender vowel m ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 f a 48.2 ## 2 f i 38.2 ## 3 m a 88.4 ## 4 m i 58.1 Now let’s look at the \\(t\\)-statistic, which shows whether our regression coefficients help to explain the variance in the decibel values: lm4 %&gt;% tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 48.2 3.68 13.1 1.37e-24 ## 2 voweli -10.0 5.21 -1.92 5.69e- 2 ## 3 genderm 40.2 5.21 7.72 4.47e-12 ## 4 voweli:genderm -20.2 7.36 -2.75 7.00e- 3 The slope for gender (\\(t\\) = 7.7, \\(p\\) &lt; 0.001) and the slope for vowel (\\(t\\) = 1.9, \\(p\\) = 0.06) (almost) differ significantly from zero. Additionally, there was a significant interaction between the factors (\\(t\\) = 2.8, \\(p\\) &lt; 0.01). Above, we established that our model with interaction better describes the data than the previous model without interaction. This is also reflected in the \\(F\\)-statistic: lm4 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 4.73e- 1 ## 2 adj.r.squared 4.60e- 1 ## 3 sigma 2.02e+ 1 ## 4 statistic 3.47e+ 1 ## 5 p.value 4.29e-16 ## 6 df 3 e+ 0 ## 7 logLik -5.29e+ 2 ## 8 AIC 1.07e+ 3 ## 9 BIC 1.08e+ 3 ## 10 deviance 4.71e+ 4 ## 11 df.residual 1.16e+ 2 ## 12 nobs 1.2 e+ 2 Now, approximately 46% of the variance in the data is described by the two factors and their interaction. The model with the interaction therefore describes more variance than the model without interaction; that is, it provides more precise estimates of the regression coefficients. The \\(F\\)-test also shows that the regression model describes the data better than the intercept-only model (\\(R^2\\) = 0.46, \\(F\\)[3, 116] = 34.7, \\(p\\) &lt; 0.001). (Of course, under realistic conditions, we would also test the assumptions about the residuals here!) 4.4.2.1 Post-hoc Tests with emmeans If, in a multiple regression with at least two categorical independent variables, the interaction has a significant effect on the measured variable, then so-called post-hoc tests should be performed. The package emmeans with its eponymous function is exceptionally well-suited for this purpose. Several detailed vignettes are available for this package. The emmeans() function performs \\(t\\)-tests on all combinations of the values of the categorical independent variables. This allows us to determine which combination(s) resulted in the interaction being significant in the regression. The function takes the result of the regression model as an argument, followed by the formula pairwise ~ vowel | gender (pronounced: pairwise comparisons for vowel given gender). This formula means that the difference between the vowels and its significance are determined for each gender. emmeans(lm4, pairwise ~ vowel | gender) ## $emmeans ## gender = f: ## vowel emmean SE df lower.CL upper.CL ## a 48.2 3.68 116 40.9 55.5 ## i 38.2 3.68 116 30.9 45.5 ## ## gender = m: ## vowel emmean SE df lower.CL upper.CL ## a 88.4 3.68 116 81.1 95.7 ## i 58.1 3.68 116 50.9 65.4 ## ## Confidence level used: 0.95 ## ## $contrasts ## gender = f: ## contrast estimate SE df t.ratio p.value ## a - i 10.0 5.2 116 1.923 0.0569 ## ## gender = m: ## contrast estimate SE df t.ratio p.value ## a - i 30.2 5.2 116 5.806 &lt;.0001 The result of this function is two tables, one called emmeans and one called contrasts. In the emmeans table, column emmeans contains our estimates for the means, first for women (gender = f), then for men (gender = m). emmeans stands for estimated marginal means. Here you will also find the standard error SE, the degrees of freedom df, and the 95% confidence intervals lower.CL and upper.CL. In the contrasts table, you will find the differences between /a/ and /i/, first for women, then for men. Here, you will again first see an estimate. This is simply the difference between the emmeans for /a/ and /i/ for each gender. The standard error SE is followed by the degrees of freedom df, the \\(t\\)-value t.ratio, and the \\(p\\)-value p.value. In accordance with the boxplots we created at the very beginning of this chapter, we see that there is no significant volume difference between /a/ and /i/ for women (\\(t\\)[116] = 1.9, \\(p\\) = 0.06), but there is for men (\\(t\\)[116] = 5.8, \\(p\\) &lt; 0.001). Now we should also check whether there were significant differences between women and men for each vowel type. To do this, we simply change the formula to pairwise ~ gender | vowel: emmeans(lm4, pairwise ~ gender | vowel) ## $emmeans ## vowel = a: ## gender emmean SE df lower.CL upper.CL ## f 48.2 3.68 116 40.9 55.5 ## m 88.4 3.68 116 81.1 95.7 ## ## vowel = i: ## gender emmean SE df lower.CL upper.CL ## f 38.2 3.68 116 30.9 45.5 ## m 58.1 3.68 116 50.9 65.4 ## ## Confidence level used: 0.95 ## ## $contrasts ## vowel = a: ## contrast estimate SE df t.ratio p.value ## f - m -40.2 5.2 116 -7.721 &lt;.0001 ## ## vowel = i: ## contrast estimate SE df t.ratio p.value ## f - m -20.0 5.2 116 -3.838 0.0002 We can report that there is a significant difference between men and women for both /a/ (\\(t\\)[116] = 7.7, \\(p\\) &lt; 0.001) and /i/ (\\(t\\)[116] = 3.8, \\(p\\) &lt; 0.001). This also agrees with our visual impression from the initial boxplots. 4.5 Mix of Continuous and Categorical Variables Finally, we examine whether the fundamental frequency values in the data frame vlax are influenced by the vowel vowel and the loudness dB. This will be a regression model with one categorical and one continuous independent variable. For a graph, it is useful to place the continuous independent variable on the x-axis and represent the levels of the categorical variable with colors. We also show the regression lines that geom_smooth() can calculate for us. ggplot(vlax) + aes(x = dB, y = f0, col = vowel) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; In general, the familiar positive linear relationship between f0 and dB also appears to exist here. However, this relationship seems to exist only for the vowel /I/, but not for the vowel /O/. Therefore, there is likely an interaction between the two independent variables, but for demonstration purposes, we will first calculate a model without interaction. 4.5.1 Without Interaction lm5 &lt;- lm(f0 ~ dB + vowel, data = vlax) lm5 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -259. ## 2 dB 6.17 ## 3 vowelO -22.9 Here too, the categorical variable vowel is transformed behind the scenes using dummy coding. We see that the second slope is called vowelO, which means that the vowel “I” has already been processed in the intercept. Therefore, if the vowel is “I” and dB = 0, then the fundamental frequency should be -258.9 Hz. The influence of loudness on the fundamental frequency is, as expected, positive; that is, with every increase in loudness of 1 dB, the fundamental frequency increases by 6.2 Hz (you can see this, for example, in the comparison of the first two lines in d5). If the vowel is “O” instead of “I”, the fundamental frequency decreases by 22.9 Hz (see the first and third lines in d5). At average volume, the fundamental frequency is 200 Hz when producing “I” and 177.1 Hz when producing “O” (see lines four and five in d5). d5 &lt;- data.frame(dB = c(0, 1, 0, mean(vlax$dB), mean(vlax$dB)), vowel = c(&quot;I&quot;, &quot;I&quot;, &quot;O&quot;, &quot;I&quot;, &quot;O&quot;)) d5 %&lt;&gt;% mutate(estimated_f0 = predict(lm5, d5)) d5 ## dB vowel estimated_f0 ## 1 0.0 I -258.9 ## 2 1.0 I -252.7 ## 3 0.0 O -281.8 ## 4 74.4 I 200.0 ## 5 74.4 O 177.1 We can calculate a regression line for “I” and “O” using the linear model by obtaining the regression coefficients and inserting them into the formula as before: k &lt;- lm5 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) k ## [1] -258.9 b_1 &lt;- lm5 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_1 ## [1] 6.168 b_2 &lt;- lm5 %&gt;% tidy() %&gt;% filter(term == &quot;vowelO&quot;) %&gt;% pull(estimate) b_2 ## [1] -22.93 For the vowel “I” (\\(x_2 = 0\\)) the y-intercept is -258.9 Hz and the slope is 6.17 Hz. \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot x_2 \\\\ &amp;= -258.87 + (6.17 \\cdot dB) + (-22.93 \\cdot 0) \\\\ &amp;= -258.87 + (6.17 \\cdot dB) \\end{aligned} \\] For the vowel “O” (\\(x_2 = 1\\)) the y-intercept is -281.8 Hz, but the slope is also 6.17 Hz. \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot x_2 \\\\ &amp;= -258.87 + (6.17 \\cdot dB) + (-22.93 \\cdot 1) \\\\ &amp;= -281.79 + (6.17 \\cdot dB) \\end{aligned} \\] The regression lines calculated here cannot correspond to those in the plot, because the lines calculated here have the same slope and are therefore parallel to each other, while the lines drawn by geom_smooth() intersect. From this, we can conclude that this model, without interaction, does not fit the data particularly well. Nevertheless, the \\(t\\)-statistic shows that the regression coefficients differ significantly from zero (slope for loudness: \\(t\\) = 8.1, \\(p\\) &lt; 0.001; slope for vowel: \\(t\\) = 2.6, \\(p\\) &lt; 0.01). This means that both coefficients are good predictors of the fundamental frequency in this model. lm5 %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -259. 56.3 -4.60 8.25e- 6 ## 2 dB 6.17 0.761 8.11 8.74e-14 ## 3 vowelO -22.9 8.75 -2.62 9.59e- 3 As always, we also take a look at the goodness-of-fit criteria for the model and find that the two independent variables, vowel sound and volume, can describe approximately 26.6% of the variance in the fundamental frequency values. The \\(F\\)-test shows that our regression model describes the data more successfully than a model without predictors (\\(R^2\\) = 0.27, \\(F\\)[2, 175] = 33.0, \\(p\\) &lt; 0.001). lm5 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 2.74e- 1 ## 2 adj.r.squared 2.66e- 1 ## 3 sigma 4.28e+ 1 ## 4 statistic 3.30e+ 1 ## 5 p.value 6.81e-13 ## 6 df 2 e+ 0 ## 7 logLik -9.20e+ 2 ## 8 AIC 1.85e+ 3 ## 9 BIC 1.86e+ 3 ## 10 deviance 3.21e+ 5 ## 11 df.residual 1.75e+ 2 ## 12 nobs 1.78e+ 2 (Of course, under realistic conditions we would also check the assumptions about the residuals here!) 4.5.2 With Interaction Now we calculate the model including the interaction between the independent variables: lm6 &lt;- lm(f0 ~ dB * vowel, data = vlax) lm6 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -319. ## 2 dB 6.98 ## 3 vowelO 615. ## 4 dB:vowelO -8.34 The dummy coding remains the same, meaning the vowel “I” is treated as zero and the vowel “O” as one. The intercept is at -318.5 Hz. The influence of volume on the fundamental frequency is still positive at approximately 7 Hz. It is noticeable that the coefficient vowelO has changed significantly compared to the model without interaction, from -22.9 to +615.2 Hz. This makes sense when we look at the figure again (we have adjusted the x-axis so that it starts at zero): ggplot(vlax) + aes(x = dB, y = f0, col = vowel) + geom_point() + xlim(0, 90) + geom_smooth(method = &quot;lm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; If you trace the blue regression line to the y-axis, you’ll arrive at just under 300 Hz – this corresponds to the intercept (db = 0 and vowel = \"I\") plus the slope for vowelO. The same applies to the red line, which intersects the y-axis at a point not shown here, namely at approximately -311 Hz – this corresponds to the intercept plus the slope for dB. We can replicate these calculations as follows (or you can manually insert the regression coefficients into the formula, as shown above): k &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) b_1 &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_2 &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;vowelO&quot;) %&gt;% pull(estimate) b_3 &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;dB:vowelO&quot;) %&gt;% pull(estimate) # &quot;I&quot; at 0 dB k ## [1] -318.5 # &quot;I&quot; at 1 dB k + b_1 ## [1] -311.5 # &quot;O&quot; at 0 dB k + b_2 ## [1] 296.6 # &quot;O&quot; at 1 dB k + b_1 + b_2 + b_3 ## [1] 295.3 Once again, the predict() function can do the calculations for you: d6 &lt;- data.frame(dB = c(0, 1, 0, 1), vowel = c(&quot;I&quot;, &quot;I&quot;, &quot;O&quot;, &quot;O&quot;)) d6 %&lt;&gt;% mutate(estimated_f0 = predict(lm6, d6)) d6 ## dB vowel estimated_f0 ## 1 0 I -318.5 ## 2 1 I -311.5 ## 3 0 O 296.6 ## 4 1 O 295.3 If you now look at the difference between lines 1 and 2 in d6, you will see that the loudness for the vowel “I” is positively correlated with the fundamental frequency, because as the loudness increases (from 0 to 1 dB), the estimated fundamental frequency also increases. For the vowel “O”, however, the fundamental frequency decreases with increasing loudness (see lines 3 and 4 in d6), i.e., the correlation between loudness and fundamental frequency is negative for the vowel “O” – this also corresponds to the regression lines in the plot above and the correlation values \\(r\\) calculated as follows: vlax %&gt;% group_by(vowel) %&gt;% summarise(r = cor(dB, f0)) ## # A tibble: 2 × 2 ## vowel r ## &lt;fct&gt; &lt;dbl&gt; ## 1 I 0.591 ## 2 O -0.115 Now let’s look at the \\(t\\)-statistic, which shows whether our regression coefficients help to reliably estimate the fundamental frequency values: lm6 %&gt;% tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -319. 57.6 -5.53 1.17e- 7 ## 2 dB 6.98 0.779 8.96 4.93e-16 ## 3 vowelO 615. 192. 3.21 1.58e- 3 ## 4 dB:vowelO -8.34 2.50 -3.33 1.05e- 3 The slope for loudness (\\(t\\) = 9.0, \\(p\\) &lt; 0.001) and the slope for vowel (\\(t\\) = 3.2, \\(p\\) &lt; 0.01) differ significantly from zero and are therefore well-suited as predictors of fundamental frequency. Additionally, there was a significant interaction between the factors (\\(t\\) = 3.3, \\(p\\) &lt; 0.001). The \\(F\\)-statistic shows that the regression model with the two predictors and their interaction describes the data better than a mean model (\\(R^2\\) = 0.31, \\(F\\)[3, 174] = 27.0, \\(p\\) &lt; 0.001). Based on the \\(R^2\\) value, we see that the model now also describes more variance in the fundamental frequency values than the model without interaction; that is, the goodness-of-fit of the model has increased with the addition of the interaction. lm6 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 3.18e- 1 ## 2 adj.r.squared 3.06e- 1 ## 3 sigma 4.17e+ 1 ## 4 statistic 2.70e+ 1 ## 5 p.value 2.21e-14 ## 6 df 3 e+ 0 ## 7 logLik -9.14e+ 2 ## 8 AIC 1.84e+ 3 ## 9 BIC 1.85e+ 3 ## 10 deviance 3.02e+ 5 ## 11 df.residual 1.74e+ 2 ## 12 nobs 1.78e+ 2 For this model, we do not calculate post-hoc tests despite the significant interaction between the independent variables because one of the independent variables is continuous. Post-hoc tests with emmeans only make sense for multiple categorical variables. (Of course, under realistic conditions, we would also test the assumptions about the residuals here!) "],["mixed-linear-regression.html", "5 Mixed Linear Regression 5.1 Load Packages and Data 5.2 Mixed Models (LMERs): Introduction 5.3 Random Intercepts vs. Random Slopes 5.4 LMER in R 5.5 Convergence Problems and Simplifying the Model 5.6 Reporting Results 5.7 Quality Criteria for Mixed Models", " 5 Mixed Linear Regression 5.1 Load Packages and Data Please load the following packages and datasets: library(emmeans) library(lmerTest) ## Loading required package: lme4 ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step library(MuMIn) ## Registered S3 methods overwritten by &#39;MuMIn&#39;: ## method from ## nobs.multinom broom ## nobs.fitdistr broom library(tidyverse) library(magrittr) url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; df &lt;- read.table(file.path(url, &quot;int_new.txt&quot;), stringsAsFactors = T, header = T) %&gt;% mutate(subject = factor(subject, levels = paste0(&quot;S&quot;, 1:10))) %&gt;% as_tibble() 5.2 Mixed Models (LMERs): Introduction A Linear Mixed Effects Regression (LMER; also: Linear Mixed Model) is used to test whether a measured dependent variable is influenced by one or more independent variables. This type of model is called “mixed” because it can include both variables whose influence on the dependent variable is of interest to researchers (fixed effects) and variables whose influence is arbitrary and therefore uninteresting (random effects). Fixed effects in a mixed model can be categorical or continuous, while random effects are exclusively categorical. This extends simple and multiple linear regression, which used only fixed effects as predictors. In phonetics and linguistics, study participants and items or words are often used as random effects in LMERs. This is because we select a random sample of participants and/or words from a specific language and hope that the results will be generalizable to other participants and/or words from the same language. This random selection process introduces noise into our experiment, which we can then “remove” using LMER. While we expect fixed effects to have a predictable influence on the dependent variable, the influence of random effects is unpredictable, i.e., random. Let’s look at the data frame df, which we already know from previous chapters (it has been slightly modified for this chapter): df ## # A tibble: 120 × 5 ## db vowel gender subject word ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 100 a m S1 w1 ## 2 70 a m S2 w1 ## 3 90 a m S3 w1 ## 4 60 a m S4 w1 ## 5 80 a m S5 w1 ## 6 50 a f S6 w1 ## 7 40 a f S7 w1 ## 8 60 a f S8 w1 ## 9 30 a f S9 w1 ## 10 20 a f S10 w1 ## # ℹ 110 more rows Here we have the speakers in the subject column and the words produced in the word column. We are interested in whether the vowel sound (vowel) and gender (gender) have an influence on the volume (db). To answer this question, we will look at the data in a figure: ggplot(df) + aes(x = vowel, y = db, fill = gender) + geom_boxplot() We see (as in the previous chapter) that women produced vowels more quietly than men and that /i/ was produced somewhat quieter than /a/. It can also be assumed that there is an interaction between gender and vowel sound, since the effect of gender is more pronounced for /a/ than for /i/ (or vice versa: the effect of vowel sound is more pronounced for men than for women). However, we know that we recorded several participants and various words to investigate this question. We are not interested in the precise influence of the individuals or words on the volume; on the contrary, we want to derive a general statement about the participants and words that (theoretically) applies to all possible participants and words. Should there be significant volume differences between the individual participants and/or the individual words, we have caused noise through our random selection of participants and words. Using boxplots, we examine whether there are significant variations in decibel levels between the speakers and/or between the words. ggplot(df) + aes(x = subject, y = db) + geom_boxplot() ggplot(df) + aes(x = word, y = db) + geom_boxplot() Since there are sometimes large volume differences between individual speakers and between individual words, it is absolutely essential that we use random effects for these two variables in the mixed model we will construct for our research question. A second extremely important reason for random effects is that we frequently conduct so-called repeated measure experiments, in which, for example, several data points come from the same participant and/or words are repeated multiple times. This means that the data points are no longer independent of each other (for example, data points from the same participant will often be closer together than those from other participants). However, this is an important assumption in regression models, which we have not yet discussed. Therefore, when considering which statistical model might be suitable for the data and our research question, we must take into account whether the data points are independent of each other or not (in previous chapters, we sometimes ignored this assumption for didactic reasons). If the data points cannot be considered independent, a mixed model must be used, as random effects account for the dependencies between data points. One difference between the independence assumption and the other two assumptions you have encountered so far is that the independence assumption refers directly to the measured data, while the normality and variance assumptions refer to the residuals. In simple and multiple linear regression, an intercept and then a slope are estimated for each independent variable (per fixed effect). Mixed models also do this, but we can now allow the intercept and the slope(s) to differ for the levels of the random effects. To explain the concepts of random intercept and random slope, we will first focus on subject as a random effect. 5.3 Random Intercepts vs. Random Slopes 5.3.1 Random Intercepts You may recall that, using treatment coding, the vowel /a/ is interpreted as zero and the vowel /i/ as one. Similarly, the level f (female) of the variable gender is interpreted as zero and m (male) as one. The estimation of the mean volume of /a/ for women is therefore the intercept in our mixed model (just as it was previously in the multiple regression). Using the boxplots above, we observed that the participants differed significantly in terms of volume. It is now of interest to us whether participants of the same gender also differ in the mean volume of /a/ (the level of vowel processed in the “general” intercept). If so, we can use a random intercept for the variable subject, requiring that the intercept be allowed to vary from person to person (hence the alternative term varying intercepts). The following figure shows the decibel levels for the vowels /a/ and /i/, separated by participant. Participants S1-S5 are male, and participants S6-S10 are female. The orange crosses mark the mean decibel levels per person and vowel. If we now look at the mean /a/ levels per participant within the same row (the leftmost orange cross in each panel), there are significant differences, for example, between S1 and S4 or between S8 and S10. This indicates that it is advisable to calculate a random intercept for each speaker. ggplot(df) + aes(x = vowel, y = db) + geom_point() + facet_wrap(~subject, nrow = 2) + geom_point(data = df %&gt;% group_by(subject, vowel) %&gt;% summarise(db = mean(db)), color = &quot;orange&quot;, size = 3, stroke = 2, shape = 4) + geom_line(data = df %&gt;% group_by(subject, vowel) %&gt;% summarise(db = mean(db)), aes(x = vowel, y = db, group = subject), lty = &quot;dashed&quot;) ## `summarise()` has grouped output by &#39;subject&#39;. You can ## override using the `.groups` argument. ## `summarise()` has grouped output by &#39;subject&#39;. You can ## override using the `.groups` argument. In R notation, a random intercept is written as (1 | subject). Here, 1 represents intercept, and the formula essentially means “estimate speaker-specific intercepts.” In practice, however, a mixed model using such a random intercept doesn’t calculate one intercept per person, but rather the standard deviation of the subject intercepts from the estimated “general” intercept across all data points. This is important because calculating the mixed model would take an extremely long time and likely produce errors if it actually estimated one intercept per subject. 5.3.2 Random Slopes In the figure above, you can see the mean values for /a/ and /i/, as well as a dashed line connecting them. This dashed line represents, in effect, the ideal regression line for each participant. Since the lines vary considerably in steepness, we can infer that the individual participants have different slopes. For example, participant S7 makes hardly any difference in volume between /a/ and /i/ (the slope is approximately 0), while participant S6 produces /a/ significantly louder than /i/ (the slope is negative). Again, we are only comparing the slopes within each gender group, i.e., for S1-S5 and for S6-S10. Here, it seems sensible to also calculate speaker-specific random slopes because the effect of the vowel on volume differs for each participant. It is generally assumed that a random intercept should also be estimated when estimating random slopes. Therefore, the formula for random slopes is (1 + vowel | subject), meaning “estimate speaker-specific intercepts and speaker-specific slopes relative to the vowel.” However, the shorthand notation (vowel | subject) is often used instead, as it makes it clear that the function should calculate both random slopes and random intercepts. Here again, the deviation from the “general” slope is estimated, and not actually a slope per subject. If you really only want to estimate the random slope without the random intercept, the formula needs to be (0 | subject). You might now be wondering why the fixed effect gender seems to play a somewhat subordinate role here compared to vowel. Why, for example, are we not interested in (1 + gender | subject) or (1 + vowel + gender | subject)? This is because the levels of the variable gender do not vary per participant, as the participants here are either male or female, as the following table shows: table(df$subject, df$gender) ## ## f m ## S1 0 12 ## S2 0 12 ## S3 0 12 ## S4 0 12 ## S5 0 12 ## S6 12 0 ## S7 12 0 ## S8 12 0 ## S9 12 0 ## S10 12 0 Therefore, there is no variation here that we could factor out using the corresponding random slope. For the levels of the variable vowel, however, we have values for each participant (they produced both /a/ and /i/). This, in turn, means that the effect of vowel on volume can differ from participant to participant (the effect of gender on volume, however, cannot differ between participants). table(df$subject, df$vowel) ## ## a i ## S1 6 6 ## S2 6 6 ## S3 6 6 ## S4 6 6 ## S5 6 6 ## S6 6 6 ## S7 6 6 ## S8 6 6 ## S9 6 6 ## S10 6 6 5.3.3 Determining the Random Effects Structure for word Now that we know what the random effect looks like for subject, we’ll go through the same procedure for the variable word. This time, we’ll start by finding out if there are measurements for each word from both genders and from both vowels: table(df$word, df$gender) ## ## f m ## w1 10 10 ## w2 10 10 ## w3 10 10 ## w4 10 10 ## w5 10 10 ## w6 10 10 table(df$word, df$vowel) ## ## a i ## w1 20 0 ## w2 20 0 ## w3 20 0 ## w4 0 20 ## w5 0 20 ## w6 0 20 This seems to be the case for gender, but not for vowel; that is, the effect of gender on volume can vary from word to word. However, the effect of vowel on volume cannot vary from word to word because every word contained either /a/ or /i/. Therefore, the maximum random effects structure we could create for the variable word is (1 + gender | word) (both random intercept and random slope relative to gender). First, let’s see if we even need a random intercept for word (although the word-specific boxplots above already strongly suggest that we do). We’ll use a similar plot to the one before, allowing us to compare the mean volume levels for women per word (f was the level of the variable gender processed in the intercept). Of course, you could also calculate these mean levels using tidyverse functions. The words in the top row of the image contain /a/, those in the bottom row contain /i/. ggplot(df) + aes(x = gender, y = db) + geom_point() + facet_wrap(~word) + geom_point(data = df %&gt;% group_by(word, gender) %&gt;% summarise(db = mean(db)), color = &quot;orange&quot;, size = 3, stroke = 2, shape = 4) + geom_line(data = df %&gt;% group_by(word, gender) %&gt;% summarise(db = mean(db)), aes(x = gender, y = db, group = word), lty = &quot;dashed&quot;) ## `summarise()` has grouped output by &#39;word&#39;. You can ## override using the `.groups` argument. ## `summarise()` has grouped output by &#39;word&#39;. You can ## override using the `.groups` argument. The average loudness level for women (the leftmost orange cross in each panel) differs for various words. For example, the word w1 has a significantly lower average loudness level than w3 when produced by women. The average volume levels for women in words containing /i/ (w4-w6) also differ considerably. Therefore, calculating a random intercept for word is appropriate: (1 | word). Interestingly, the dashed lines connecting the orange crosses are almost parallel for the three words in each row in the figure. This means that the effect of gender on the words w1, w2, and w3 is the same (or at least very similar). The same applies to the words w4, w5, and w6, which hardly differ in the slope of their dashed lines. Thus, within a vowel group, the effect of gender on the different words is the same. This means that a word-specific random slope relative to gender (1 + gender | word) would not be appropriate for this dataset. (1 | word) remains the random effect structure for the variable word. 5.4 LMER in R The classic package used for LMERs is called lme4. Here, we’re using lmerTest instead, which is a wrapper for lme4. The function for calculating a mixed model is called lmer(). Our complete formula contains the usual first part for the fixed effects (including interaction, if needed), and then the random effects for subject and word. This time, we’ll look at the results of the mixed model using the summary() function. The function optionally takes the argument corr = F to suppress the display of a specific correlation table that isn’t relevant to our purposes. Further Information: Show LMER results Instead of summary(), you can also download the package broom.mixed, which provides the functions tidy(), augment() and glance() for LMERs. The function lmer() receives the formula we painstakingly developed, using the familiar fixed effects structure db ~ gender * vowel, followed by the random effects connected by plus signs. Additionally, the data frame is specified with the argument data. In this case, we also specify REML = F. REML stands for Restricted Maximum Likelihood. By giving the function the argument REML = F, a true maximum likelihood estimate of the desired parameters is now performed instead (more on this later). df.lmer &lt;- lmer(db ~ gender * vowel + (1 + vowel | subject) + (1 | word), data = df, REML = F) df.lmer %&gt;% summary(corr = F) ## Linear mixed model fit by maximum likelihood . ## t-tests use Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: ## db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## Data: df ## ## AIC BIC logLik -2*log(L) df.resid ## 850.7 875.8 -416.4 832.7 111 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.286 -0.506 0.018 0.496 2.023 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 215.8 14.69 ## voweli 110.0 10.49 -0.47 ## word (Intercept) 193.1 13.90 ## Residual 28.2 5.31 ## Number of obs: 120, groups: subject, 10; word, 6 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 48.17 10.41 12.49 4.63 ## genderm 40.19 9.39 9.53 4.28 ## voweli -10.01 12.35 7.93 -0.81 ## genderm:voweli -20.21 6.91 9.17 -2.92 ## Pr(&gt;|t|) ## (Intercept) 0.00053 *** ## genderm 0.00180 ** ## voweli 0.44140 ## genderm:voweli 0.01658 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here, we are first shown exactly what kind of model was calculated and which formula was used. This is followed by a list of measures for the model selection, namely AIC (Akaike information criterion), BIC (Bayesian information criterion), and logLik (logarithmic likelihood). These absolute values have no meaning; however, when compared with values from other models for the same data, lower values for AIC and BIC, as well as higher values for logLik, indicate a better fit of the model to the data. df.resid stands for residual degrees of freedom; this is the number of data points minus the number of estimated parameters. The data frame df has 120 observations (rows, data points), and nine parameters were estimated by our model: for the fixed effects: (Intercept), genderm, voweli, genderm:voweli for the random effects: subject (Intercept), subject voweli, word (Intercept), Residual, Corr for the speaker-specific slope by vowel. The usual summary statistics for the residuals follow. New to us is the table for random effects, while the table for fixed effects is familiar. We will examine these two tables in more detail below. 5.4.1 Fixed Effects The table for the fixed effects looks familiar. We see the estimate for the intercept and then the estimates for the slopes genderm, voweli, and the interaction genderm:voweli. The estimated average dB level for the /a/ of women is 48.2 dB. Men apparently speak significantly louder, as the slope for genderm is high and positive. This means that the /a/ of men is approximately \\(48.2 + 40.2 = 88.4\\) dB. The vowel /i/, on the other hand, is produced more quietly than /a/, as the slope is negative at -10 dB, meaning that the /i/ of women was produced at approximately \\(48.2 - 10.0 = 38.2\\) dB. Finally, we see that 20.2 dB must be subtracted for the /i/ of men, meaning the estimated mean for the /i/ of men is \\(48.2 + 40.2 + (-10.0) + (-20.2) = 58.2\\) dB. We are also given the standard errors and the results of the \\(t\\)-statistic, which tests whether the regression coefficients differ significantly from zero. According to this statistic, gender had a significant influence on loudness (\\(t\\)[9.5] = 4.3, \\(p\\) &lt; 0.01), and the interaction between gender and vowel was also significant (\\(t\\)[9.2] = 2.9, \\(p\\) &lt; 0.05). The vowel, however, had no significant influence on loudness. As you can see, the \\(t\\)-statistic is reported here along with the degrees of freedom from the df column, because the degrees of freedom are the defining parameter for the Student’s \\(t\\) distribution. The degrees of freedom are also an estimate here, which is why the values are often decimal. Since there is a significant interaction between the categorical independent variables, we will later perform post-hoc tests using emmeans. 5.4.2 Random Effects Now let’s take a closer look at the random effects. As mentioned earlier, instead of calculating an intercept and a slope for each participant, the standard deviation of the person-specific intercepts and slopes from the estimated “general” intercept and the estimated “general” slope is estimated. Each value in the Std.Dev column is therefore a parameter estimated by the mixed model. The standard deviation for the speaker-specific intercepts is 14.7 dB, meaning the speaker variation around the “general” intercept of 48.2 dB is \\(\\pm 14.7\\) dB, which is relatively large. We can apply the 68-95-99.7 rule here: 95% of the speaker-specific intercepts should lie within the range of \\(Intercept \\pm 2 \\cdot Std.Dev\\), i.e., between 77.6 dB and 18.8 dB. This large range of values shows that there were indeed large differences in the intercepts of the different test subjects. The speaker variation around the “general” slope for voweli of -10 dB is 10.5 dB, which is a very large standard deviation: this means that 95% of the speaker-specific slopes lie in the range of -31 dB to 11 dB, which in turn means that the vowel effect was extremely different for each speaker. This again justifies the calculation of the speaker-specific random slope. Additionally, the Corr column contains the correlation value \\(r\\) for the correlation between the speaker-specific random intercept and the speaker-specific random slope. Since the correlation here is negative, this means that subjects with a higher intercept have a steeper negative slope for /i/. So, if someone produced the /a/ particularly loudly, that person also produced a quieter /i/ (note: we are describing a correlation here, not causality!). The correlation Corr is considered another parameter that was estimated by the model. 5.5 Convergence Problems and Simplifying the Model When calculating LMERs, so-called convergence problems regularly occur. The most common error is the following: This error, roughly speaking, means that the desired model could not be estimated. This is usually due to the inclusion of unnecessarily complex random effect structures and/or too many independent variables and interactions overall. Therefore, you should carefully consider which formula you use for the mixed model; every fixed effect, every interaction, and every random effect should be meaningful for the data and the research question. Previously, we established that it would not be meaningful to estimate a random slope for gender given a word (1 + gender | word) for the data in df. We will do this anyway to demonstrate how to handle the resulting error. df.wrong &lt;- lmer(db ~ gender * vowel + (1 + vowel | subject) + (1 + gender | word), data = df, REML = F) ## boundary (singular) fit: see help(&#39;isSingular&#39;) df.wrong %&gt;% summary(corr = F) ## Linear mixed model fit by maximum likelihood . ## t-tests use Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: ## db ~ gender * vowel + (1 + vowel | subject) + (1 + gender | word) ## Data: df ## ## AIC BIC logLik -2*log(L) df.resid ## 854.6 885.3 -416.3 832.6 109 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2670 -0.5051 0.0023 0.4921 2.0442 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 215.7933 14.690 ## voweli 110.0413 10.490 -0.47 ## word (Intercept) 197.0661 14.038 ## genderm 0.0809 0.284 -1.00 ## Residual 28.1476 5.305 ## Number of obs: 120, groups: subject, 10; word, 6 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 48.17 10.48 12.26 4.60 ## genderm 40.19 9.39 9.53 4.28 ## voweli -10.01 12.46 7.78 -0.80 ## genderm:voweli -20.21 6.92 9.18 -2.92 ## Pr(&gt;|t|) ## (Intercept) 0.00058 *** ## genderm 0.00180 ** ## voweli 0.44562 ## genderm:voweli 0.01660 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) The introduction of the word-specific random slope has indeed created a convergence problem – and yet the result is still displayed. However, this result is not reliable and must not be reported under any circumstances. Let’s look at the random effect word. Here, a random intercept was estimated, for which then a standard deviation of 14 dB was estimated. So far, so good. But then we see that the random slope has an extremely small standard deviation of only 0.3 dB. This means that the word does not vary at all with the gender of the participants (we had previously established this using a plot). Nevertheless, the mixed model attempted to estimate the random slope and the correlation between the random intercept and the random slope – and failed. You can see this from the fact that \\(r = -1\\) (or when \\(r = 1\\)) in the Corr column, because a perfect correlation does not exist. The 1 or -1 indicates that the correlation could not be estimated. For such cases, there’s the step() function from the lmerTest package. This function examines all fixed and random effects structures in the model and calculates which ones are significant and thus contribute something essential to the model, and which ones are not. The variables that don’t contribute are eliminated. This leaves a model that only includes the statistically relevant variables. However, this does not mean that you can’t still keep variables in the model that aren’t significant (as long as the model still converges)! Here’s the result of step(): df.step &lt;- df.wrong %&gt;% step() ## Warning: the &#39;nobars&#39; function has moved to the reformulas package. Please update your imports, or ask an upstream package maintainter to do so. ## This warning is displayed once per session. ## Warning: the &#39;findbars&#39; function has moved to the reformulas package. Please update your imports, or ask an upstream package maintainter to do so. ## This warning is displayed once per session. ## boundary (singular) fit: see help(&#39;isSingular&#39;) df.step ## Backward reduced random-effect table: ## ## Eliminated npar logLik ## &lt;none&gt; 11 -416 ## gender in (1 + gender | word) 1 9 -416 ## vowel in (1 + vowel | subject) 0 7 -442 ## (1 | word) 0 8 -512 ## AIC LRT Df ## &lt;none&gt; 855 ## gender in (1 + gender | word) 851 0.1 2 ## vowel in (1 + vowel | subject) 897 50.6 2 ## (1 | word) 1040 191.0 1 ## Pr(&gt;Chisq) ## &lt;none&gt; ## gender in (1 + gender | word) 0.96 ## vowel in (1 + vowel | subject) 1e-11 *** ## (1 | word) &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Backward reduced fixed-effect table: ## Degrees of freedom method: Satterthwaite ## ## Eliminated Sum Sq Mean Sq NumDF DenDF ## gender:vowel 0 241 241 1 9.17 ## F value Pr(&gt;F) ## gender:vowel 8.55 0.017 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Model found: ## db ~ gender + vowel + (1 + vowel | subject) + (1 | word) + gender:vowel Occasionally, the function throws the above error multiple times, specifically when it calculates a new model with a modified formula and the error persists. First, we see the Backward reduced random-effect table in the results. Backward reduction is a process where the most complex model is tested first, and then the non-significant terms are gradually removed. Here, we see that the Random Slope gender in the Random Effect (1 + gender | word) is not significant and has therefore been removed. This leaves only the Random Intercept (1 | word). This was also re-evaluated in line 3 and deemed important. The Random Slope vowel in (1 + vowel | subject) can also remain. The same procedure is followed for the Fixed Effects. Finally, step() shows us which model it ultimately selected: db ~ gender + vowel + (1 + vowel | subject) + (1 | word) + gender:vowel Our fixed effects are written out here as gender + vowel + ... gender:vowel. Random intercept and slope for participants remain unchanged, and for the random factor word, only the intercept is now estimated. Thus, we obtain exactly the model we calculated previously. Using the get_model() function, applied to the result of the step() function, we can display the results of the simplified model: df.lmer.new &lt;- df.step %&gt;% get_model() df.lmer.new %&gt;% summary(corr = F) ## Linear mixed model fit by maximum likelihood . ## t-tests use Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: ## db ~ gender + vowel + (1 + vowel | subject) + (1 | word) + gender:vowel ## Data: df ## ## AIC BIC logLik -2*log(L) df.resid ## 850.7 875.8 -416.4 832.7 111 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.286 -0.506 0.018 0.496 2.023 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 215.8 14.69 ## voweli 110.0 10.49 -0.47 ## word (Intercept) 193.1 13.90 ## Residual 28.2 5.31 ## Number of obs: 120, groups: subject, 10; word, 6 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 48.17 10.41 12.49 4.63 ## genderm 40.19 9.39 9.53 4.28 ## voweli -10.01 12.35 7.93 -0.81 ## genderm:voweli -20.21 6.91 9.17 -2.92 ## Pr(&gt;|t|) ## (Intercept) 0.00053 *** ## genderm 0.00180 ** ## voweli 0.44140 ## genderm:voweli 0.01658 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.6 Reporting Results We know the results of the \\(t\\)-tests for the regression coefficients and have found that the interaction between the two categorical independent variables was significant. Therefore, for the sake of completeness, we perform the pairwise comparisons using emmeans: emmeans(df.lmer.new, pairwise ~ vowel | gender)$contrasts ## gender = f: ## contrast estimate SE df t.ratio p.value ## a - i 10.0 14.4 10 0.696 0.5024 ## ## gender = m: ## contrast estimate SE df t.ratio p.value ## a - i 30.2 14.4 10 2.101 0.0619 ## ## Degrees-of-freedom method: kenward-roger emmeans(df.lmer.new, pairwise ~ gender | vowel)$contrasts ## vowel = a: ## contrast estimate SE df t.ratio p.value ## f - m -40.2 10.10 10.9 -3.965 0.0023 ## ## vowel = i: ## contrast estimate SE df t.ratio p.value ## f - m -20.0 9.32 10.8 -2.144 0.0557 ## ## Degrees-of-freedom method: kenward-roger When reporting your results, describe the model in detail. In a scientific publication, also dedicate a few lines to describing the direction of the fixed effects (e.g., “The mixed model showed that /a/ was produced more loudly than /i/, especially by male participants”) and whether this aligns with your expectations/hypotheses. Here is an example of a short results report addressing the question: Is volume influenced by gender and vowel? A linear mixed-effects regression was performed with loudness in decibels as the dependent variable, as well as fixed effects for gender and vowel (including the interaction between them) and random effects for subjects and words. For subjects, both random intercept and random slope by vowel were estimated, and for words, only the random intercept. The model revealed a significant effect of gender (\\(t\\)[9.5] = 4.3, \\(p\\) &lt; 0.01) on loudness. There was also a significant interaction between gender and vowel (\\(t\\)[9.2] = 2.9, \\(p\\) &lt; 0.05). Post-hoc \\(t\\)-tests showed a significant difference between women and men for /a/ (\\(t\\)[10.8] = 4.0, \\(p\\) &lt; 0.01), but not for /i/. 5.7 Quality Criteria for Mixed Models 5.7.1 Marginal and Conditional \\(R^2\\) You may have noticed that the summary() function did not return either the \\(R^2\\) or the \\(F\\)-statistic, as it did previously with the lm() regressions. In the case of \\(R^2\\), this is because we now need to evaluate separately for fixed and random effects how much variance they each represent in the data. The MuMIn package provides the r.squaredGLMM() function, which we can apply to our mixed model: df.lmer %&gt;% r.squaredGLMM() ## R2m R2c ## [1,] 0.4586 0.9637 Consequently, we are now shown two \\(R^2\\) values. The first, R2m (which stands for marginal \\(R^2\\)), is the proportion of the variance in the measured volume values described by the fixed effects, in this case 46%. The second, R2c (for conditional \\(R^2\\)), is the proportion of the variance in the measured volume values described by the fixed effects and random effects together. With 96.4%, we have thus described virtually the entire variance in the data! (You will never see such high values in actual linguistic studies). From this, we can deduce that the random effects describe \\(96.4 - 45.9 = 50.5\\)% of the variance. Therefore, it was very important for the dataset df that we included the random effects in the model. 5.7.2 Likelihood Ratio Tests The model’s output lacked an indicator of model goodness-of-fit, such as the \\(F\\)-statistic in lm(), in addition to the \\(R^2\\) value. For LMERs, goodness-of-fit is evaluated using likelihood ratio tests. These tests compare the mixed model used with another mixed model in which a fixed or random effect has been eliminated. This allows us to assess whether omitting a factor decreases or maintains the model’s goodness-of-fit. At this point, we should examine the term likelihood, especially in contrast to probability. In a statistical context, probability is the probability of a result given the parameters (e.g., regression coefficients). The \\(p\\)-values describe this type of probability. Likelihood, on the other hand, is the likelihood of parameters given the data. How likely is the estimate for the intercept (here: 48.2 dB) for the data in df? Or how likely is the estimate for the slope for gender (40.2 dB) for the data in df? Mixed models perform these estimates according to the principle of maximum likelihood (after we have specified REML = F in lmer()), i.e., the goal of the mixed model is to find the regression coefficients that are most likely for the given data. The output of a mixed model using lmer() includes three values: AIC, BIC, and log likelihood. The latter is the logarithm of the maximized likelihood; this value is always negative, but the closer it is to zero, the better the model. The likelihood ratio test compares the log likelihoods of two mixed models and determines whether the chosen model describes the data significantly better than a model that omits a variable. Here, we compare our model df.lmer with several other models; first, with a model that omits the variable gender. df.gender &lt;- lmer(db ~ vowel + (1 + vowel | subject) + (1 | word), data = df, REML = F) The anova() function tests the comparison of the models for significance using a \\(\\chi^2\\)-test (pronounced: chi /kai/ squared). “Anova” actually stands for analysis of variance; however, the default of the anova() function is the \\(\\chi^2\\)-test, which is performed by the argument test = \"Chisq\". Otherwise, this function only receives the names of the two models as arguments. The \\(\\chi^2\\)-test checks whether the logarithmic likelihood of the two models differs significantly. anova(df.lmer, df.gender) ## Data: df ## Models: ## df.gender: db ~ vowel + (1 + vowel | subject) + (1 | word) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.gender 7 857 877 -422 843 ## df.lmer 9 851 876 -416 833 10.6 2 ## Pr(&gt;Chisq) ## df.gender ## df.lmer 0.005 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results of this test show the two models that were compared. The first column of the table, npar, shows the number of parameters estimated for the model. Following this are the information criteria AIC, BIC, logLik, and deviance, which we already know from the results of lmer(). The top row shows the values for the restrictive model, and the bottom row shows those for our model. For our model, the \\(\\chi^2\\)-value is also given in the Chisq column. The \\(\\chi^2\\) distribution is described by the parameter Df (degrees of freedom). The degrees of freedom here represent the number of regression coefficients that were not estimated in the restrictive model (here: the fixed effect for gender and the interaction between gender and vowel; also calculable from the difference between the two values in the npar column). Finally, the column Pr(&gt;Chisq) contains the \\(p\\)-value, which was read from the \\(\\chi^2\\) distribution with two degrees of freedom for the value 10.59. Since the \\(p\\)-value is below 0.05, the two models differ significantly; because the log likelihood for our model is higher than that of the restrictive model (and AIC and BIC are lower for our model than for the restrictive model), our model fits the data better than a model without the variable gender. The formula for the \\(\\chi^2\\)-value is: \\[ \\begin{aligned} LR &amp;= -2ln \\cdot \\left( \\frac{L_{m_1}}{L_{m_2}} \\right) \\\\ &amp;= 2 \\cdot (log(L_{m_2}) - log(L_{m_1})) \\end{aligned} \\] \\(LR\\) here stands for likelihood ratio, which is why the formula also contains a division: the likelihood \\(L\\) for the restrictive model \\(m_1\\) divided by the likelihood for our full model \\(m_2\\). \\(ln\\) stands for the natural logarithm. However, this formula can be rearranged so that instead of the unknown likelihoods, we can use our log likelihoods from the result of lmer(). For the likelihood ratio test above, the \\(\\chi^2\\)-value can therefore be calculated manually as follows: 2 * (-416.36 - (-421.65)) ## [1] 10.58 Further Information: \\(\\chi^2\\) distribution Just as you have already learned about the normal, \\(t\\), and \\(F\\) distributions, the functions dchisq(), pchisq(), and qchisq() allow you to work with the \\(\\chi^2\\) distribution yourself. You can, if desired, plot a \\(\\chi^2\\) distribution with the desired degrees of freedom or calculate the \\(p\\)-value for a specific \\(\\chi^2\\) value. See also: ?dchisq Finally, we report the results of the likelihood ratio test: A likelihood ratio test showed that the chosen model provided better estimates for the model parameters than a comparable mixed model without the variable gender (\\(\\chi^2\\)[2] = 10.6, \\(p\\) &lt; 0.01)… Now we can do the same for some further comparisons: df.vowel &lt;- lmer(db ~ gender + (1 | subject) + (1 | word), data = df, REML = F) anova(df.lmer, df.vowel) ## Data: df ## Models: ## df.vowel: db ~ gender + (1 | subject) + (1 | word) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.vowel 5 939 953 -465 929 ## df.lmer 9 851 876 -416 833 96.7 4 ## Pr(&gt;Chisq) ## df.vowel ## df.lmer &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 …and as a comparable mixed model without the variable vowel (\\(\\chi^2\\)[4] = 96.7, \\(p\\) &lt; 0.001)… df.subject &lt;- lmer(db ~ gender * vowel + (1 | word), data = df, REML = F) anova(df.lmer, df.subject) ## Data: df ## Models: ## df.subject: db ~ gender * vowel + (1 | word) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.subject 6 1018 1035 -503 1006 ## df.lmer 9 851 876 -416 833 174 3 ## Pr(&gt;Chisq) ## df.subject ## df.lmer &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 …and as a comparable mixed model without the variable subject (\\(\\chi^2\\)[3] = 173.6, \\(p\\) &lt; 0.001)… df.word &lt;- lmer(db ~ gender * vowel + (1 + vowel | subject), data = df, REML = F) anova(df.lmer, df.word) ## Data: df ## Models: ## df.word: db ~ gender * vowel + (1 + vowel | subject) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.word 8 1040 1062 -512 1024 ## df.lmer 9 851 876 -416 833 191 1 ## Pr(&gt;Chisq) ## df.word ## df.lmer &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 … and finally as a comparable mixed model without the variable word (\\(\\chi^2\\)[1] = 191.0, \\(p\\) &lt; 0.001). "],["logistic-regression.html", "6 Logistic Regression 6.1 Load Packages and Data 6.2 From Linear to Logistic Regression 6.3 The Sigmoid Function and Tipping Point 6.4 Tipping Points in Perceptual Studies 6.5 Categorical Independent Factor", " 6 Logistic Regression 6.1 Load Packages and Data Please load the following packages and data frames: library(magrittr) library(tidyverse) url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; ovowel &lt;- read.table(file.path(url, &quot;ovokal.txt&quot;)) %&gt;% as_tibble() %&gt;% rename(vowel = Vokal, year = Jahr, subject = Vpn) %&gt;% mutate(vowel = ifelse(vowel == &quot;hoch&quot;, &quot;high&quot;, &quot;low&quot;)) pvp &lt;- read.table(file.path(url, &quot;pvp.txt&quot;)) %&gt;% as_tibble() %&gt;% rename(response = Urteil) sz &lt;- read.table(file.path(url, &quot;sz.txt&quot;)) %&gt;% as_tibble() %&gt;% rename(fricative = Frikativ, dialect = Dialekt) 6.2 From Linear to Logistic Regression Logistic regression (just like linear regression) is a statistical test that examines whether a dependent variable is influenced by an independent factor. In contrast to the linear regression, the dependent variable in a logistic regression is always categorical and binary, while the independent variable can be either numeric (continuous) or categorical. Logistic regression allows us to estimate the probability of a particular value, assuming a relationship exists between the dependent and independent variables. Examples: To what extent is the vocalization of a final /l/ in English (feel vs. ‘feeu’) influenced by dialect? Dependent variable: Vocalization (categorical with two levels: yes, no) Independent variable: Dialect (categorical with two or more levels) Is “passt” more likely to be produced with /ʃ/ in Augsburg compared to Munich? Dependent variable: Fricative (categorical with two levels: /s/, /ʃ/) Independent variable: Dialect (categorical with two levels: Augsburg, Munich) The vowel /a/ in /lam/ is synthesized with different durations and played back to listeners. Do the participants hear “lahm” (long /a:/) more often than “Lamm” (short /a/) as the vowel duration increases? Dependent variable: Vowel (categorical with two levels: /a/, /a:/) Independent variable: Duration (continuous) Since the dependent variable in logistic regression is always a factor with two levels, these levels can also be coded as 1 and 0, and we can ask what the probability \\(P\\) is that the dependent variable \\(y\\) takes the value 1 based on the given data: \\(P(y = 1)\\). Similarly, we can ask for the probability \\(Q\\) that \\(y\\) takes the value 0: \\(1 - P(y = 1)\\). For the third example above, this would mean the following: \\(P\\): Probability that subjects hear “lahm” with increasing vowel duration (“success,” because based on our knowledge or previous findings, e.g., from other experiments, we assume that subjects should hear “lahm” with increasing vowel duration) \\(Q\\): Probability that subjects hear “Lamm” with increasing vowel duration (“failure,” because again, based on our previous knowledge of this phenomenon, we assume that it would be strange if subjects heard “Lamm” with increasing vowel duration) The division (ratio) of \\(P\\) and \\(Q\\) is called the odds (probability of winning): \\(Odds = \\frac{P(y = 1)}{1 - P(y = 1)} = \\frac{P}{Q}\\) The odds of winning always lie within a range of 0 to infinity. One might consider simply using the odds as the dependent variable in a linear regression, since it’s no longer a categorical, binary dependent variable. The problem is that lm() doesn’t know that the odds can only take values from zero to infinity and would therefore also predict values outside this range. Furthermore, the ratio of \\(P\\) to \\(Q\\) says nothing about how many observations were used in calculating this ratio (the more observations, the more meaningful the calculated odds). So we need a function that transforms the odds into something that, firstly, falls within the range \\(\\pm\\)infinity and, secondly, weights the proportions based on the number of observations. This function is generally called a link function and, in the case of logistic regression, is the logit transformation of the odds. The logit is the logarithm of the odds of winning and is therefore also referred to as log odds: \\(log(\\frac{P}{Q})\\) 6.2.1 An Example for \\(P\\), \\(Q\\), and Logit Between 1950 and 2005, words like lost in an aristocratic form of English (Received Pronunciation) were increasingly produced low vowel /lɔst/ instead of with the high vowel /lost/. We have data in the ovowel data frame to support this hypothesis: head(ovowel) ## # A tibble: 6 × 3 ## year vowel subject ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1950 high S1 ## 2 1950 high S2 ## 3 1950 high S3 ## 4 1950 high S4 ## 5 1950 high S5 ## 6 1950 high S6 Our research question is: Is the pronunciation of the vowel (high vs. low = dependent variable) influenced by the year (1950…2005 = independent numerical variable)? We want to calculate \\(P\\) (the probability that the vowel was low) and \\(Q\\) (the probability that the vowel was high) for each year. Based on our current understanding, the direction of change in the pronunciation of the vowel is from high to low, so we define it as “success” if the vowel was produced low and as “failure” if it was produced high. As a first step in calculating \\(P\\) and \\(Q\\), we code low and high pronunciation as 1 and 0, respectively, as TRUE and FALSE: ovowel %&lt;&gt;% mutate(success = ifelse(vowel == &quot;low&quot;, TRUE, FALSE), failure = !success) head(ovowel) ## # A tibble: 6 × 5 ## year vowel subject success failure ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1950 high S1 FALSE TRUE ## 2 1950 high S2 FALSE TRUE ## 3 1950 high S3 FALSE TRUE ## 4 1950 high S4 FALSE TRUE ## 5 1950 high S5 FALSE TRUE ## 6 1950 high S6 FALSE TRUE Then we take the first year, 1950, and calculate \\(P\\) and \\(Q\\) by counting how many “successes” and “failures” we have for that year: P &lt;- ovowel %&gt;% filter(year == 1950) %&gt;% summarise(P = sum(success)) %&gt;% pull(P) P ## [1] 5 Q &lt;- ovowel %&gt;% filter(year == 1950) %&gt;% summarise(Q = sum(failure)) %&gt;% pull(Q) Q ## [1] 30 This means that in 1950, the vowel /o/ in words like lost was only produced 5 times as a low vowel, but 30 times as a high vowel. We would have to do this for every level of the independent variable (for every year)… But that would be very cumbersome. So we group the data frame by year: df &lt;- ovowel %&gt;% group_by(year) %&gt;% summarise(P = sum(success), Q = sum(failure)) df ## # A tibble: 6 × 3 ## year P Q ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1950 5 30 ## 2 1960 21 18 ## 3 1971 26 15 ## 4 1980 20 13 ## 5 1993 32 4 ## 6 2005 34 2 Using \\(P\\) and \\(Q\\) we can now calculate the log odds (the logit): df$log_odds &lt;- log(df$P/df$Q) df ## # A tibble: 6 × 4 ## year P Q log_odds ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1950 5 30 -1.79 ## 2 1960 21 18 0.154 ## 3 1971 26 15 0.550 ## 4 1980 20 13 0.431 ## 5 1993 32 4 2.08 ## 6 2005 34 2 2.83 Let’s look at the distribution of log odds over the decades: ggplot(df) + aes(x = year, y = log_odds) + geom_point() It is these log odds that we will use to construct a regression line using logistic regression. This regression line is defined in the same way as the linear regression line, but it estimates the log odds: \\(log(\\frac{P}{Q}) = bx + k\\) Once again, \\(b\\) is the slope \\(x\\) is a value on the x-axis \\(k\\) is the y-intercept Here, however, we cannot calculate \\(b\\) and \\(k\\) as easily as in linear regression; that is, we have them estimated directly. 6.2.2 The Logistic Regression Line For linear regression, we estimated the regression coefficients using the lm() function, which employs the least squares method. The logistic regression line, on the other hand, is approximated using the maximum likelihood method, which ensures that the estimated data points of the logistic model are as close as possible to the actual values. To estimate the regression coefficients, we use the function glm(), which stands for Generalized Linear Model. In addition to the formula y ~ x and the data frame, the function receives the argument family = binomial, which tells the function to perform the logit transformation. The dependent variable must be a factor; if necessary, you must convert the variable into a factor using as.factor(): class(ovowel$vowel) # not a factor ## [1] &quot;character&quot; lreg &lt;- glm(as.factor(vowel) ~ year, family = binomial, data = ovowel) We’ll look at the summary of this model later. First, we’ll show an alternative to the above application of glm() to the original data frame. glm() can also be executed on \\(P\\) and \\(Q\\) from the combined data frame df by concatenating \\(P\\) and \\(Q\\) using cbind() and using them as dependent variables: lreg2 &lt;- glm(cbind(P, Q) ~ year, family = binomial, data = df) We can use coef() to display the regression coefficients: coefs &lt;- coef(lreg) coefs ## (Intercept) year ## -138.11742 0.07026 # or with $coefficients lreg$coefficients ## (Intercept) year ## -138.11742 0.07026 Using these parameters, the straight regression line can be superimposed onto the data in the logit space with either geom_smooth() (which receives method = \"glm\" as an argument) or geom_abline() (with the estimated coefs). # with geom_smooth(): ggplot(df) + aes(x = year, y = log_odds) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # with geom_abline(): ggplot(df) + aes(x = year, y = log_odds) + geom_point() + geom_abline(intercept = coefs[1], slope = coefs[2], color = &quot;blue&quot;) The values estimated by logistic regression are the log odds. We can again use the predict() function to display the estimated log odds: log_odds_estimate &lt;- predict(lreg) log_odds_estimate ## 1 2 3 4 5 6 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 7 8 9 10 11 12 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 13 14 15 16 17 18 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 19 20 21 22 23 24 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 25 26 27 28 29 30 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 31 32 33 34 35 36 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -0.4017 ## 37 38 39 40 41 42 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 43 44 45 46 47 48 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 49 50 51 52 53 54 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 55 56 57 58 59 60 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 61 62 63 64 65 66 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 67 68 69 70 71 72 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 73 74 75 76 77 78 ## -0.4017 -0.4017 0.3712 0.3712 0.3712 0.3712 ## 79 80 81 82 83 84 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 85 86 87 88 89 90 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 91 92 93 94 95 96 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 97 98 99 100 101 102 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 103 104 105 106 107 108 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 109 110 111 112 113 114 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 115 116 117 118 119 120 ## 0.3712 1.0036 1.0036 1.0036 1.0036 1.0036 ## 121 122 123 124 125 126 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 127 128 129 130 131 132 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 133 134 135 136 137 138 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 139 140 141 142 143 144 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 145 146 147 148 149 150 ## 1.0036 1.0036 1.0036 1.0036 1.9170 1.9170 ## 151 152 153 154 155 156 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 157 158 159 160 161 162 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 163 164 165 166 167 168 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 169 170 171 172 173 174 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 175 176 177 178 179 180 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 181 182 183 184 185 186 ## 1.9170 1.9170 1.9170 1.9170 2.7601 2.7601 ## 187 188 189 190 191 192 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 193 194 195 196 197 198 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 199 200 201 202 203 204 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 205 206 207 208 209 210 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 211 212 213 214 215 216 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 217 218 219 220 ## 2.7601 2.7601 2.7601 2.7601 The output of predict() in this case consists of 220 numbers, one number per row in the original data frame ovowel. As you can see, the estimated log odds are repeated. This is because one log odd value is calculated for each level (or value) of the independent variable; in this case, there are six unique log odd values, one for each year: unique(log_odds_estimate) ## [1] -1.1043 -0.4017 0.3712 1.0036 1.9170 2.7601 We can plot these predicted values in red on our plot from above and find that the predicted values lie exactly on the regression line (we use geom_abline() here): ggplot(df) + aes(x = year, y = log_odds) + geom_point() + geom_abline(intercept = coefs[1], slope = coefs[2], color = &quot;blue&quot;) + geom_point(data = data.frame(x = unique(ovowel$year), y = unique(log_odds_estimate)), mapping = aes(x, y), color = &quot;red&quot;) Just like with linear regression, we can also use predict() to predict the log odds values for x-values that are not present in the original dataset. For example, if we want to estimate the logit values for the years 2000 to 2020, it works as follows: predict(lreg, data.frame(year = 2000:2020)) ## 1 2 3 4 5 6 7 8 9 ## 2.409 2.479 2.549 2.620 2.690 2.760 2.830 2.901 2.971 ## 10 11 12 13 14 15 16 17 18 ## 3.041 3.111 3.182 3.252 3.322 3.393 3.463 3.533 3.603 ## 19 20 21 ## 3.674 3.744 3.814 6.2.3 Regression with glm() The result of the function glm() is an object of the classes “glm” and “lm”: class(lreg) ## [1] &quot;glm&quot; &quot;lm&quot; We apply the summary() function to the result of the logistic regression lreg and look at the result again line by line: summary(lreg) ## ## Call: ## glm(formula = as.factor(vowel) ~ year, family = binomial, data = ovowel) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -138.1174 20.9988 -6.58 4.8e-11 *** ## year 0.0703 0.0107 6.59 4.3e-11 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 290.57 on 219 degrees of freedom ## Residual deviance: 229.45 on 218 degrees of freedom ## AIC: 233.5 ## ## Number of Fisher Scoring iterations: 4 The summary begins again with the call, i.e., the function that was used. 6.2.3.1 Coefficients The table of regression coefficients follows: The first row of this table contains the values for the intercept, the second for slope. The first column again shows the estimates for the regression coefficients, which were determined using a maximum likelihood method. The second column shows the standard error, which describes how reliable the estimates are (the smaller the better). A Wald test was performed on the two estimates, which checks whether the estimates differ significantly from zero. The result of this test is the \\(z\\)-value, which can also be calculated by dividing the estimate by the standard error. We are particularly interested in the second row, whose \\(z\\)-value and \\(p\\)-value show whether the independent variable year contributes significantly to explaining the log odds values. If the \\(p\\)-value, which is in the fourth column, is less than 0.05 (see also the significance level asterisks), then the coefficient differs significantly from zero. In the case of the dependent variable, we see that the \\(p\\)-value is less than 0.001, i.e., the variable is a good predictor for the log odds. By default, a statement about the dispersion parameter is printed after the coefficients table. We can ignore this. 6.2.3.2 Deviances and AIC The following two lines contain the null deviance and the residual deviance as well as the AIC (Akaike Information Criterion): The null deviance describes how well a model without independent variables would explain the data. A model without independent variables is characterized solely by the intercept. On its own, the null deviance is difficult to interpret. Therefore, the residual deviance is listed directly below it, describing how well our full model explains the data. The difference between null and residual deviance reveals how helpful our independent variable is in the model. The degrees of freedom are calculated by subtracting the number of parameters in the model from the number of observations in the data frame. With null deviance, there is only one parameter (the intercept), while with residual deviance, there are two (the intercept and the independent variable). The smaller the deviations (i.e., the discrepancies between the actual and the estimated values), the better. AIC stands for Akaike Information Criterion and is particularly helpful when comparing different regression models for the same dataset (for example, if you were to calculate another model with more than one independent variable for ovowel). The smaller the AIC, the better the model describes the variance in the data. Since we only have the one model here, AIC is irrelevant for our purposes. 6.2.3.3 Fisher Scoring Iterations In logistic regression, an iterative algorithm calculates the regression parameters, and the Fisher scoring iterations indicate how many iterations were required. This is also irrelevant for our purposes. 6.2.3.4 Deviance Residuals The deviance residuals are the differences between the empirically observed and estimated values. They are not shown in the result of the logistic regression, but we can look at them using resid() and compute the usual summary statistics for them with summary(): summary(resid(lreg)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.3755 -0.7566 0.3503 0.0586 0.7903 1.6677 6.2.4 The \\(\\chi^2\\)-Test &amp; Reporting Results For the linear regression, the \\(F\\)-test was our test statistic. Instead, we perform a \\(\\chi^2\\)-test for the logistic regression, which checks whether our model is a better fit for our data than an intercept-only model. You already know this test from the mixed models; it compares to models to each other. Since we already saw in the coefficients table above that the Wald test was significant for our independent variable, it is likely that our full model is better than a model without independent variables. We once again use the anova() function for this: anova(lreg) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: as.factor(vowel) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 219 291 ## year 1 61.1 218 230 5.4e-15 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The result of the \\(\\chi^2\\)-test has two rows: one for the null model (intercept-only) and the other for the model with the independent variable year. The degrees of freedom for null and residual deviation, which are reported in the column Resid. Dev., are found in the column Resid. Df. These values were reported in abbreviated form in the summary of the logistic model. From the \\(\\chi^2\\)-test result, we are particularly interested in the value in the column Pr(&gt;Chi), which contains the \\(p\\)-value. If this value is less than 0.05 (see also the significance asterisks), then the model with the variable year is a better fit for the data than the null model. Our initial research question was: Is the pronunciation of the vowel (high vs. low) influenced by the year? We can now report: Year had a significant influence on the proportion of ‘lost’ with low/high vowel (\\(\\chi^2\\)[1] = 61.1, \\(p\\) &lt; 0.001). 6.3 The Sigmoid Function and Tipping Point We presented the results of a logistic regression above in the logit space. However, one can also use the y-intercept and the slope to plot proportions on the y-axis instead of log odds. In this case, the regression line is no longer straight, but sigmoidal (S-shaped). The formula for the sigmoid function is: \\(f(x) = \\frac{e^{bx+k}}{1 + e^{bx+k}}\\) In this formula, \\(e\\) is the exponential function, \\(b\\) and \\(k\\) are the slope and the intercept, respectively. The larger the slope \\(b\\) is (in the figure: 1, 0.5, 0.25), the steeper the sigmoid curve (black, red, green) becomes: When the slope is zero, the y-axis is a straight line with a y-value of 0.5. Changing the y-intercept \\(k\\) (0, 1, -1 in the plot) when the slope is \\(b = 0\\) shifts the straight horizontal line up or down (black, red, green): 6.3.1 The Tipping Point The tipping point is the point at which the sigmoid curve is steepest. At this point, the value on the y-axis (the proportion) is always 0.5 (shown below as a horizontal line). The x-value of the tipping point is calculated using \\(\\frac{-k}{b}\\). For \\(k = 4\\) and \\(b = 0.8\\), for example, this would be \\(-4/0.8 = -5\\) (dashed line): 6.3.2 Plotting Proportions For our example above, we now want to plot proportions and then fit a sigmoidal regression curve to our data. We take our aggregated data frame df and calculate the proportion of \\(P\\) (the proportion of “successes”) per year: df$proportions &lt;- df$P / (df$P + df$Q) df ## # A tibble: 6 × 5 ## year P Q log_odds proportions ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1950 5 30 -1.79 0.143 ## 2 1960 21 18 0.154 0.538 ## 3 1971 26 15 0.550 0.634 ## 4 1980 20 13 0.431 0.606 ## 5 1993 32 4 2.08 0.889 ## 6 2005 34 2 2.83 0.944 For the year 1950, the proportion of “successes” (where the vowel /o/ was produced low) is 14.3%, for the year 1960 it is already 53.8%, and so on. We can now plot these proportions stored in the newly created column df$proportions and then use geom_smooth() to draw a sigmoidal regression line through the data. To this end, we use the arguments method = \"glm\" (generalized linear model), se = F (don’t show standard error), and additionally method.args = list(family = \"quasibinomial\") so that the function knows that we’re plotting proportions. ggplot(df) + aes(x = year, y = proportions) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F, method.args = list(family = &quot;quasibinomial&quot;)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; In this plot, the complete “S” of the sigmoidal curve is not visible because our x-axis is limited. However, we can easily calculate further proportion values using predict(). As we saw earlier, predict() returns the log odds, not the proportions. We obtain the proportions by using the argument type = \"response\" in predict(): predict(lreg, type = &quot;response&quot;) ## 1 2 3 4 5 6 7 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 8 9 10 11 12 13 14 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 15 16 17 18 19 20 21 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 22 23 24 25 26 27 28 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 29 30 31 32 33 34 35 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 36 37 38 39 40 41 42 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 43 44 45 46 47 48 49 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 50 51 52 53 54 55 56 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 57 58 59 60 61 62 63 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 64 65 66 67 68 69 70 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 71 72 73 74 75 76 77 ## 0.4009 0.4009 0.4009 0.4009 0.5917 0.5917 0.5917 ## 78 79 80 81 82 83 84 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 85 86 87 88 89 90 91 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 92 93 94 95 96 97 98 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 99 100 101 102 103 104 105 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 106 107 108 109 110 111 112 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 113 114 115 116 117 118 119 ## 0.5917 0.5917 0.5917 0.7318 0.7318 0.7318 0.7318 ## 120 121 122 123 124 125 126 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 127 128 129 130 131 132 133 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 134 135 136 137 138 139 140 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 141 142 143 144 145 146 147 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 148 149 150 151 152 153 154 ## 0.7318 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 155 156 157 158 159 160 161 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 162 163 164 165 166 167 168 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 169 170 171 172 173 174 175 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 176 177 178 179 180 181 182 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 183 184 185 186 187 188 189 ## 0.8718 0.8718 0.9405 0.9405 0.9405 0.9405 0.9405 ## 190 191 192 193 194 195 196 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 197 198 199 200 201 202 203 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 204 205 206 207 208 209 210 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 211 212 213 214 215 216 217 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 218 219 220 ## 0.9405 0.9405 0.9405 These are now the estimated values for all 220 observations in the original data frame. We now want some estimates for the years before 1950 and after 2010. So we also give the predict() function a data frame with the desired years: more_props &lt;- predict(lreg, data.frame(year = c(1910, 1920, 1930, 1940, 2020, 2030)), type = &quot;response&quot;) more_props ## 1 2 3 4 5 6 ## 0.01955 0.03871 0.07519 0.14101 0.97842 0.98919 We will now create a data frame containing only the year and proportions, using the original data frame and the values just estimated: df_new &lt;- data.frame(year = c(df$year, 1910, 1920, 1930, 1940, 2020, 2030), proportions = c(df$proportions, more_props)) ggplot(df_new) + aes(x = year, y = proportions) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F, method.args = list(family = &quot;quasibinomial&quot;)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; We can also calculate the tipping point for this data, using the coefs already stored above: -coefs[1] / coefs[2] ## (Intercept) ## 1966 According to our model, the year in which the pronunciation of /o/ in the Received Pronunciation changes from “high” to “low” is approximately 1965. 6.4 Tipping Points in Perceptual Studies Tipping points are frequently used in perception tests, which are constructed as follows: We synthesized an 11-step continuum between /pUp/ and /pYp/. Phonetically, the difference between /U/ and /Y/ is the second formant, which is low for /U/ and high for /Y/. We gradually varied this F2 value in the continuum over 11 steps. The first and last tokens in this continuum sound very clearly like PUPP or PÜPP, but in between, it can be difficult for listeners to distinguish between PUPP and PÜPP. Each token from the continuum was then played to several German participants in randomized order, and the participant had to decide whether it was PUPP or PÜPP. We are interested to find out at which F2 value the participants’ perception switches from PUPP to PÜPP. In other words, we are interested in the tipping point. We have stored data from such a perception experiment in the data frame pvp: head(pvp) ## # A tibble: 6 × 3 ## Vpn F2 response ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 VP18 1239 Y ## 2 VP18 1088 Y ## 3 VP18 803 U ## 4 VP18 956 U ## 5 VP18 1328 Y ## 6 VP18 861 U unique(pvp$response) ## [1] &quot;Y&quot; &quot;U&quot; unique(pvp$F2) ## [1] 1239 1088 803 956 1328 861 989 1121 808 1310 ## [11] 1436 We expect that, with increasing F2 levels, subjects will be more likely to hear /Y/ than /U/, so we code the response /Y/ as success and /U/ as failure: pvp %&lt;&gt;% mutate(success = ifelse(response == &quot;Y&quot;, T, F), failure = !success) head(pvp) ## # A tibble: 6 × 5 ## Vpn F2 response success failure ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 VP18 1239 Y TRUE FALSE ## 2 VP18 1088 Y TRUE FALSE ## 3 VP18 803 U FALSE TRUE ## 4 VP18 956 U FALSE TRUE ## 5 VP18 1328 Y TRUE FALSE ## 6 VP18 861 U FALSE TRUE For the steps of the F2 continuum, we calculate \\(P\\) and \\(Q\\): df &lt;- pvp %&gt;% group_by(F2) %&gt;% summarise(P = sum(success), Q = sum(failure)) df ## # A tibble: 11 × 3 ## F2 P Q ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 803 0 10 ## 2 808 0 10 ## 3 861 0 10 ## 4 956 0 10 ## 5 989 0 10 ## 6 1088 2 8 ## 7 1121 4 6 ## 8 1239 9 1 ## 9 1310 9 1 ## 10 1328 10 0 ## 11 1436 10 0 We then calculate the proportions of \\(P\\) and \\(Q\\) and plot the sigmoidal regression line in the data: df$proportions &lt;- df$P / (df$P + df$Q) df ## # A tibble: 11 × 4 ## F2 P Q proportions ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 803 0 10 0 ## 2 808 0 10 0 ## 3 861 0 10 0 ## 4 956 0 10 0 ## 5 989 0 10 0 ## 6 1088 2 8 0.2 ## 7 1121 4 6 0.4 ## 8 1239 9 1 0.9 ## 9 1310 9 1 0.9 ## 10 1328 10 0 1 ## 11 1436 10 0 1 ggplot(df) + aes(x = F2, y = proportions) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F, method.args = list(family = &quot;quasibinomial&quot;)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; To determine the tipping point of this sigmoid curve, we calculate the Generalized Linear Model: pvp.glm &lt;- glm(as.factor(response) ~ F2, family = binomial, data = pvp) Using the estimated regression coefficients, we can now calculate the subjects’ perceptual tipping point: coefs &lt;- coef(pvp.glm) -coefs[1] / coefs[2] ## (Intercept) ## 1151 This means that above an F2 value of approximately 1151 Hz, the test subjects hear “PÜPP” rather than “PUPP”. Finally, we want to determine whether the test subjects’ judgments were actually influenced by F2. For this, we use the \\(\\chi^2\\)-test. anova(pvp.glm) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: as.factor(response) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 109 148.1 ## F2 1 109 108 39.1 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We report: The proportion of pUp/pYp responses was significantly influenced by F2 (\\(\\chi^2\\)[1] = 109.0, \\(p\\) &lt; 0.001). 6.5 Categorical Independent Factor The logistic regression can be used in a similar way when the independent variable is categorical. The key difference is that no tipping point needs to be calculated and no sigmoid needs to be plotted. In the data frame sz, we stored information about how 20 participants pronounced the word “Sonne” (sun): either with an initial [s] (voiceless) or an initial [z] (voiced). Of the 20 participants, 9 came from Bavaria and 11 from Schleswig-Holstein: head(sz) ## # A tibble: 6 × 3 ## fricative dialect Vpn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 z SH S1 ## 2 z SH S2 ## 3 z SH S3 ## 4 z SH S4 ## 5 s SH S5 ## 6 s SH S6 Our question is: Is voicing (two levels: s, z) influenced by dialect (two levels: BY, SH)? Since both variables are categorical in this case, we can create a bar plot to get an idea of the data: ggplot(sz) + aes(fill = fricative, x = dialect) + geom_bar(position = &quot;fill&quot;) It appears that the initial fricative is produced significantly more often voiceless in Bavaria than in Schleswig-Holstein. Now, as before, we apply a logistic regression followed by a \\(\\chi^2\\)-test to the data: sz.glm &lt;- glm(as.factor(fricative) ~ dialect, family = binomial, data = sz) anova(sz.glm, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: as.factor(fricative) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 19 27.7 ## dialect 1 5.3 18 22.4 0.021 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The \\(\\chi^2\\)-test shows: The distribution of voiced and voiceless /s/ in words like Sonne was significantly influenced by dialect (\\(\\chi^2\\)[1] = 5.3, \\(p\\) &lt; 0.05). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
