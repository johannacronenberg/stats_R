<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Simple Linear Regression | Statistics in R: An Introduction for Phoneticians</title>
  <meta name="description" content="An introduction to statistics in R focussing on linear regressions. This introduction was written as study material for the students of the Institute of Phonetics and Speech Processing at the University of Munich." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Simple Linear Regression | Statistics in R: An Introduction for Phoneticians" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introduction to statistics in R focussing on linear regressions. This introduction was written as study material for the students of the Institute of Phonetics and Speech Processing at the University of Munich." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Simple Linear Regression | Statistics in R: An Introduction for Phoneticians" />
  
  <meta name="twitter:description" content="An introduction to statistics in R focussing on linear regressions. This introduction was written as study material for the students of the Institute of Phonetics and Speech Processing at the University of Munich." />
  

<meta name="author" content="Johanna Cronenberg" />


<meta name="date" content="2025-12-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-inferential-statistics.html"/>
<link rel="next" href="multiple-linear-regression.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Statistics in R: An Introduction</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i><b>1</b> Setup</a>
<ul>
<li class="chapter" data-level="1.1" data-path="setup.html"><a href="setup.html#installation"><i class="fa fa-check"></i><b>1.1</b> Installation</a></li>
<li class="chapter" data-level="1.2" data-path="setup.html"><a href="setup.html#r-projects"><i class="fa fa-check"></i><b>1.2</b> R Projects</a></li>
<li class="chapter" data-level="1.3" data-path="setup.html"><a href="setup.html#packages-and-r-version"><i class="fa fa-check"></i><b>1.3</b> Packages and R Version</a></li>
<li class="chapter" data-level="1.4" data-path="setup.html"><a href="setup.html#sessions"><i class="fa fa-check"></i><b>1.4</b> Sessions</a></li>
<li class="chapter" data-level="1.5" data-path="setup.html"><a href="setup.html#types-of-documents"><i class="fa fa-check"></i><b>1.5</b> Types of Documents</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="setup.html"><a href="setup.html#r-scripts"><i class="fa fa-check"></i><b>1.5.1</b> R Scripts</a></li>
<li class="chapter" data-level="1.5.2" data-path="setup.html"><a href="setup.html#r-markdown"><i class="fa fa-check"></i><b>1.5.2</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="setup.html"><a href="setup.html#help"><i class="fa fa-check"></i><b>1.6</b> Help</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="setup.html"><a href="setup.html#introduction-to-programming-in-r"><i class="fa fa-check"></i><b>1.6.1</b> Introduction to Programming in R</a></li>
<li class="chapter" data-level="1.6.2" data-path="setup.html"><a href="setup.html#recognizing-errors"><i class="fa fa-check"></i><b>1.6.2</b> Recognizing Errors</a></li>
<li class="chapter" data-level="1.6.3" data-path="setup.html"><a href="setup.html#ask-the-community"><i class="fa fa-check"></i><b>1.6.3</b> Ask the Community</a></li>
<li class="chapter" data-level="1.6.4" data-path="setup.html"><a href="setup.html#help-with-ggplot2"><i class="fa fa-check"></i><b>1.6.4</b> Help with <code>ggplot2</code></a></li>
<li class="chapter" data-level="1.6.5" data-path="setup.html"><a href="setup.html#statistics-in-r-literature"><i class="fa fa-check"></i><b>1.6.5</b> Statistics in R: Literature</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="setup.html"><a href="setup.html#acknowledgements"><i class="fa fa-check"></i><b>1.7</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html"><i class="fa fa-check"></i><b>2</b> Introduction to Inferential Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#load-packages-and-data"><i class="fa fa-check"></i><b>2.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#basic-terminology"><i class="fa fa-check"></i><b>2.2</b> Basic Terminology</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#normal-distribution"><i class="fa fa-check"></i><b>2.3</b> Normal Distribution</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#testing-for-normal-distribution"><i class="fa fa-check"></i><b>2.3.1</b> Testing for Normal Distribution</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#rule-confidence-intervals"><i class="fa fa-check"></i><b>2.3.2</b> 68–95–99.7 Rule &amp; Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#load-packages-and-data-1"><i class="fa fa-check"></i><b>3.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>3.2</b> Introduction</a></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>3.3</b> Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-regression-line"><i class="fa fa-check"></i><b>3.4</b> The Regression Line</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#theoretical-information"><i class="fa fa-check"></i><b>3.4.1</b> Theoretical Information</a></li>
<li class="chapter" data-level="3.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-lines-with-ggplot2"><i class="fa fa-check"></i><b>3.4.2</b> Regression Lines with <code>ggplot2</code></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#linear-regression-with-lm"><i class="fa fa-check"></i><b>3.5</b> Linear Regression with <code>lm()</code></a></li>
<li class="chapter" data-level="3.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residuals"><i class="fa fa-check"></i><b>3.6</b> Residuals</a></li>
<li class="chapter" data-level="3.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#testing-assumptions"><i class="fa fa-check"></i><b>3.7</b> Testing Assumptions</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#normal-distribution-of-residuals"><i class="fa fa-check"></i><b>3.7.1</b> Normal Distribution of Residuals</a></li>
<li class="chapter" data-level="3.7.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#constant-variance-of-the-residuals"><i class="fa fa-check"></i><b>3.7.2</b> Constant Variance of the Residuals</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#understanding-all-results-of-lm"><i class="fa fa-check"></i><b>3.8</b> Understanding all Results of <code>lm()</code></a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimated-y-values-and-residuals"><i class="fa fa-check"></i><b>3.8.1</b> Estimated <span class="math inline">\(y\)</span>-Values and Residuals</a></li>
<li class="chapter" data-level="3.8.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-coefficients-and-t-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Regression Coefficients and <span class="math inline">\(t\)</span>-Statistic</a></li>
<li class="chapter" data-level="3.8.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#quality-criteria-for-the-model-and-f-statistic"><i class="fa fa-check"></i><b>3.8.3</b> Quality Criteria for the Model and <span class="math inline">\(F\)</span>-Statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#reporting-the-result"><i class="fa fa-check"></i><b>3.9</b> Reporting the Result</a></li>
<li class="chapter" data-level="3.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#load-packages-and-data-2"><i class="fa fa-check"></i><b>4.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#continuous-independent-variables"><i class="fa fa-check"></i><b>4.3</b> Continuous Independent Variables</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#without-interaction"><i class="fa fa-check"></i><b>4.3.1</b> Without Interaction</a></li>
<li class="chapter" data-level="4.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#with-interaction"><i class="fa fa-check"></i><b>4.3.2</b> With Interaction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#categorical-independent-variables"><i class="fa fa-check"></i><b>4.4</b> Categorical Independent Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#without-interaction-1"><i class="fa fa-check"></i><b>4.4.1</b> Without Interaction</a></li>
<li class="chapter" data-level="4.4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#with-interaction-1"><i class="fa fa-check"></i><b>4.4.2</b> With Interaction</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#mix-of-continuous-and-categorical-variables"><i class="fa fa-check"></i><b>4.5</b> Mix of Continuous and Categorical Variables</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#without-interaction-2"><i class="fa fa-check"></i><b>4.5.1</b> Without Interaction</a></li>
<li class="chapter" data-level="4.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#with-interaction-2"><i class="fa fa-check"></i><b>4.5.2</b> With Interaction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Mixed Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#load-packages-and-data-3"><i class="fa fa-check"></i><b>5.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="5.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#mixed-models-lmers-introduction"><i class="fa fa-check"></i><b>5.2</b> Mixed Models (LMERs): Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-intercepts-vs.-random-slopes"><i class="fa fa-check"></i><b>5.3</b> Random Intercepts vs. Random Slopes</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-intercepts"><i class="fa fa-check"></i><b>5.3.1</b> Random Intercepts</a></li>
<li class="chapter" data-level="5.3.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-slopes"><i class="fa fa-check"></i><b>5.3.2</b> Random Slopes</a></li>
<li class="chapter" data-level="5.3.3" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#determining-the-random-effects-structure-for-word"><i class="fa fa-check"></i><b>5.3.3</b> Determining the Random Effects Structure for <code>word</code></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#lmer-in-r"><i class="fa fa-check"></i><b>5.4</b> LMER in R</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#fixed-effects"><i class="fa fa-check"></i><b>5.4.1</b> Fixed Effects</a></li>
<li class="chapter" data-level="5.4.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-effects"><i class="fa fa-check"></i><b>5.4.2</b> Random Effects</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#convergence-problems-and-simplifying-the-model"><i class="fa fa-check"></i><b>5.5</b> Convergence Problems and Simplifying the Model</a></li>
<li class="chapter" data-level="5.6" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#reporting-results"><i class="fa fa-check"></i><b>5.6</b> Reporting Results</a></li>
<li class="chapter" data-level="5.7" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#quality-criteria-for-mixed-models"><i class="fa fa-check"></i><b>5.7</b> Quality Criteria for Mixed Models</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#marginal-and-conditional-r2"><i class="fa fa-check"></i><b>5.7.1</b> Marginal and Conditional <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.7.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>5.7.2</b> Likelihood Ratio Tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="logistic-regression.html"><a href="logistic-regression.html#load-packages-and-data-4"><i class="fa fa-check"></i><b>6.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#from-linear-to-logistic-regression"><i class="fa fa-check"></i><b>6.2</b> From Linear to Logistic Regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#an-example-for-p-q-and-logit"><i class="fa fa-check"></i><b>6.2.1</b> An Example for <span class="math inline">\(P\)</span>, <span class="math inline">\(Q\)</span>, and Logit</a></li>
<li class="chapter" data-level="6.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#the-logistic-regression-line"><i class="fa fa-check"></i><b>6.2.2</b> The Logistic Regression Line</a></li>
<li class="chapter" data-level="6.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#regression-with-glm"><i class="fa fa-check"></i><b>6.2.3</b> Regression with <code>glm()</code></a></li>
<li class="chapter" data-level="6.2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#the-chi2-test-reporting-results"><i class="fa fa-check"></i><b>6.2.4</b> The <span class="math inline">\(\chi^2\)</span>-Test &amp; Reporting Results</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="logistic-regression.html"><a href="logistic-regression.html#the-sigmoid-function-and-tipping-point"><i class="fa fa-check"></i><b>6.3</b> The Sigmoid Function and Tipping Point</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#the-tipping-point"><i class="fa fa-check"></i><b>6.3.1</b> The Tipping Point</a></li>
<li class="chapter" data-level="6.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#plotting-proportions"><i class="fa fa-check"></i><b>6.3.2</b> Plotting Proportions</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="logistic-regression.html"><a href="logistic-regression.html#tipping-points-in-perceptual-studies"><i class="fa fa-check"></i><b>6.4</b> Tipping Points in Perceptual Studies</a></li>
<li class="chapter" data-level="6.5" data-path="logistic-regression.html"><a href="logistic-regression.html#categorical-independent-factor"><i class="fa fa-check"></i><b>6.5</b> Categorical Independent Factor</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
			<a class="btn pull-right js-toolbar-action" href="einfache-lineare-regression.html"><i class="fa fa-language"></i></a>
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics in R: An Introduction for Phoneticians</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Simple Linear Regression<a href="simple-linear-regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="load-packages-and-data-1" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Load Packages and Data<a href="simple-linear-regression.html#load-packages-and-data-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Load the following packages and data frame:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="simple-linear-regression.html#cb65-1" tabindex="-1"></a><span class="fu">library</span>(broom)</span>
<span id="cb65-2"><a href="simple-linear-regression.html#cb65-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb65-3"><a href="simple-linear-regression.html#cb65-3" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">&quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot;</span></span>
<span id="cb65-4"><a href="simple-linear-regression.html#cb65-4" tabindex="-1"></a>queen <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="fu">file.path</span>(url, <span class="st">&quot;queen.txt&quot;</span>)) <span class="sc">%&gt;%</span> <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb65-5"><a href="simple-linear-regression.html#cb65-5" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">age =</span> Alter)</span></code></pre></div>
</div>
<div id="introduction" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Introduction<a href="simple-linear-regression.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far, we have used descriptive statistics to examine specific measurements (variables) in more detail and learned about empirical and theoretical distributions. However, such variables are often dependent on other variables. For example, many studies have shown that our reaction time decreases with increasing sleep deprivation. This means that the variable reaction time depends on the variable sleep deprivation. Therefore, we also speak of dependent and independent variables. These dependencies can be described using simple linear regression. In other words, the value of the dependent variable <span class="math inline">\(y\)</span> is predicted by the independent variable <span class="math inline">\(x\)</span>. Before we perform a linear regression, we will discuss regression lines and correlation.</p>
</div>
<div id="correlation" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Correlation<a href="simple-linear-regression.html#correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Correlation, also known as <em>Pearson’s correlation</em> <span class="math inline">\(r\)</span>, is a measure of the association between two variables and can be calculated using the <code>cor()</code> function. Here, we will be working with the <code>queen</code> data frame.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="simple-linear-regression.html#cb66-1" tabindex="-1"></a>queen <span class="sc">%&gt;%</span> <span class="fu">head</span>()</span></code></pre></div>
<pre><code>## # A tibble: 6 × 6
##     age    f0    F1    F2    F3    F4
##   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1    26  297.  566. 1873. 2895. 4091.
## 2    28  283.  526. 1846. 2930. 4089.
## 3    29  260.  518. 1785. 2880. 4065.
## 4    30  258.  521. 1786. 2804. 4103.
## 5    31  262.  533. 1819. 2952. 4097.
## 6    32  260.  545. 1694. 2772. 4056.</code></pre>
<p>This data frame holds the average fundamental frequency values of Queen Elizabeth II during her annual Christmas addresses. We are interested in whether the Queen’s age has had an influence on her fundamental frequency. First, let’s get an overview of the situation. It is important that, when plotting data where we suspect a correlation, we always place the independent variable (here: age) on the x-axis and the dependent variable (here: f0) on the y-axis.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="simple-linear-regression.html#cb68-1" tabindex="-1"></a><span class="fu">ggplot</span>(queen) <span class="sc">+</span> </span>
<span id="cb68-2"><a href="simple-linear-regression.html#cb68-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> age, <span class="at">y =</span> f0) <span class="sc">+</span> </span>
<span id="cb68-3"><a href="simple-linear-regression.html#cb68-3" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-28-1.svg" width="672" /></p>
<p>It looks like there might be a connection: the older the Queen got, the more her fundamental frequency decreased! We can verify our visual impression using the correlation <span class="math inline">\(r\)</span>:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="simple-linear-regression.html#cb69-1" tabindex="-1"></a><span class="fu">cor</span>(queen<span class="sc">$</span>age, queen<span class="sc">$</span>f0)</span></code></pre></div>
<pre><code>## [1] -0.8346</code></pre>
<p>The correlation <span class="math inline">\(r\)</span> takes values exclusively between -1 and 1. The closer the value is to zero, the weaker the relationship between the two variables. A strong negative correlation of -0.84 indicates a strong negative correlation, meaning our visual impression appears to be correct.</p>
</div>
<div id="the-regression-line" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> The Regression Line<a href="simple-linear-regression.html#the-regression-line" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="theoretical-information" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Theoretical Information<a href="simple-linear-regression.html#theoretical-information" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The regression line of simple linear regression can be described by the following formula:</p>
<p><span class="math inline">\(y = k + bx\)</span></p>
<p>Here, <span class="math inline">\(k\)</span> is the y-intercept and <span class="math inline">\(b\)</span> is the slope. Because the intercept and slope unambiguously describe a regression line, these two parameters are also called <strong>regression coefficients</strong>. Using the formula above, if the intercept <span class="math inline">\(k\)</span> and the slope <span class="math inline">\(b\)</span> are known, the corresponding <span class="math inline">\(y\)</span> values can be predicted for all possible <span class="math inline">\(x\)</span> values. The regression line is always an infinite, perfectly straight line and also passes through the mean of the distribution.</p>
<p>In the following figure, you see three regression lines: blue and green have the same intercept but opposite slopes; blue and orange have different intercepts but the same slope. The exact value of the slope indicates by how much the <span class="math inline">\(y\)</span> value increases or decreases when <span class="math inline">\(x\)</span> is increased by one unit. For <span class="math inline">\(x = 0\)</span> in the figure, <span class="math inline">\(y = 1\)</span> (for blue and green). For <span class="math inline">\(x = 1\)</span>, <span class="math inline">\(y = 1 + b\)</span>, so for blue <span class="math inline">\(y = 1 + 0.5 = 1.5\)</span> and for green <span class="math inline">\(y = 1 + (-0.5) = 0.5\)</span>. For the orange line, when <span class="math inline">\(x = 0\)</span>, <span class="math inline">\(y = 2\)</span>, and when <span class="math inline">\(x = 1\)</span>, <span class="math inline">\(y = 2 + 0.5 = 2.5\)</span>.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="simple-linear-regression.html#cb71-1" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb71-2"><a href="simple-linear-regression.html#cb71-2" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">ylim</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">&quot;x&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;y&quot;</span>) <span class="sc">+</span></span>
<span id="cb71-3"><a href="simple-linear-regression.html#cb71-3" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="fl">0.5</span>, <span class="at">intercept =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span> </span>
<span id="cb71-4"><a href="simple-linear-regression.html#cb71-4" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">intercept =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span> </span>
<span id="cb71-5"><a href="simple-linear-regression.html#cb71-5" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope =</span> <span class="fl">0.5</span>, <span class="at">intercept =</span> <span class="dv">2</span>, <span class="at">color =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span> </span>
<span id="cb71-6"><a href="simple-linear-regression.html#cb71-6" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-30-1.svg" width="672" /></p>
<p>In summary, the blue and orange lines describe a positive correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> (the larger <span class="math inline">\(x\)</span>, the larger <span class="math inline">\(y\)</span>), while the green line describes a negative correlation (the larger <span class="math inline">\(x\)</span>, the smaller <span class="math inline">\(y\)</span>).</p>
<p><strong>Important: Correlation is not causation!</strong> Linear regression can only describe the correlation between two variables, not causality. We introduce causality through our knowledge. For example, we know that sleep deprivation causes a slower reaction time. Linear regression can only show whether a relationship exists between reaction time and sleep deprivation, but from a regression perspective, it could just as easily mean that a slower reaction time causes sleep deprivation.</p>
</div>
<div id="regression-lines-with-ggplot2" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Regression Lines with <code>ggplot2</code><a href="simple-linear-regression.html#regression-lines-with-ggplot2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To fit a regression line through a <code>ggplot2</code> figure, we can use either <code>geom_abline()</code> (see above) or <code>geom_smooth()</code>. The first function takes the arguments <code>slope</code> and <code>intercept</code>, as you can see in the image above. The function <code>geom_smooth()</code>, on the other hand, takes the argument <code>method = "lm"</code>. “lm” stands for <em>linear model</em>, meaning that the function calculates slope and intercept for us, assuming that the data are in a linear relationship. We also specify the argument <code>se = F</code> because we don’t want confidence intervals displayed here. This is what the regression line looks like in the case of the Queen:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="simple-linear-regression.html#cb72-1" tabindex="-1"></a><span class="fu">ggplot</span>(queen) <span class="sc">+</span> </span>
<span id="cb72-2"><a href="simple-linear-regression.html#cb72-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> age, <span class="at">y =</span> f0) <span class="sc">+</span> </span>
<span id="cb72-3"><a href="simple-linear-regression.html#cb72-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb72-4"><a href="simple-linear-regression.html#cb72-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> F, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-31-1.svg" width="672" /></p>
<p>The difference between <code>geom_abline()</code> and <code>geom_smooth()</code> is that <code>geom_abline()</code> draws a theoretically infinitely long, straight line (but of course we only see a portion of it), while <code>geom_smooth()</code> is limited by the range of values of the data. <code>geom_smooth()</code> can also draw other types of regression lines.</p>
</div>
</div>
<div id="linear-regression-with-lm" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Linear Regression with <code>lm()</code><a href="simple-linear-regression.html#linear-regression-with-lm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we are ready to perform a linear regression using the <code>lm()</code> function. This function takes only a formula and the data frame as arguments. The formula is <code>y ~ x</code>, meaning we want to predict the <span class="math inline">\(y\)</span> values (the fundamental frequency) as a function of the <span class="math inline">\(x\)</span> values (the age). Linear regression estimates the intercept and slope so that a regression line can be fitted through the data points that has the smallest possible distance to all points (this method is also called <strong>least squares</strong>; more on that below).</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="simple-linear-regression.html#cb74-1" tabindex="-1"></a>queen.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(f0 <span class="sc">~</span> age, <span class="at">data =</span> queen)</span>
<span id="cb74-2"><a href="simple-linear-regression.html#cb74-2" tabindex="-1"></a>queen.lm</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = f0 ~ age, data = queen)
## 
## Coefficients:
## (Intercept)          age  
##      288.19        -1.07</code></pre>
<p>The coefficients can be displayed on their own using <code>coef()</code>:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="simple-linear-regression.html#cb76-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">coef</span>()</span></code></pre></div>
<pre><code>## (Intercept)         age 
##     288.186      -1.074</code></pre>
<p>So we see that the estimated intercept is 288.2 and the slope is -1.07. Confusingly, the slope is always referred to by the same name as the <span class="math inline">\(x\)</span> variable, in this case, “age”. The coefficients mean the following: At an age of zero years (<span class="math inline">\(x = 0\)</span>), the mean fundamental frequency is approximately 288 Hz, assuming a perfect linear relationship between age and the Queen’s fundamental frequency. With each additional year (<span class="math inline">\(x\)</span> increases by 1), the fundamental frequency decreases by 1.07 Hz. By substituting the intercept and slope into our previous formula, we can now predict the corresponding fundamental frequency for all possible ages:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="simple-linear-regression.html#cb78-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">40</span>, <span class="dv">50</span>)</span>
<span id="cb78-2"><a href="simple-linear-regression.html#cb78-2" tabindex="-1"></a>f0_fitted <span class="ot">&lt;-</span> <span class="fu">coef</span>(queen.lm)[<span class="dv">1</span>] <span class="sc">+</span> <span class="fu">coef</span>(queen.lm)[<span class="dv">2</span>] <span class="sc">*</span> x</span>
<span id="cb78-3"><a href="simple-linear-regression.html#cb78-3" tabindex="-1"></a>f0_fitted</span></code></pre></div>
<pre><code>## [1] 288.2 245.2 234.5</code></pre>
<p>As mentioned, the Queen’s estimated fundamental frequency at birth was 288 Hz. At age 40, it was likely only 245 Hz, and at age 50, 234.5 Hz. As you can see, our “fitted” model can also predict or estimate <span class="math inline">\(y\)</span> values that were not included in the original dataset (such as the f0 value for the 50-year-old Queen). However, all these points lie precisely on the regression line, and since the regression line is infinitely long, the estimate does not necessarily make sense for all values. For example, do you consider it likely that the Queen’s fundamental frequency was 288 Hz at birth? Children typically have a fundamental frequency of 300 to 400 Hz. You must always be aware of whether the estimates are <em>meaningful</em> or not when working with your data. In R, you can perform the estimations using the <code>predict()</code> function, which takes as arguments the model <code>queen.lm</code> and a data frame containing the <span class="math inline">\(x\)</span> values for which <span class="math inline">\(y\)</span> is to be estimated. The <span class="math inline">\(x\)</span> variable must have the same name as in the original data frame, in this case, “age”.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="simple-linear-regression.html#cb80-1" tabindex="-1"></a><span class="fu">predict</span>(queen.lm, <span class="fu">data.frame</span>(<span class="at">age =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="at">by =</span> <span class="dv">10</span>)))</span></code></pre></div>
<pre><code>##     1     2     3     4     5     6     7     8     9 
## 288.2 277.4 266.7 256.0 245.2 234.5 223.8 213.0 202.3 
##    10    11 
## 191.6 180.8</code></pre>
</div>
<div id="residuals" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Residuals<a href="simple-linear-regression.html#residuals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Above we see the estimated <span class="math inline">\(y\)</span> value for <span class="math inline">\(x = 40\)</span>, which is the fundamental frequency for the Queen at 40 years old, namely approximately 245 Hz. However, the actual measured value is much lower:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="simple-linear-regression.html#cb82-1" tabindex="-1"></a>queen <span class="sc">%&gt;%</span> <span class="fu">filter</span>(age <span class="sc">==</span> <span class="dv">40</span>) <span class="sc">%&gt;%</span> <span class="fu">pull</span>(f0)</span></code></pre></div>
<pre><code>## [1] 228.7</code></pre>
<p>The differences between the estimated and measured <span class="math inline">\(y\)</span> values are called <strong>residuals</strong>. The following figure shows a section of the previous plot for the age range between 30 and 40 years. The black dots are the actually measured f0 values, while the red dots are the estimated values. They lie exactly on the blue regression line. The vertical dashed lines represent the residuals. This figure illustrates why residuals are also referred to as <strong>errors</strong>.</p>
<p><img src="img/resid.png" /></p>
<p>The difference between the actual and estimated values is calculated as the sum of the squared residuals and is therefore also called the <strong>sum of squares of error</strong> (SSE). The method used to estimate the parameters of the regression line is called <strong>least squares</strong> because it attempts to keep the SSE as small as possible. This results in the regression line being positioned through the data so that all data points are as close to the line as possible.</p>
<p>The residuals can be output using <code>resid()</code>:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="simple-linear-regression.html#cb84-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">resid</span>()</span></code></pre></div>
<pre><code>##       1       2       3       4       5       6 
##  36.273  25.151   2.743   1.576   6.633   5.814 
##       7       8       9      10      11      12 
##  -3.825   5.283  -7.605 -12.949 -22.552  -8.751 
##      13      14      15      16      17      18 
## -16.571  -6.042  -7.350 -11.662  -7.231 -10.822 
##      19      20      21      22      23      24 
##  -1.267  -9.038   6.623   6.383  17.037  10.016 
##      25      26      27      28      29      30 
##   1.064  -9.792  11.148   2.725  -6.990   3.978</code></pre>
<p>SSE can be calculated with <code>deviance()</code>:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="simple-linear-regression.html#cb86-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">deviance</span>()</span></code></pre></div>
<pre><code>## [1] 4412</code></pre>
</div>
<div id="testing-assumptions" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Testing Assumptions<a href="simple-linear-regression.html#testing-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Statistical models such as the linear regression are based on assumptions about the data that must be met for the model’s result to be meaningful. In the case of linear regression, these assumptions relate to the residuals.</p>
<div id="normal-distribution-of-residuals" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Normal Distribution of Residuals<a href="simple-linear-regression.html#normal-distribution-of-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first assumption is that the residuals are normally distributed. We check this with a Q-Q plot:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="simple-linear-regression.html#cb88-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">augment</span>(queen.lm)) <span class="sc">+</span> </span>
<span id="cb88-2"><a href="simple-linear-regression.html#cb88-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">sample =</span> .resid) <span class="sc">+</span> </span>
<span id="cb88-3"><a href="simple-linear-regression.html#cb88-3" tabindex="-1"></a>  <span class="fu">stat_qq</span>() <span class="sc">+</span> </span>
<span id="cb88-4"><a href="simple-linear-regression.html#cb88-4" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>() <span class="sc">+</span> </span>
<span id="cb88-5"><a href="simple-linear-regression.html#cb88-5" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;samples&quot;</span>) <span class="sc">+</span> </span>
<span id="cb88-6"><a href="simple-linear-regression.html#cb88-6" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;theoretical quantiles&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-39-1.svg" width="672" /></p>
<p>That looks okay, but not perfect. In addition, let’s look at the probability distribution with a superimposed normal distribution:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="simple-linear-regression.html#cb89-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">augment</span>(queen.lm)) <span class="sc">+</span> </span>
<span id="cb89-2"><a href="simple-linear-regression.html#cb89-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> .resid) <span class="sc">+</span> </span>
<span id="cb89-3"><a href="simple-linear-regression.html#cb89-3" tabindex="-1"></a>  <span class="fu">geom_density</span>() <span class="sc">+</span> </span>
<span id="cb89-4"><a href="simple-linear-regression.html#cb89-4" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">40</span>, <span class="dv">40</span>) <span class="sc">+</span> </span>
<span id="cb89-5"><a href="simple-linear-regression.html#cb89-5" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;residuals&quot;</span>) <span class="sc">+</span></span>
<span id="cb89-6"><a href="simple-linear-regression.html#cb89-6" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm, </span>
<span id="cb89-7"><a href="simple-linear-regression.html#cb89-7" tabindex="-1"></a>                <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="fu">mean</span>(<span class="fu">augment</span>(queen.lm)<span class="sc">$</span>.resid), <span class="at">sd =</span> <span class="fu">sd</span>(<span class="fu">augment</span>(queen.lm)<span class="sc">$</span>.resid)), </span>
<span id="cb89-8"><a href="simple-linear-regression.html#cb89-8" tabindex="-1"></a>                <span class="at">inherit.aes =</span> F, </span>
<span id="cb89-9"><a href="simple-linear-regression.html#cb89-9" tabindex="-1"></a>                <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-40-1.svg" width="672" /></p>
<p>If we are still undecided, we can perform a Shapiro-Wilk test:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="simple-linear-regression.html#cb90-1" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="fu">augment</span>(queen.lm)<span class="sc">$</span>.resid)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  augment(queen.lm)$.resid
## W = 0.94, p-value = 0.1</code></pre>
<p>Since the <span class="math inline">\(p\)</span>-value is higher than <span class="math inline">\(\alpha = 0.05\)</span>, the residuals of the model seem to be approximately normally distributed.</p>
</div>
<div id="constant-variance-of-the-residuals" class="section level3 hasAnchor" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Constant Variance of the Residuals<a href="simple-linear-regression.html#constant-variance-of-the-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The second assumption states that the variance of the residuals should be similar for all estimated values. This assumption is also known as <strong>homoscedasticity</strong> (try saying that three times fast). If the assumption is not met, we speak of <strong>heteroscedasticity</strong>. To visually represent the variance, we plot the residuals against the estimated values. Since the mean of the residuals is always approximately zero (dashed line in the figure at <span class="math inline">\(y = 0\)</span>), we can recognize the constant variance by the fact that the points are evenly distributed around this mean.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="simple-linear-regression.html#cb92-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">augment</span>(queen.lm)) <span class="sc">+</span> </span>
<span id="cb92-2"><a href="simple-linear-regression.html#cb92-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> .fitted, <span class="at">y =</span> .resid) <span class="sc">+</span> </span>
<span id="cb92-3"><a href="simple-linear-regression.html#cb92-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb92-4"><a href="simple-linear-regression.html#cb92-4" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;estimated f0 values&quot;</span>) <span class="sc">+</span> </span>
<span id="cb92-5"><a href="simple-linear-regression.html#cb92-5" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;residuals&quot;</span>) <span class="sc">+</span> </span>
<span id="cb92-6"><a href="simple-linear-regression.html#cb92-6" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-42-1.svg" width="672" /></p>
<p>Since we have very few data points here, it’s difficult to assess whether there’s a recognizable pattern in the plot that would indicate that the errors don’t have constant variance. The two outliers in the upper right are certainly not a good sign; but the rest looks okay. For now, we’ll assume that the residuals are homoscedastic. To develop your intuition for what constitutes good and bad residual plots, I recommend Figures 6.2 and 6.3 in Winter (2020, pp. 111f.).</p>
</div>
</div>
<div id="understanding-all-results-of-lm" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> Understanding all Results of <code>lm()</code><a href="simple-linear-regression.html#understanding-all-results-of-lm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The details mentioned in this section for calculating the various values are extremely rare in statistics books, statistics blogs, R vignettes, or other information sources. You don’t need to memorize these details; the point is that you can understand the results of <code>lm()</code> – and that involves more than just the <span class="math inline">\(p\)</span>-value.</p>
<div id="estimated-y-values-and-residuals" class="section level3 hasAnchor" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> Estimated <span class="math inline">\(y\)</span>-Values and Residuals<a href="simple-linear-regression.html#estimated-y-values-and-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As you may have noticed, I used the <code>augment()</code> function to utilize the results of the linear model in <code>ggplot2</code>. This function comes from the package <a href="https://cran.r-project.org/web/packages/broom/vignettes/broom.html"><code>broom</code></a>, which we loaded at the beginning and which also provides two other helpful functions: <code>tidy()</code> and <code>glance()</code>. The result of these functions is always a <code>tibble</code>, an object that we can easily process further (unlike the strange regression objects):</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="simple-linear-regression.html#cb93-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> class</span></code></pre></div>
<pre><code>## [1] &quot;lm&quot;</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="simple-linear-regression.html#cb95-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">augment</span>() <span class="sc">%&gt;%</span> <span class="fu">class</span>()</span></code></pre></div>
<pre><code>## [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;</code></pre>
<p>Let’s first take a look at the results of <code>augment()</code>:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="simple-linear-regression.html#cb97-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">augment</span>()</span></code></pre></div>
<pre><code>## # A tibble: 30 × 8
##       f0   age .fitted .resid   .hat .sigma  .cooksd
##    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
##  1  297.    26    260.  36.3  0.0946   10.5 0.482   
##  2  283.    28    258.  25.2  0.0845   11.7 0.202   
##  3  260.    29    257.   2.74 0.0798   12.8 0.00225 
##  4  258.    30    256.   1.58 0.0753   12.8 0.000694
##  5  262.    31    255.   6.63 0.0710   12.7 0.0115  
##  6  260.    32    254.   5.81 0.0670   12.7 0.00826 
##  7  249.    33    253.  -3.82 0.0632   12.8 0.00334 
##  8  257.    34    252.   5.28 0.0596   12.7 0.00597 
##  9  243.    35    251.  -7.60 0.0563   12.7 0.0116  
## 10  236.    37    248. -12.9  0.0503   12.5 0.0297  
## # ℹ 20 more rows
## # ℹ 1 more variable: .std.resid &lt;dbl&gt;</code></pre>
<p>This function appends further columns to the original data (columns <code>f0</code> and <code>age</code>), namely the fitted (i.e., the model-estimated) f0 values <code>.fitted</code>, the residuals <code>.resid</code>, and others that are not of interest to us at this time.</p>
</div>
<div id="regression-coefficients-and-t-statistic" class="section level3 hasAnchor" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> Regression Coefficients and <span class="math inline">\(t\)</span>-Statistic<a href="simple-linear-regression.html#regression-coefficients-and-t-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <code>tidy()</code> function returns a table of the estimated regression coefficients:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="simple-linear-regression.html#cb99-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">tidy</span>()</span></code></pre></div>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   288.       6.98      41.3  1.23e-26
## 2 age            -1.07     0.134     -8.02 9.93e- 9</code></pre>
<p>The <code>estimate</code> column contains the estimates for intercept (first row) and slope (second row). The <code>std.error</code> column contains the so-called <strong>Standard Error</strong>, a measure of the accuracy of the estimate. Here, we want the smallest possible values (relative to the estimate) because this means that the model’s estimates for the regression coefficients are precise. A column with the test statistic, <code>statistic</code>, follows. So far, we haven’t discussed the statistical significance of the regression. Here, a <strong><span class="math inline">\(t\)</span>-test</strong> is performed to determine whether the estimated regression coefficients differ significantly from zero. If the regression coefficients are close to zero, they contribute nothing to predicting the <span class="math inline">\(y\)</span> value (recall the formula for the regression line: if <span class="math inline">\(k\)</span> or <span class="math inline">\(b\)</span> are equal to zero, they have no effect on the regression line). The value in the <code>statistic</code> column, which in this case is also called the <strong><span class="math inline">\(t\)</span>-value</strong>, is calculated as <code>estimate / std.error</code>, e.g. for the slope:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="simple-linear-regression.html#cb101-1" tabindex="-1"></a><span class="fu">as.numeric</span>(<span class="fu">tidy</span>(queen.lm)[<span class="dv">2</span>, <span class="st">&quot;estimate&quot;</span>] <span class="sc">/</span> <span class="fu">tidy</span>(queen.lm)[<span class="dv">2</span>, <span class="st">&quot;std.error&quot;</span>])</span></code></pre></div>
<pre><code>## [1] -8.016</code></pre>
<p>The <span class="math inline">\(t\)</span>-statistic has its own probability density distribution, called the Student’s <span class="math inline">\(t\)</span> distribution or simply the <span class="math inline">\(t\)</span> distribution, which we can plot using the <code>dt()</code> function in <code>ggplot2</code> (analogous to the <code>dnorm()</code> function for normal distributions). This function takes an argument called <code>df</code>, which stands for <strong>degrees of freedom</strong>. The degrees of freedom are usually the sample size minus the number of coefficients, so here <code>nrow(queen) - 2</code>.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="simple-linear-regression.html#cb103-1" tabindex="-1"></a><span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb103-2"><a href="simple-linear-regression.html#cb103-2" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb103-3"><a href="simple-linear-regression.html#cb103-3" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm) <span class="sc">+</span></span>
<span id="cb103-4"><a href="simple-linear-regression.html#cb103-4" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dt, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df =</span> <span class="dv">28</span>), <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>) <span class="sc">+</span></span>
<span id="cb103-5"><a href="simple-linear-regression.html#cb103-5" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dt, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df =</span> <span class="dv">5</span>), <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>) <span class="sc">+</span></span>
<span id="cb103-6"><a href="simple-linear-regression.html#cb103-6" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-47-1.svg" width="672" /></p>
<p>Here you see the <span class="math inline">\(t\)</span> distribution in orange compared to the black normal distribution with a mean of zero and a standard deviation of one; the two distributions are very similar. With decreasing degrees of freedom (for example, 5 degrees of freedom for the dark green distribution), the normal and <span class="math inline">\(t\)</span> distributions become less similar. As you know, the area under these distributions is always 1, meaning that even with the <span class="math inline">\(t\)</span> distribution, we can use a function to calculate the probability of a value falling within a specific range. The <span class="math inline">\(t\)</span>-value for the slope in our example is approximately -8.02. Under the orange <span class="math inline">\(t\)</span> distribution, which matches our data, there is only a very, very small area under the curve for the range of values from negative infinity to -8.02 (you’ll need a bit of imagination here, as the range in the figure above only starts at -5). Following the example of <code>pnorm()</code>, the function for calculating the area under the <span class="math inline">\(t\)</span> distribution is called <code>pt()</code>. Using the <span class="math inline">\(t\)</span>-value and the degrees of freedom, we can calculate the <span class="math inline">\(p\)</span>-value:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="simple-linear-regression.html#cb104-1" tabindex="-1"></a><span class="dv">2</span> <span class="sc">*</span> <span class="fu">pt</span>(<span class="sc">-</span><span class="fl">8.016248</span>, <span class="at">df =</span> <span class="dv">28</span>)</span></code></pre></div>
<pre><code>## [1] 9.929e-09</code></pre>
<div class="gray">
<p><strong>Further Information: two-tailed t-test</strong></p>
<p>We need to multiply the probability value calculated by <code>pt()</code> by 2 because the calculated <span class="math inline">\(t\)</span>-test is not a one-tailed but a two-tailed <span class="math inline">\(t\)</span>-test. The extreme ends of the distribution are called the <em>tail</em>; for the normal and <span class="math inline">\(t\)</span> distributions, very high and very low values are unlikely (i.e., there is very little area under the distribution from negative infinity to a very low x-value or from a very high x-value to positive infinity).</p>
<p>When we calculate a probability (or area) here with <code>pt()</code>, it only applies to the <em>lower tail</em> from negative infinity to the specified x-value. However, the same probability applies to the area from the positive x-value (here: <code>abs(-8.016248)</code>) to positive infinity (the <em>upper tail</em>). Therefore, we simplify things by multiplying the result above by 2. We could also have written:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="simple-linear-regression.html#cb106-1" tabindex="-1"></a><span class="fu">pt</span>(<span class="sc">-</span><span class="fl">8.016248</span>, <span class="at">df =</span> <span class="dv">28</span>) <span class="sc">+</span> <span class="fu">pt</span>(<span class="fu">abs</span>(<span class="sc">-</span><span class="fl">8.016248</span>), <span class="at">df =</span> <span class="dv">28</span>, <span class="at">lower.tail =</span> F)</span></code></pre></div>
<pre><code>## [1] 9.929e-09</code></pre>
<p>You have seen something similar when calculating the 95% confidence interval for normal distributions: Our aim was to divide the area of 0.05 equally between both symmetrical halves of the distribution, i.e., we considered both <em>tails</em>, and not just one of them.</p>
</div>
<p>The <span class="math inline">\(p\)</span>-value can be interpreted as follows: If we assume that the actual slope is zero (this is the null hypothesis of this <span class="math inline">\(t\)</span>-test), then the observed slope of -1.07 is highly unexpected.</p>
</div>
<div id="quality-criteria-for-the-model-and-f-statistic" class="section level3 hasAnchor" number="3.8.3">
<h3><span class="header-section-number">3.8.3</span> Quality Criteria for the Model and <span class="math inline">\(F\)</span>-Statistic<a href="simple-linear-regression.html#quality-criteria-for-the-model-and-f-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now that we understand the output of <code>tidy()</code>, only <code>glance()</code> remains to be discovered. The <code>glance()</code> function displays at a glance a few criteria that can be used to assess the <strong>quality of the model</strong> (also called goodness-of-fit; for better readability, we convert the data frame to the long format):</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="simple-linear-regression.html#cb108-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">glance</span>() <span class="sc">%&gt;%</span> <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(r.squared<span class="sc">:</span>nobs))</span></code></pre></div>
<pre><code>## # A tibble: 12 × 2
##    name             value
##    &lt;chr&gt;            &lt;dbl&gt;
##  1 r.squared      6.97e-1
##  2 adj.r.squared  6.86e-1
##  3 sigma          1.26e+1
##  4 statistic      6.43e+1
##  5 p.value        9.93e-9
##  6 df             1   e+0
##  7 logLik        -1.17e+2
##  8 AIC            2.41e+2
##  9 BIC            2.45e+2
## 10 deviance       4.41e+3
## 11 df.residual    2.8 e+1
## 12 nobs           3   e+1</code></pre>
<p>The value <code>r.squared</code> is exactly what the name suggests: the squared correlation value <span class="math inline">\(r\)</span>, which we calculated above using <code>cor()</code>:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="simple-linear-regression.html#cb110-1" tabindex="-1"></a><span class="fu">cor</span>(queen<span class="sc">$</span>age, queen<span class="sc">$</span>f0)<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.6965</code></pre>
<p>The <strong><span class="math inline">\(R^2\)</span></strong> value describes the proportion of the variance in the data that is described by the fitted model. In this case, approximately 70% of the variance in the data is described by the model with a predictor (i.e., an independent variable). In linguistics, much lower <span class="math inline">\(R^2\)</span> values are more common because our subject of study is often influenced by many arbitrary factors that we cannot capture. The <code>adj.r.squared</code> value is a form of <span class="math inline">\(R^2\)</span> that is normalized for the number of independent variables. This is important because with a higher number of independent variables, <span class="math inline">\(R^2\)</span> will automatically increase, even if one or more of the variables do not contribute statistically to explaining or estimating the y-values. The <em>adjusted</em> <span class="math inline">\(R^2\)</span>, on the other hand, incorporates the number of independent variables into the calculation of <span class="math inline">\(R^2\)</span> and is therefore more reliable than the simple <span class="math inline">\(R^2\)</span>. Since there is only one variable here, <span class="math inline">\(R^2\)</span> and <em>adjusted</em> <span class="math inline">\(R^2\)</span> are very similar.</p>
<p>The column <code>sigma</code> contains the <strong>Residual Standard Error</strong>, which is an estimate of the standard deviation of the error distribution. We can calculate this value using <code>sigma()</code> (we will return to this value shortly):</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="simple-linear-regression.html#cb112-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">sigma</span>()</span></code></pre></div>
<pre><code>## [1] 12.55</code></pre>
<p>Then we see another <code>statistic</code> along with <code>p.value</code>. This time it’s the <strong><span class="math inline">\(F\)</span> statistic</strong>, meaning we read the <span class="math inline">\(p\)</span>-value from the <span class="math inline">\(F\)</span> distribution, which can be plotted using <code>df()</code> in <code>ggplot2</code> (the function’s name stands for <span class="math inline">\(F\)</span> distribution, not for degrees of freedom). This distribution depends on two parameters: <code>glance(queen.lm)$df</code> and <code>glance(queen.lm)$df.residual</code>. These are the degrees of freedom for the model and for the residuals. In orange, you see the <span class="math inline">\(F\)</span> distribution that fits our data (namely, with one degree of freedom for the model and 28 degrees of freedom for the residuals). The distribution can only take values greater than zero. In dark green, you see an <span class="math inline">\(F\)</span> distribution where the degrees of freedom for the model have been changed, and in black, for comparison, the normal distribution.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="simple-linear-regression.html#cb114-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(<span class="at">x =</span> <span class="dv">0</span><span class="sc">:</span><span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb114-2"><a href="simple-linear-regression.html#cb114-2" tabindex="-1"></a>  <span class="fu">aes</span>(x) <span class="sc">+</span></span>
<span id="cb114-3"><a href="simple-linear-regression.html#cb114-3" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dnorm) <span class="sc">+</span></span>
<span id="cb114-4"><a href="simple-linear-regression.html#cb114-4" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> df, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df1 =</span> <span class="dv">1</span>, <span class="at">df2 =</span> <span class="dv">28</span>), <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>) <span class="sc">+</span></span>
<span id="cb114-5"><a href="simple-linear-regression.html#cb114-5" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> df, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df1 =</span> <span class="dv">7</span>, <span class="at">df2 =</span> <span class="dv">28</span>), <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>) <span class="sc">+</span></span>
<span id="cb114-6"><a href="simple-linear-regression.html#cb114-6" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;density&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: Computation failed in `stat_function()`.
## Caused by error in `fun()`:
## ! could not find function &quot;fun&quot;</code></pre>
<pre><code>## Warning: Computation failed in `stat_function()`.
## Caused by error in `fun()`:
## ! could not find function &quot;fun&quot;</code></pre>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-53-1.svg" width="672" /></p>
<p>Looking at the orange distribution here, we can already see that an <span class="math inline">\(F\)</span>-value of 64.3 (see <code>statistic</code>) would be extremely unlikely, hence the very small <span class="math inline">\(p\)</span>-value. The null hypothesis of the <span class="math inline">\(F\)</span>-test performed here is that a model without predictors explains the data as well as, or better than, the model with the predictor <code>age</code>. That is, if the <span class="math inline">\(p\)</span>-value resulting from the <span class="math inline">\(F\)</span>-test is very small, we can conclude that our model with the independent variable <code>age</code> explains the data better than a model without predictors.</p>
<p>We will now try to understand the values in the columns <code>statistic</code>, <code>df</code>, <code>df.residual</code>, and <code>deviance</code>. You already know the latter as <strong>SSE</strong> (sum of squared error):</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="simple-linear-regression.html#cb117-1" tabindex="-1"></a>SSE <span class="ot">&lt;-</span> queen.lm <span class="sc">%&gt;%</span> <span class="fu">deviance</span>()</span>
<span id="cb117-2"><a href="simple-linear-regression.html#cb117-2" tabindex="-1"></a>SSE</span></code></pre></div>
<pre><code>## [1] 4412</code></pre>
<p><code>df</code>, as already indicated, is the number of degrees of freedom of the model and is calculated as the number of regression coefficients <span class="math inline">\(k\)</span> minus 1:</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="simple-linear-regression.html#cb119-1" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">length</span>(queen.lm<span class="sc">$</span>coefficients)</span>
<span id="cb119-2"><a href="simple-linear-regression.html#cb119-2" tabindex="-1"></a>df.SSR <span class="ot">&lt;-</span> k<span class="dv">-1</span></span>
<span id="cb119-3"><a href="simple-linear-regression.html#cb119-3" tabindex="-1"></a>df.SSR</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p><code>df.residual</code> is the number of degrees of freedom of the residuals, which is calculated as the number of observations <span class="math inline">\(N\)</span> minus the number of regression coefficients <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="simple-linear-regression.html#cb121-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> queen <span class="sc">%&gt;%</span> <span class="fu">nrow</span>()</span>
<span id="cb121-2"><a href="simple-linear-regression.html#cb121-2" tabindex="-1"></a>df.SSE <span class="ot">&lt;-</span> N <span class="sc">-</span> k</span>
<span id="cb121-3"><a href="simple-linear-regression.html#cb121-3" tabindex="-1"></a>df.SSE</span></code></pre></div>
<pre><code>## [1] 28</code></pre>
<p>Finally, to understand the <span class="math inline">\(F\)</span>-value in the <code>statistic</code> column, let’s look at its formula: <span class="math inline">\(F = \frac{MSR}{MSE}\)</span>. Here, <strong>MSR</strong> stands for <em>mean squares of regression</em> and <strong>MSE</strong> for <em>mean squares of error</em>. These two values describe the variance of the estimated y-values and the variance of the residuals. To calculate MSE and MSR, we first need to calculate two other values: <em>sum of squares of Y</em> (<strong>SSY</strong>), which describes the distance of the data points from the mean of <span class="math inline">\(y\)</span>, and <em>sum of squares of regression</em> (<strong>SSR</strong>), which describes the difference between SSY and SSR. First, SSY:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="simple-linear-regression.html#cb123-1" tabindex="-1"></a>SSY <span class="ot">&lt;-</span> <span class="fu">sum</span>((queen<span class="sc">$</span>f0 <span class="sc">-</span> <span class="fu">mean</span>(queen<span class="sc">$</span>f0))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb123-2"><a href="simple-linear-regression.html#cb123-2" tabindex="-1"></a>SSY</span></code></pre></div>
<pre><code>## [1] 14537</code></pre>
<p>In practice, SSY is the sum of all distances between the black (actually measured) data points and the orange line that intersects the y-axis at <code>mean(queen$f0)</code> and has a slope of zero (this figure again only shows a section of the entire range of values):</p>
<p><img src="img/resid2.png" /></p>
<p>You can already see visually that the orange line describes the data much less accurately than the blue regression line, which is why SSY is much larger than SSE.</p>
<p>To calculate SSR, we now only need to find the difference between SSY and SSE:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="simple-linear-regression.html#cb125-1" tabindex="-1"></a>SSR <span class="ot">&lt;-</span> SSY <span class="sc">-</span> SSE</span>
<span id="cb125-2"><a href="simple-linear-regression.html#cb125-2" tabindex="-1"></a>SSR</span></code></pre></div>
<pre><code>## [1] 10125</code></pre>
<p>Now we can finally calculate MSE and MSR:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="simple-linear-regression.html#cb127-1" tabindex="-1"></a>MSE <span class="ot">&lt;-</span> SSE<span class="sc">/</span>df.SSE</span>
<span id="cb127-2"><a href="simple-linear-regression.html#cb127-2" tabindex="-1"></a>MSE</span></code></pre></div>
<pre><code>## [1] 157.6</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="simple-linear-regression.html#cb129-1" tabindex="-1"></a>MSR <span class="ot">&lt;-</span> SSR<span class="sc">/</span>df.SSR</span>
<span id="cb129-2"><a href="simple-linear-regression.html#cb129-2" tabindex="-1"></a>MSR</span></code></pre></div>
<pre><code>## [1] 10125</code></pre>
<p>… and the <span class="math inline">\(F\)</span>-value is obtained from the division of MSE and MSR:</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="simple-linear-regression.html#cb131-1" tabindex="-1"></a>F_wert <span class="ot">&lt;-</span> MSR <span class="sc">/</span> MSE</span>
<span id="cb131-2"><a href="simple-linear-regression.html#cb131-2" tabindex="-1"></a>F_wert</span></code></pre></div>
<pre><code>## [1] 64.26</code></pre>
<p>Incidentally, there is a quadratic relationship between the <span class="math inline">\(t\)</span>-statistic and our <span class="math inline">\(F\)</span>-statistic: <span class="math inline">\(F = t^2\)</span> or <span class="math inline">\(t = \sqrt{F}\)</span>:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="simple-linear-regression.html#cb133-1" tabindex="-1"></a><span class="fu">sqrt</span>(F_wert)</span></code></pre></div>
<pre><code>## [1] 8.016</code></pre>
<p>Therefore, the <span class="math inline">\(p\)</span>-value is exactly the same for both statistics.</p>
<p>Finally, let’s return to the residual standard error, which we described above as an estimate of the standard deviation of the residuals. This is calculated as the square root of MSE:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="simple-linear-regression.html#cb135-1" tabindex="-1"></a><span class="fu">sqrt</span>(MSE)</span></code></pre></div>
<pre><code>## [1] 12.55</code></pre>
<p>When we determine the standard deviation of the residuals, it should be very close to the residual standard error (however, the latter is only an estimate, therefore the values do not have to be the same):</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="simple-linear-regression.html#cb137-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">augment</span>() <span class="sc">%&gt;%</span> <span class="fu">pull</span>(.resid) <span class="sc">%&gt;%</span> <span class="fu">sd</span>()</span></code></pre></div>
<pre><code>## [1] 12.33</code></pre>
<p>If the residual standard error is exactly zero, then all data points lie exactly on the regression line, i.e., every y-value from the dataset can be calculated exactly by the corresponding x-value using a linear model.</p>
</div>
</div>
<div id="reporting-the-result" class="section level2 hasAnchor" number="3.9">
<h2><span class="header-section-number">3.9</span> Reporting the Result<a href="simple-linear-regression.html#reporting-the-result" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We have now reviewed all the (relevant) results from <code>lm()</code>. We did this using the <code>broom</code> functions. A more traditional way to summarize the results of the linear regression is provided by <code>summary()</code>:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="simple-linear-regression.html#cb139-1" tabindex="-1"></a>queen.lm <span class="sc">%&gt;%</span> <span class="fu">summary</span>()</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = f0 ~ age, data = queen)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -22.55  -8.46  -0.10   6.24  36.27 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  288.186      6.977   41.31  &lt; 2e-16 ***
## age           -1.074      0.134   -8.02  9.9e-09 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.6 on 28 degrees of freedom
## Multiple R-squared:  0.697,  Adjusted R-squared:  0.686 
## F-statistic: 64.3 on 1 and 28 DF,  p-value: 9.93e-09</code></pre>
<p>You should recognize all the numbers here.</p>
<p>It is extremely important that we report our results correctly. For this, we need:</p>
<ul>
<li><span class="math inline">\(R^2\)</span> (or, because it’s more stable: <em>adjusted</em> <span class="math inline">\(R^2\)</span>): 0.69</li>
<li>the <span class="math inline">\(F\)</span>-value: 64.3</li>
<li>the degrees of freedom for the model and the residuals: 1 and 28</li>
<li>the <span class="math inline">\(p\)</span>-value, or the next higher significance level: <span class="math inline">\(p &lt; 0.001\)</span></li>
</ul>
<p>We report: <strong>There is a significant linear relationship between the Queen’s age and her fundamental frequency (<span class="math inline">\(R^2\)</span> = 0.69, <span class="math inline">\(F\)</span>[1, 28] = 64.3, <span class="math inline">\(p\)</span> &lt; 0.001).</strong></p>
</div>
<div id="summary" class="section level2 hasAnchor" number="3.10">
<h2><span class="header-section-number">3.10</span> Summary<a href="simple-linear-regression.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>In a linear regression, the values of the dependent variable <span class="math inline">\(y\)</span> are estimated using the values of the independent variable <span class="math inline">\(x\)</span>, assuming a linear relationship exists between them.</li>
<li>The regression line is the straight line to which the data points are closest (least squares method).</li>
<li>The function <code>lm()</code> estimates the regression coefficients y-intercept and slope.</li>
<li>A <span class="math inline">\(t\)</span>-test is performed to determine if the regression coefficients differ from zero. If <span class="math inline">\(p &lt; 0.05\)</span> in the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(x\)</span>, then the slope differs significantly from zero, meaning <span class="math inline">\(x\)</span> is a good predictor of <span class="math inline">\(y\)</span>.</li>
<li>The differences between the actual and estimated y-values are called residuals or errors; the residual standard error is an estimate of the standard deviation of the error distribution.</li>
<li><span class="math inline">\(R^2\)</span> is the square of the correlation coefficient <span class="math inline">\(r\)</span> and describes the proportion of the variance in the dependent variable <span class="math inline">\(y\)</span> that is described by the linear model.</li>
<li>An <span class="math inline">\(F\)</span>-test is also performed to check whether the linear model successfully explains a significant proportion of the variance in the dependent variable. If <span class="math inline">\(p &lt; 0.05\)</span> in the <span class="math inline">\(F\)</span>-test, then the model with the chosen predictor describes the empirical data better than a model without predictors (mean model).</li>
<li><em>SSE</em> is the <em>sum of squares of error</em> and describes the distance of the data points from the regression line; the <em>least squares</em> method attempts to minimize SSE.</li>
<li><em>SSR</em> is the <em>sum of squares of regression</em> and describes how well the regression model performs compared to the mean model (SSY).</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-inferential-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": null,
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": null,
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "none",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  },
  "info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
