[["index.html", "Statistik in R: eine Einführung für PhonetikerInnen 1 Setup 1.1 Installation und Kursverzeichnis 1.2 R Projekte 1.3 Packages und R Version 1.4 Sessions 1.5 Dokumentarten 1.6 Hilfe zur Selbsthilfe 1.7 Acknowledgements", " Statistik in R: eine Einführung für PhonetikerInnen Johanna Cronenberg 2025-12-10 1 Setup 1.1 Installation und Kursverzeichnis Laden Sie die Statistik-Software R herunter und installieren Sie sie. Die neueste Version ist derzeit 4.5.x (Stand: Dezember 2025). Laden Sie außerdem RStudio herunter und installieren Sie es. Schauen Sie sich diese kurze Einführung in RStudio an. Legen Sie außerdem ein Verzeichnis für diesen Kurs auf Ihrer Festplatte an. 1.2 R Projekte Für dieses Tutorial werden wir ein R project anlegen, da Ihnen dies die Arbeit mit R auf lange Sicht erleichtern wird. Öffnen Sie RStudio und klicken Sie oben rechts auf die Schaltfläche Project: (None). Klicken Sie dann auf New Project und im Folgenden auf Existing Directory und wählen Sie mit Browse das Verzeichnis aus, das Sie eben angelegt haben. Schließen Sie den Vorgang ab, indem Sie auf Create Project klicken. RStudio öffnet nun automatisch das Projekt, das genauso benannt ist wie Ihr Verzeichnis (siehe Schaltfläche oben rechts). Sie können das Projekt über diese Schaltfläche schließen und anschließend auch wieder öffnen. Das Projekt lässt sich außerdem öffnen, indem Sie in Ihrem Verzeichnis auf die neu angelegte Datei mit der Endung .Rproj klicken. Sollten Sie Probleme beim Erstellen des Projekts haben, empfehlen wir Ihnen diese Video-Kurzanleitung. Weiterführende Infos: R Projekte R Projekte haben viele Vorteile, insbesondere wenn Sie in mehreren Projekten/Kursen mit R arbeiten und für jeden ein eigenes R Projekt haben. Das Projekt merkt sich, welche Dateien Sie geöffnet haben, und stellt diese beim nächsten Öffnen des Projekts wieder her, sodass Sie da weiterarbeiten können, wo Sie aufgehört haben. Außerdem ist das working directory des Projekts Ihr Verzeichnis, das heißt alle Dateien, die Sie im Verlauf des Tutorials/Kurses dort ablegen, können Sie ganz einfach über das Panel unten links (Tab Files) oder über relative Pfade öffnen. Ihr Arbeitsverzeichnis können Sie übrigens überprüfen, indem Sie in der Konsole getwd() eingeben und Enter drücken. 1.3 Packages und R Version Für R gibt es viele Tausend packages bzw. libraries, die uns die Arbeit erleichtern werden. Bitte installieren Sie nun folgende Packages (das dauert eine Weile!): install.packages(c(&quot;Rcpp&quot;, &quot;remotes&quot;, &quot;knitr&quot;, &quot;tidyverse&quot;, &quot;magrittr&quot;, &quot;rmarkdown&quot;, &quot;gridExtra&quot;, &quot;emmeans&quot;, &quot;broom&quot;, &quot;lmerTest&quot;, &quot;pbkrtest&quot;, &quot;MuMIn&quot;, &quot;partR2&quot;)) Weiterführende Infos: Installation von R Paketen Sollte der obige Befehl den Fehler installation of package had non-zero exit status werfen, hat die Installation eines oder mehrerer Pakete nicht geklappt. Für Windows kann es sein, dass Sie in diesem Fall zusätzlich Rtools installieren müssen. Für MacOS müssen Sie ggf. die XCode command-line tools installieren und/oder resetten. Öffnen Sie dafür ein Mac Terminal und führen Sie folgende Befehle aus: xcode-select --install # Falls die Installation der R Packages dann immer noch nicht klappt: xcode-select --reset Wenn Sie sich unsicher sind, wie Sie auftretende Fehler bei der Installation der R Packages für Ihr Betriebssystem beheben können, können Sie mich auch gerne fragen! Einige Basispakete werden automatisch aktiviert beim Öffnen von RStudio, die meisten aber müssen Sie erst laden, bevor Sie die Funktionen verwenden können, die die Pakete anbieten. Zum Laden von Packages benutzen Sie den Befehl library(): library(tidyverse) Weiterführende Infos: Updates Bitte überprüfen Sie regelmäßig, ob Ihre Packages Updates benötigen – die Updates werden in R nicht automatisch eingespielt! Klicken Sie hierfür in der Werkzeugleiste auf Tools &gt; Check for Package Updates. Auch RStudio selbst erhält ab und zu Updates, dies können Sie überprüfen mit Help &gt; Check for Updates. R muss ebenfalls aktuell gehalten werden. Sie können Ihre R Version überprüfen mit getRversion(). Besuchen Sie einfach in regelmäßigen Abständen die R Webseite und schauen Sie, ob eine neue stabile Version verfügbar ist. 1.4 Sessions Eine Session beginnt, wenn man R bzw. RStudio startet bzw. wenn man ein Projekt öffnet. Man beendet eine Session entweder mit Session &gt; Quit Session in der Werkzeugleiste oder mit Strg+Q bzw. Ctrl+Q oder mit der Konsoleneingabe q(). Die Session endet außerdem automatisch, wenn Sie RStudio schließen. Sie werden dann gefragt, ob Sie das workspace image speichern wollen. Wenn Sie die Variablen, die Sie in der Session angelegt haben, im Environment behalten und in der nächsten Session wieder verwenden wollen, klicken Sie auf Save. Der workspace wird dann in Ihrem Verzeichnis in einer Datei mit der Endung .RData abgelegt. Wenn Sie den workspace nicht speichern möchten, klicken Sie auf Don't save. Falls Sie die Session doch nicht beenden wollen, klicken Sie auf Cancel. Für dieses Tutorial bitte ich Sie, den workspace nicht zu speichern (Don't save). 1.5 Dokumentarten 1.5.1 R Skripte Die Konsole in RStudio ist die direkte Verbindung zu R, d.h. dort kann R Code direkt ausgeführt werden. Um aber Ihren Code jederzeit replizieren zu können, müssen Sie ihn in einem Dokument festhalten. Üblicherweise werden Sie das in Ihrem Arbeitsalltag in einem R Skript machen. Ein R Skript kann einfach erstellt werden über File &gt; New File &gt; R Script (bzw. Strg + Shift + N) und sollte immer mit der Dateiendung .R abgespeichert werden. Ein R Skript enthält ausschließlich ausführbaren Code. Beim Ausführen eines Skriptes wird eine Zeile nur dann von R ignoriert, wenn sie mit # beginnt; dann ist die Zeile auskommentiert. Es gibt verschiedene Möglichkeiten, ein R Skript auszuführen. Markieren Sie die gewünschten Zeilen (wenn es nur eine Zeile ist, setzen Sie einfach den Cursor in die Zeile), und klicken Sie in der kleinen Werkzeugleiste im Panel mit dem geöffneten Skript auf Run oder drücken Sie Strg+Enter bzw. Ctrl+Enter. Das Ergebnis sehen Sie sofort in der Konsole. 1.5.2 R Markdown In den letzten Jahren hat sich aber auch eine andere Dokumentart etabliert, insbesondere für die Erstellung von Berichten und Lehrmaterial: das R Markdown. R Markdown ist eine Art Textdokument, in das man Code Snippets einbetten kann, die ganz normal ausgeführt werden können (wie oben beschrieben). Ein solches Dokument enthält häufig mehr Text als Code. Sie können ein R Markdown erstellen mit File &gt; New File &gt; R Markdown und es ist Konvention, das Dokument mit der Dateiendung .Rmd abzuspeichern. Eine R Markdown Datei wird im Normalfall in ein anderes Format umgewandelt (“ge-knitted”), z.B. in eine HTML, eine PDF, oder sogar ein Word Dokument. Dies geschieht entweder über den Wollknäuel-Button mit der Aufschrift Knit oder mittels: library(rmarkdown) render(&quot;document.Rmd&quot;) Ich benutze R Markdown auch, um z.B. die HTML herzustellen, die Sie gerade lesen. Im Markdown Dokument werden für Textmarkierungen besondere Zeichen verwendet, die dann beim knitten interpretiert und umgesetzt werden: # Überschrift: Mit einem Hashtag bekommt man die größtmögliche Überschrift; je mehr Hashtags man benutzt, desto kleiner wird die Überschrift. **fett**: Mit doppeltem Asterisk vor und hinter einer Textpassage wird der Text fett gesetzt. *kursiv*: Mit einfachem Asterisk wird der Text kursiv. `code`: Die einfachen rückwärts gewandten Anführungszeichen heben den darin enthaltenen Text hervor; das wird üblicherweise für Code oder Variablen benutzt, wenn man sich außerhalb eines Code Snippets befindet; dieser Code kann aber nicht ausgeführt werden! ```: Die dreifachen rückwarts gewandten Anführungszeichen markieren den Anfang und das Ende eine Code Snippets (auch Code Block genannt). Dazwischen darf nur Code geschrieben werden; Text muss mit einem Hashtag als Kommentar verfasst werden. Am Anfang des Code Snippets wird außerdem in geschweiften Klammern angegeben, welche Programmiersprache man im Code Block schreibt (in unserem Fall: {r}). Noch mehr Informationen dazu finden Sie im Cheatsheet zu R Markdown (insb. Seite 2, linke Spalte). 1.6 Hilfe zur Selbsthilfe 1.6.1 Einführung in Programmieren mit R Wenn Sie mit R noch nicht vertraut sind, lege ich Ihnen nahe, zuerst mein Tutorial Programmieren in R: eine Einführung für PhonetikerInnen durchzugehen! 1.6.2 Fehler erkennen Warnzeichen: Wenn Sie einen Syntaxfehler in einem Dokument haben (beispielsweise eine vergessene Klammer), sehen Sie am Rand kleine rote Warnzeichen. Diese sollten nicht ignoriert werden, denn sie weisen darauf hin, dass Sie einen Fehler gemacht haben. Wenn Sie den Fehler korrigieren, verschwinden die Warnzeichen. “Knit”: Wir empfehlen, dass Sie Ihr Markdown-Dokument regelmäßig in eine HTML überführen, indem Sie oben in der Werkzeugleiste auf “Knit” klicken. Wenn alles klappt, sehen Sie hoffentlich in einem neuen Fenster oder im Viewer (Panel unten rechts in RStudio) die kompilierte HTML. Wenn Sie aber Syntaxfehler oder andere Fehler in Ihrem Code haben, wird die HTML nicht erstellt und Sie kriegen stattdessen einen Fehler in der Konsole angezeigt. Dort sehen Sie auch, in welcher Zeile der Fehler ist. Code einzeln ausführen: Führen Sie jede neu geschriebene Zeile Code aus. So sehen Sie Ihr Ergebnis und können überlegen, ob das Ergebnis das gewünschte ist oder nicht. 1.6.3 Community nutzen Es gibt eine sehr große und hilfsbereite R Community, die Ihnen das Programmieren lernen mit R erleichtern wird. Hier ein paar gute Links und Befehle, falls Sie mal nicht weiter wissen: Stack Overflow: Ein Blog, auf dem Sie höchstwahrscheinlich eine Antwort auf Ihre Frage zu R finden werden. Am einfachsten googlen Sie Ihre Frage auf Englisch; die Antwort eines Mitglieds von Stack Overflow wird bei den ersten Suchergebnissen dabei sein. Hadley Wickham’s “R for Data Science”: Hadley Wickham ist der Chief Programmer des “tidyverse”, mit dem wir uns noch auseinandersetzen werden. Seine Bücher sind sehr verständlich, gut strukturiert und kurzweilig zu lesen. Cheatsheets: Das sind PDFs, die eine Funktionsübersicht mit Erklärungen und ggf. Beispielen in absoluter Kurzform bieten. Sie finden einige Cheatsheets in der obersten Werkzeugleiste unter Help &gt; Cheatsheets. Insbesondere die ersten drei sind für Sie interessant. Ansonsten kann man Cheatsheets auch googlen und findet dann z.B. Data Transformation with dplyr oder diese sehr ausführliche Reference Card. Vignetten: Zu einigen Paketen gibt es so genannte “Vignetten”, das sind meist HTMLs oder PDFs, die die AutorInnen eines Pakets geschrieben haben. Sie können mit folgender Konsoleneingabe nach Vignetten suchen: # zum Beispiel zu einer Library aus dem tidyverse: vignette(&quot;dplyr&quot;) In RStudio können Sie sich über die Eigenschaften einer Funktion informieren, indem Sie im Panel unten rechts mit dem Tab Help die gewünschte Funktion ins Suchfeld eingeben. Sie erhalten dann u.a. Informationen über die Argumente der Funktion und Beispiele. Dasselbe erreichen Sie über diese Konsoleneingaben (beispielhaft für Hilfe zur Funktion getwd()): ?getwd help(&quot;getwd&quot;) 1.6.4 Hilfe zu ggplot2 ggplot2 ist nicht nur bekannt, sondern auch beliebt! Dementsprechend viel Hilfe bekommen Sie von der R Community. Hier ein paar gute Quellen für Hilfe bei der Erstellung von Abbildungen: Kapitel Data Visualisation in Hadley Wickham’s “R for Data Science” Cookbook for R Cheatsheet ggplot2 Stack Overflow 1.6.5 Statistik in R: Literatur Wenn Sie mehr Informationen zu benötigen, seien Ihnen folgende Werke ans Herz gelegt: Bodo Winter’s “Statistics for Linguists: An Introduction using R”: Ein noch relativ neues Buch voller hervorragender Erklärungen zu allen wichtigen Themen der Inferenzstatistik. Ist über die Uni-Bib München digital verfügbar. Stefan Gries’ “Statistics for Linguistics with R: A Practical Introduction”: Nützlich für die Entscheidungsfindung, welches statistische Modell zu den eigenen Daten und der eigenen Fragestellung passt. Da das Buch von 2009 ist, ist der Code z.T. veraltet, aber aus statistischer Sicht ist der Inhalt noch aktuell. Ist über die Uni-Bib München digital verfügbar. Harald Baayen’s “Analyzing Linguistic Data: A Practical Introduction to Statistics”: Einführung für eher Fortgeschrittene. Hier ist der R Code ebenfalls oft veraltet, aber die Erklärungen und Beispiele zu den Statistikgrundlagen sind hilfreich. Als physisches Exemplar in der Uni-Bib München verfügbar. 1.7 Acknowledgements Dieses Lehrmaterial basiert auf ursprünglichem Material von Jonathan Harrington und Ulrich Reubold, die meine Statistik-Lehrer waren. Mein Dank gilt insbesondere Prof. Jonathan Harringon, der mir zugetraut hat, Programmieren und Statistik zu unterrichten und mich immer bedingungslos unterstützt hat. Die Weiterentwicklung meines Materials hat vor allem von der Lektüre von Bodo Winters “Statistics for Linguists: An Introduction using R” profitiert – ein wirklich fantastisches Statistik-Buch, das mir gezeigt hat, das es möglich ist, Statistik auf unterhaltsame und nicht-überfordernde Weise zu lehren. Außerdem möchte ich allen Studierenden danken, die meine Kurse besucht und durch ihre aktive Teilnahme bereichert haben. "],["einführung-in-die-inferenzstatistik.html", "2 Einführung in die Inferenzstatistik 2.1 Packages und Daten laden 2.2 Grundbegriffe 2.3 Normalverteilung", " 2 Einführung in die Inferenzstatistik 2.1 Packages und Daten laden Starten Sie das R Projekt, das Sie für dieses Tutorial angelegt haben. Öffnen Sie hierfür RStudio und benutzen Sie die Schaltfläche oben rechts oder navigieren Sie zu Ihrem Kursverzeichnis und klicken Sie auf die .Rproj Datei. Laden Sie dann das tidyverse und den folgenden Data Frame: library(tidyverse) ## ── Attaching core tidyverse packages ────────────────── ## ✔ dplyr 1.1.4 ✔ readr 2.1.6 ## ✔ forcats 1.0.1 ✔ stringr 1.6.0 ## ✔ ggplot2 4.0.1 ✔ tibble 3.3.0 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.2.0 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; df &lt;- read.table(file.path(url, &quot;vdata.txt&quot;)) %&gt;% as_tibble() %&gt;% rename(vokal = V, spannung = Tense, konsonant = Cons, tempo = Rate, subject = Subj) %&gt;% mutate(dauer = log(dur)) %&gt;% select(-c(X, Y)) 2.2 Grundbegriffe Statistical analysis […] is a strikingly subjective process – Bodo Winter Eine Population oder Grundgesamtheit ist im statistischen Sinne die Menge aller Einheiten (d.h. Personen, Wörter, etc.), die in bestimmten Identifikationskriterien (z.B. Geschlecht, Herkunft, grammatikalische Funktion, etc.) übereinstimmen. Stellen Sie sich z.B. vor, Sie möchten die durchschnittliche Grundfrequenz (f0) aller Frauen in Deutschland erfassen. Dann ist Ihre Population die Menge aller Frauen in Deutschland, also ca. 40 Mio. Menschen. Den Populationsmittelwert \\(\\mu\\) (sprich: /myː/, geschrieben oft: mu) können Sie in diesem Beispiel nur ermitteln, indem Sie zu jede einzelne Frau in Deutschland besuchen und deren Grundfrequenz messen, was natürlich schon rein wirtschaftlich unmöglich ist. Stattdessen erhebt man in der Wissenschaft meist Stichproben (samples), also z.B. nur einen Teil der weiblichen Bevölkerung, und geht davon aus, dass der so erhaltene Stichprobenmittelwert \\(m\\) nicht allzu weit weg ist vom tatsächlichen Populationsmittelwert \\(\\mu\\). Je größer die Stichprobe ist, desto mehr wird sich deren Mittelwert \\(m\\) und Standardabweichung \\(s\\) dem tatsächlichen Populationsmittelwert \\(\\mu\\) und der Populationsstandardabweichung \\(\\sigma\\) (sigma) annähern. Für die Merkmale einer Population werden im Normalfall griechische Symbole verwendet, für die Merkmale einer empirisch erhobenen Verteilung (d.h. einer Stichprobe) werden römische Buchstaben benutzt. Die Methoden der Inferenzstatistik ermöglichen uns Rückschlüsse von der Stichprobe auf die Population. Genauer gesagt: die Inferenzstatistik hilft uns dabei, die Parameter der Population zu schätzen. Es gibt auch Maße wie z.B. den Standard Error, die beschreiben wie gut oder schlecht die Schätzung ist. Bevor ein statistischer Test durchgeführt wird, muss klar sein, was getestet wird: die sogenannte Null-Hypothese (H0). Wir handeln dabei nach dem Prinzip der Falsifizierung, d.h. wir versuchen H0 mit einem statistischen Test zu widerlegen. Zusätzlich spricht man häufig von der Alternativ-Hypothese (H1), die genau das Gegenteil der Null-Hypothese ist. Hier ein Beispiel: Wir haben ein Experiment durchgeführt, weil wir herausfinden wollen, ob die Spannung eines Vokals einen Einfluss auf die Vokaldauer hat. Die gesammelten Daten sind im Data Frame df festgehalten. Dann stellen wir folgende Hypothesen auf: H0: Die Vokaldauer wird nicht von der Spannung beeinflusst. H1: Die Vokaldauer wird von der Spannung beeinflusst. Wir führen dann einen statistischen Test aus, um H0 zu falsifizieren. Aber Achtung: Wenn wir H0 falsifiziert haben, heißt das nicht, dass wir H1 verifiziert haben! Zusätzlich müssen wir vor unserem statistischen Test das sog. Signifikanzniveau, auch \\(\\alpha\\)-Level (alpha Level) genannt, festlegen. Das ist der Wahrscheinlichkeitswert, ab dem wir unsere Null-Hypothese verwerfen bzw. als falsifiziert betrachten. In der Wissenschaft gibt es drei (arbiträr festgelegte!) \\(\\alpha\\)-Level: 0.05, 0.01, und 0.001 (ich verwende immer die englische Schreibweise für Zahlen und nutze deshalb Punkte als Dezimalzeichen). Wenn das Ergebnis unseres statistischen Tests das festgelegte Signifikanzniveau unterschreitet, betrachten wir H0 als widerlegt und bezeichnen das Ergebnis als “signifikant”. Im Falle der hier vorgestellten statistischen Tests wird jeweils der \\(p\\)-value (\\(p\\)-Wert) berechnet, der uns Auskunft über das Signifikanzniveau gibt. Bei einem \\(p\\)-value von \\(p\\) &lt; 0.05 ist das Ergebnis des Tests statistisch signifikant. 2.3 Normalverteilung Oben habe ich auf den Unterschied zwischen einer Population und einer Stichprobe hingewiesen. Parallel dazu unterscheidet man auch zwischen theoretischen und empirischen Verteilungen. Theoretische Verteilungen haben häufig feststehende Namen (Normalverteilung, Poisson-Verteilung, Student-\\(t\\)-Verteilung, etc.) und es wird davon ausgegangen, dass Messwerte, die für eine gesamte Population erhoben wurden, einer bestimmten theoretischen Verteilung folgen. Wie wir aber festgestellt haben, können wir so gut wie nie eine Population vermessen und nutzen stattdessen eine Stichprobe. Die Messwerte einer Stichprobe stellen eine empirische Verteilung dar, weil sie empirisch erhoben wurden. Wir werden häufig testen müssen, welcher theoretischen Verteilung die erhobene empirische Verteilung am ehesten entspricht. In vielen empirischen Experimenten entsprechen die Daten einer Normalverteilung (auch Gauss-Verteilung genannt). Diese Verteilung lässt sich durch die zwei Parameter Mittelwert und Standardabweichung vollständig beschreiben. Hier sehen Sie drei verschiedene Normalverteilungen: schwarz: Mittelwert \\(\\mu = 0\\) und Standardabweichung \\(\\sigma = 1\\) blau: Mittelwert \\(\\mu = 3\\) und Standardabweichung \\(\\sigma = 2\\) grün: Mittelwert \\(\\mu = -2\\) und Standardabweichung \\(\\sigma = 1.5\\) Wie Sie sehen, verschiebt der Mittelwert die Normalverteilung entlang der x-Achse, während die Standardabweichung zu Veränderungen in der Breite der Verteilung führt: je größer die Standardabweichung, desto breiter die Normalverteilung. Außerdem zeigt die Abbildung, dass die Verteilung kontinuierlich ist, d.h. sie deckt den Wertebereich von minus bis plus unendlich ab. 2.3.1 Auf Normalverteilung testen Im Data Frame df wurde die Dauer von Vokalen festgehalten. Wir wollen testen, ob die empirische Verteilung der Dauer der Normalverteilung entspricht. Hier ist zunächst die empirisch gemessene Dauer in einer Wahrscheinlichkeitsdichteverteilung: ggplot(df) + aes(x = dauer) + geom_density() + xlab(&quot;Log. Dauer&quot;) + ylab(&quot;Wahrscheinlichkeitsdichte&quot;) + xlim(3.0, 7.0) Weiterführende Infos: Logarithmierung von Daten In der Abbildung sehen Sie die Vokaldauer nicht in Millisekunden, sondern in logarithmierter Form, d.h. wir haben den natürlichen Logarithmus auf die Daten angewendet (s. Code Snippet, in dem df geladen wurde). Bei bestimmten Messwerten kann es sinnvoll sein, die Daten zu logarithmieren, nämlich dann, wenn die empirische Verteilung ge-skewed ist. Bei Reaktionszeiten ist es z.B. nicht möglich, dass es Werte unter Null gibt; auch niedrige Werte (unter 100ms) sind sehr unwahrscheinlich, aber sehr lange Reaktionszeiten kommen durchaus vor. Bei Dauerwerten ist es ähnlich. Die tatsächlichen Dauerwerte in Millisekunden sehen so aus: ggplot(df) + aes(x = dur) + geom_density() + xlab(&quot;Dauer (ms)&quot;) + ylab(&quot;Wahrscheinlichkeitsdichte&quot;) Diese Verteilung hat einen starken rechtsseitigen Skew, eben weil lange Dauerwerte vorkommen, während sehr kurze Dauerwerte selten sind. 2.3.1.1 Überlagerung der Normalverteilung Um zu testen, ob Daten normalverteilt sind, sind visuelle Methoden unter angewandten StatistikerInnen beliebter als statistische Tests. Die erste Möglichkeit ist die Überlagerung der Normalverteilung über die empirische Verteilung. Dafür nutzen wir die Funktion dnorm(), die als Argumente den Mittelwert mean und die Standardabweichung sd bekommt. In ggplot2 können wir nicht einfach “fremde” Funktionen wie dnorm() an unseren ggplot()-Code anhängen. Wir benutzen stattdessen die ggplot2-eigene Funktion stat_function(). Diese Funktion hat folgende Argumente: fun: Die Funktion, mittels derer eine neue Kurve (in unserem Fall: die Kurve der Normalverteilung) erzeugt werden soll. args: Eine Liste von Argumenten der unter fun angegebenen Funktion. In unserem Fall braucht die Funktion dnorm() die Argumente mean und sd. ggplot(df) + aes(x = dauer) + geom_density() + xlim(3.0, 7.0) + xlab(&quot;Log. Dauer&quot;) + ylab(&quot;Wahrscheinlichkeitsdichte&quot;) + stat_function(fun = dnorm, args = list(mean = mean(df$dauer), sd = sd(df$dauer)), color = &quot;blue&quot;) Es gibt leichte Unterschiede zwischen der blauen Normalverteilung und der schwarzen empirischen Verteilung. Wahrscheinlich sind unsere Daten also nicht perfekt normalverteilt, aber zumindest annähernd. 2.3.1.2 Q-Q-Plot Neben der Überlagerung der Normalverteilung auf die empirische Verteilung werden häufig sogenannte Q-Q-Plots benutzt, wobei Q für Quantil steht. Schauen Sie folgendes YouTube-Video, um zu verstehen, wie ein Q-Q-Plot berechnet wird. Und lassen Sie sich nicht verwirren: Obwohl im Video auf der y-Achse “sample quantiles” steht, sind das einfach nur die aufsteigend geordneten Datenpunkte! ggplot(df) + aes(sample = dauer) + stat_qq() + stat_qq_line() + ylab(&quot;samples&quot;) + xlab(&quot;theoretical quantiles&quot;) In ggplot2 kann diese Abbildung mittels stat_qq() erstellt werden. Zusätzlich plotten wir mit stat_qq_line() eine gerade Linie, die wir zur Orientierung nutzen können. Wenn die Punkte von der Linie abweichen, sind die Daten nicht normalverteilt (wobei leichte Abweichungen am oberen und unteren Ende der Linie recht häufig sind). In diesem Fall ist ebenfalls eine leichte Abweichung von der Normalverteilung zu erkennen. 2.3.1.3 Interpretation trainieren Anfänglich kann es schwierig sein, anhand einer oder zwei Abbildungen festzulegen, ob die geplotteten Daten normalverteilt sind. Ich zeige Ihnen deshalb vier Beispiele für eindeutig nicht normalverteilte Daten, damit Sie Ihren Blick dafür schärfen können, wie Q-Q-Plots und überlagerte Wahrscheinlichkeitsverteilungen nicht aussehen sollten, wenn Ihre Daten normalverteilt sind. Im folgenden sehen Sie vier Wahrscheinlichkeitsverteilungen: für bimodale Daten (es gibt zwei Peaks), für links und rechts ge-skew-te Daten, sowie für uniform verteilte Daten (wo jeder Wert theoretisch gleich häufig vorkommt). Hier sind dieselben Wahrscheinlichkeitsverteilungen mit der jeweils parametrisch angepassten Normalverteilung (d.h. für jede Normalverteilung wurden der der abgebildeten Verteilung entsprechende Mittelwert und die entsprechende Standardabweichung verwendet): Zuletzt erstellen wir noch für alle vier Verteilungen die Q-Q-Plots, die deutlich von der geraden Linie abweichen: 2.3.1.4 Shapiro-Wilk Test Zuletzt schauen wir uns noch den Shapiro-Wilk Test an, weil das der am häufigsten verwendete Test für Normalverteilungen ist. In R ist dazu die Funktion shapiro.test() gedacht, die als einziges Argument die Daten als Vektor bekommt: shapiro.test(df$dauer) ## ## Shapiro-Wilk normality test ## ## data: df$dauer ## W = 0.99, p-value = 2e-11 Die Null-Hypothese dieses Tests ist, dass die Daten tatsächlich normalverteilt sind. Wenn der \\(p\\)-Wert unter dem allgemein anerkannten Signifikanzniveau von \\(\\alpha = 0.05\\) liegt, müssen wir diese Hypothese ablehnen und daher davon ausgehen, dass die Daten nicht normalverteilt sind. Unsere logarithmierten Dauerwerte sind laut diesem Test also nicht normalverteilt. Für den Shapiro-Wilk Test sollte Ihr Datensatz aus weniger als 5000 Beobachtungen bestehen und nicht zu viele Ausreißer oder identische Werte besitzen, da dies die Aussagekraft des Ergebnisses stark beeinflussen kann. Trotz dieses Testergebnisses würde ich wegen der zuvor erstellten Abbildungen davon ausgehen, dass unsere Daten annähernd normalverteilt sind. Weiterführende Infos: Wissenschaftliche Notation von Zahlen Sehr große und sehr kleine Zahlen werden in R häufig in der wissenschaftlichen Notation dargestellt, also z.B. 1e+02 anstatt 100. “e” steht hierbei für Basis 10 und die Zahlen danach sind der Exponent: 1e+02 == 1 * 10^2 ## [1] TRUE 1e-02 == 1 * 10^-2 ## [1] TRUE Bei Zahlen wie 100 oder 0.01 kann man die wissenschaftliche Notation noch ganz gut im Kopf umformen. Aber natürlich kann Ihnen R dies auch abnehmen, und zwar mit der Funktion format(), die das Argument scientific = FALSE bekommt: format(1e+02, scientific = F) ## [1] &quot;100&quot; format(1e-02, scientific = F) ## [1] &quot;0.01&quot; # und für unseren p-value aus dem Shapiro-Wilk Test oben: format(2.062e-11, scientific = F) ## [1] &quot;0.00000000002062&quot; Achtung, die Funktion gibt nur Schriftzeichenobjekte zurück, Sie müssen also (falls Sie mit der umgewandelten Zahl arbeiten wollen) as.numeric() verwenden, um das Ergebnis von format() wieder in eine Zahl umzuwandeln: class(format(1e+02, scientific = F)) ## [1] &quot;character&quot; as.numeric(format(1e+02, scientific = F)) ## [1] 100 2.3.2 68–95–99.7 Regel &amp; Konfidenzintervalle 2.3.2.1 Theoretisch Für Normalverteilungen (und in Annäherung auch für quasi-normalverteilte Daten) gilt die sogenannte 68–95–99.7 Regel. Wir illustrieren dies an einer Normalverteilung mit Mittelwert \\(\\mu = 0\\) und Standardabweichung \\(\\sigma = 1\\): Die Gesamtfläche unter der Normalverteilung ist 1, d.h. wir können mithilfe der Fläche bestimmen, mit welcher Wahrscheinlichkeit Datenpunkte in einen Wertebereich fallen. Bei der Normalverteilung fallen 68% der Daten in den blauen Bereich, 95% der Daten in den blauen + roten Bereich und 99.7% aller Daten in den gesamten eingefärbten Bereich. Wie Sie anhand der x-Achse dieser Abbildung sehen können, entspricht dies \\(\\mu\\pm1\\sigma\\) (blau), \\(\\mu\\pm2\\sigma\\) (blau + rot) und \\(\\mu\\pm3\\sigma\\) (blau + rot + gelb). Die 68–95–99.7 Regel ist also eine Art Eselsbrücke, mit der man sich merken kann, dass bei (annähernd) normalverteilten Daten 68% der Datenpunkte in einem Wertebereich von \\(\\mu\\pm1\\sigma\\) liegen, usw. Die Fläche unter der Normalverteilung können wir mit pnorm() berechnen. Diese Funktion bekommt einen x-Wert und den Mittelwert und die Standardabweichung, die die Normalverteilung beschreiben (der default ist Null und Eins, deshalb lasse ich hier diese beiden Argumente weg). Dann berechnet pnorm() die Fläche von minus unendlich bis zu dem genannten x-Wert. Wenn wir einen ganz hohen x-Wert eintragen, sollte klar werden, dass die Fläche unter der Normalverteilung tatsächlich 1 ist: pnorm(100) ## [1] 1 Für x = 0 (also unseren Mittelwert) sollte die Fläche 0.5 betragen: pnorm(0) ## [1] 0.5 Das heißt 50% aller Datenpunkte fallen in den Bereich von minus unendlich bis null. Nun können wir auch die oben eingefärbten Flächen berechnen: # blaue Flächen: pnorm(0) - pnorm(-1) ## [1] 0.3413 pnorm(1) - pnorm(0) ## [1] 0.3413 # rote Flächen: pnorm(-1) - pnorm(-2) ## [1] 0.1359 pnorm(2) - pnorm(1) ## [1] 0.1359 # gelbe Flächen: pnorm(-2) - pnorm(-3) ## [1] 0.0214 pnorm(3) - pnorm(2) ## [1] 0.0214 In der Statistik interessieren wir uns häufig dafür, ob Daten in das Konfidenzintervall fallen, meist wird vom 95%-Konfidenzintervall gesprochen. Das heißt bei normalverteilten Daten prüfen wir, ob ein Datenpunkt in den Wertebereich \\(\\mu \\pm 2\\sigma\\) fällt. Um wiederum herauszufinden, welcher Wertebereich das genau ist, nutzen wir die qnorm() Funktion, die wieder Mittelwert und Standardabweichung als Argumente bekommt und zusätzlich die gewünschte Fläche unter der Verteilung. Wenn wir symmetrisch um den Mittelwert eine Fläche von insgesamt 0.95 haben wollen, ist die Fläche am linken und rechten Rand der Verteilung jeweils \\((1 - 0.95) / 2 = 0.025\\). Für unsere Normalverteilung oben können wir dann berechnen: qnorm(0.025) ## [1] -1.96 qnorm(1-0.025) ## [1] 1.96 Das heißt für die Normalverteilung mit Mittelwert \\(\\mu = 0\\) und Standardabweichung \\(\\sigma = 1\\) liegt eine Fläche von 0.95 (d.h. 95% der Datenpunkte) zwischen -1.96 und 1.96. Oben hatten wir gesagt, dass die Fläche von 0.95 \\(\\mu \\pm2\\sigma\\) entspricht – genauer hätten wir schreiben müssen \\(\\mu \\pm 1.96\\sigma\\). 2.3.2.2 Empirisch Nehmen wir an, dass wir festgestellt haben, dass die Dauerwerte in df normalverteilt sind. Was wir nun mit unserem Wissen über die 68–95–99.7 Regel und Konfidenzintervalle anstellen können, ist zum Beispiel herauszufinden, mit welcher Wahrscheinlichkeit ein neuer Messpunkt aus demselben Experiment in einen gewissen Wertebereich fällt. Die folgenden Abbildungen zeigen die Normalverteilung mit Mittelwert mean(df$dauer) und Standardabweichung sd(df$dauer) und nicht die empirische Datenverteilung. mittel &lt;- mean(df$dauer) stdabw &lt;- sd(df$dauer) Wenn wir also dasselbe Experiment, aus dem df entstanden ist, nochmal durchführen, mit welcher Wahrscheinlichkeit würde ein neuer Datenpunkt in einem Wertebereich unter 4.5 (logarithmierte Dauer) liegen? Das ist äquivalent zu der Frage, wie groß die blaue Fläche ist, denn die gesamte Fläche unter der Normalverteilung ist immer gleich 1. Hierfür benutzen wir die Funktion pnorm(): pnorm(4.5, mean = mittel, sd = stdabw) ## [1] 0.07422 Die Wahrscheinlichkeit, dass ein neuer Datenpunkt in einen Wertebereich von minus unendlich bis 4.5 fällt, liegt also bei 7.4%. Nun dasselbe für die folgende blaue Fläche: Wir fragen uns also, wie groß diese Fläche ist bzw. mit welcher Wahrscheinlichkeit ein neuer Datenpunkt in den Bereich über 5.1 fällt. Bei der Berechnung müssen wir nun aber zwei Dinge beachten: Erstens berechnet pnorm() immer die Fläche zwischen minus unendlich und einem Wert, zweitens ist die Fläche unter der Normalverteilung gleich 1. Deshalb: 1 - pnorm(5.1, mean = mittel, sd = stdabw) ## [1] 0.2536 Die Fläche hat also eine Größe von 0.254, bzw. die Wahrscheinlichkeit liegt bei 25.4%. Wenn es uns nun um die Wahrscheinlichkeit geht, mit der Werte dem Experiment in einen Wertebereich von 4.9 bis 5.5 fallen, müssen wir pnorm() wie folgt austricksen: pnorm(5.5, mean = mittel, sd = stdabw) - pnorm(4.9, mean = mittel, sd = stdabw) ## [1] 0.4965 In diesem Fall liegt die Wahrscheinlichkeit, dass ein neuer Datenpunkt in den abgefragten Wertebereich fällt, bei 49.7%. Nun wollen wir für unsere Verteilung von Dauerwerten noch das 95%-Konfidenzintervall bestimmen, d.h. den Wertebereich, in dem 95% der Daten liegen. Das bedeutet nichts anderes, als dass wir den Wertebereich zu errechnen versuchen, für den die Fläche unter der Normalverteilung 0.95 ist. Da die Fläche symmetrisch um den Mittelwert verteilt sein soll, muss auf jeder Seite der Randbereich berechnet werden, für den die Fläche jeweils \\((1 - 0.95) / 2 = 0.025\\) ist. Hierfür verwenden wir wieder qnorm(): qnorm(0.025, mean = mittel, sd = stdabw) ## [1] 4.353 qnorm(0.975, mean = mittel, sd = stdabw) ## [1] 5.469 Das bedeutet, dass ein neuer Wert aus dem Experiment mit einer Wahrscheinlichkeit von 95% in den Wertebereich von 4.35 bis 5.47 fallen wird bzw. dass 95% der Daten aus dem aktuellen Experiment in diesem Bereich liegen (unter der Annahme, dass die Daten normalverteilt sind). anzahl &lt;- df %&gt;% filter(dauer &gt; 4.35 &amp; dauer &lt; 5.47) %&gt;% nrow() anzahl ## [1] 2877 anzahl / nrow(df) ## [1] 0.9648 Tatsächlich liegen 96.5% der Daten in dem berechneten Wertebereich. Hier zeigt sich noch einmal der Unterschied zwischen theoretischer und empirischer Verteilung: Wir haben geprüft, dass unsere empirischen Daten annähernd normalverteilt sind und daraufhin mit dem Mittelwert und der Standardabweichung der Daten eine Normalverteilung erstellt. Es ist eine theoretische Berechnung, dass 95% der Daten im Wertebereich zwischen 4.35 und 5.47 liegen (und dass 7.4% der Daten in einem Bereich unter 4.5 liegen, etc.), denn dies beruht auf der theoretischen Normalverteilung. Wenn wir uns die tatsächliche Proportion der Werte in den Bereichen und im Konfidenzintervall anschauen, kann sie sich von den theoretisch berechneten Proportionen unterscheiden. "],["einfache-lineare-regression.html", "3 Einfache lineare Regression 3.1 Packages und Daten laden 3.2 Einführung 3.3 Korrelation 3.4 Die Regressionslinie 3.5 Lineare Regression mit lm() 3.6 Residuals 3.7 Annahmen überprüfen 3.8 Alle Ergebnisse von lm() verstehen 3.9 Ergebnis berichten 3.10 Zusammenfassung", " 3 Einfache lineare Regression 3.1 Packages und Daten laden Laden Sie die folgenden Packages und Data Frame: library(broom) library(tidyverse) url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; queen &lt;- read.table(file.path(url, &quot;queen.txt&quot;)) %&gt;% as_tibble() 3.2 Einführung Bisher haben wir uns mithilfe der deskriptiven Statistik bestimmte Messwerte (Variablen) genauer angeschaut und einiges über empirische und theoretische Verteilungen erfahren. Häufig stehen solche Variablen jedoch in Abhängigkeit zu anderen Variablen. Zum Beispiel ist durch viele Studien belegt worden, dass unsere Reaktionsfähigkeit mit steigendem Schlafmangel abnimmt. Das heißt, dass die Variable Reaktionszeit von der Variable Schlafmangel abhängig ist. Wir sprechen deshalb auch von abhängigen und unabhängigen Variablen. Mit der einfachen linearen Regression lassen sich diese Abhängigkeiten beschreiben. Oft wird auch davon gesprochen, dass man den Wert der abhängigen Variable \\(y\\) durch die unabhängige Variable \\(x\\) vorhersagt. Bevor wir eine lineare Regression durchführen, sprechen wir über Regressionslinien und Korrelation. 3.3 Korrelation Die Korrelation, auch Pearson’s correlation \\(r\\), ist ein Maß für die Assoziation zwischen zwei Variablen und kann mit der Funktion cor() berechnet werden. Wir werden hier mit den Data Frame queen arbeiten: queen %&gt;% head() ## # A tibble: 6 × 6 ## Alter f0 F1 F2 F3 F4 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 297. 566. 1873. 2895. 4091. ## 2 28 283. 526. 1846. 2930. 4089. ## 3 29 260. 518. 1785. 2880. 4065. ## 4 30 258. 521. 1786. 2804. 4103. ## 5 31 262. 533. 1819. 2952. 4097. ## 6 32 260. 545. 1694. 2772. 4056. Darin sind die durchschnittlichen Grundfrequenzwerte von Queen Elizabeth II. bei ihren jährlichen Weihnachtsansprachen festgehalten. Uns interessiert, ob das Alter der Queen einen Einfluss auf ihre Grundfrequenz hatte. Erstmal machen wir uns ein Bild von der Lage. Es ist wichtig, dass wir bei Abbildungen für Daten, bei denen wir eine Korrelation vermuten, die unabhängige Variable (hier: Alter) immer auf die x-Achse und die abhängige Variable (hier: f0) immer auf die y-Achse packen. ggplot(queen) + aes(x = Alter, y = f0) + geom_point() Es sieht so aus, als ob es da einen Zusammenhang geben könnte: Je älter die Queen wurde, desto mehr sank ihre Grundfrequenz! Unseren visuellen Eindruck können wir anhand der Korrelation \\(r\\) überprüfen: cor(queen$Alter, queen$f0) ## [1] -0.8346 Die Korrelation \\(r\\) nimmt ausschließlich Werte zwischen -1 und 1 an. Je näher der Wert an Null ist, desto schwächer ist die Abhängigkeit zwischen den beiden Variablen. Mit -0.84 liegt eine starke negative Korrelation vor, d.h. unser visueller Eindruck scheint zu stimmen. 3.4 Die Regressionslinie 3.4.1 Theoretische Informationen Die Regressionslinie der einfachen linearen Regression lässt sich mit folgender Formel beschreiben: \\(y = k + bx\\) Hierbei ist \\(k\\) der y-Achsenabschnitt (engl. intercept) und \\(b\\) die Steigung (engl. slope). Weil Intercept und Slope eine Regressionslinie eindeutig beschreiben, nennt man die beiden Parameter auch Regressionskoeffizienten. Durch die oben gegebene Formel lassen sich bei bekanntem Intercept \\(k\\) und bekannter Slope \\(b\\) also für alle möglichen \\(x\\)-Werte auch die entsprechenden \\(y\\)-Werte vorhersagen. Die Regressionslinie ist immer eine unendliche, exakt gerade Linie und verläuft außerdem durch den Mittelwert der Verteilung. In der folgenden Abbildung sehen Sie drei Regressionslinien: blau und grün haben dasselbe Intercept, aber entgegengesetzte Slopes; blau und orange haben unterschiedliche Intercepts, aber die gleiche Slope. Der genaue Wert der Steigung gibt an, um wie viel der \\(y\\)-Wert steigt oder sinkt, wenn man um eine \\(x\\)-Einheit erhöht. Für \\(x = 0\\) in der Abbildung ist \\(y = 1\\) (für blau und grün). Für \\(x = 1\\) ist \\(y = 1 + b\\), also für blau \\(y = 1 + 0.5 = 1.5\\) und für grün \\(y = 1 + (-0.5) = 0.5\\). Für die orange Linie gilt bei \\(x = 0\\) ist \\(y = 2\\), bei \\(x = 1\\) ist \\(y = 2 + 0.5 = 2.5\\). ggplot() + xlim(-0.5, 1) + ylim(-0.5, 3) + xlab(&quot;x&quot;) + ylab(&quot;y&quot;) + geom_abline(slope = 0.5, intercept = 1, color = &quot;blue&quot;, linewidth = 1.2) + geom_abline(slope = -0.5, intercept = 1, color = &quot;darkgreen&quot;, linewidth = 1.2) + geom_abline(slope = 0.5, intercept = 2, color = &quot;orange&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) Zusammengefasst beschreiben die blaue und orange Linie also eine positive Korrelation zwischen \\(x\\) und \\(y\\) (je größer \\(x\\), desto größer \\(y\\)), die grüne Linie beschreibt eine negative Korrelation (je größer \\(x\\), desto kleiner \\(y\\)). Achtung: Correlation is not causation! Die lineare Regression kann nur die Korrelation zwischen zwei Variablen beschreiben, nicht aber die Kausalität. Die Kausalität bringen wir mit unserem Wissen ins Spiel. Wir wissen also zum Beispiel, dass es der Schlafmangel ist, der eine langsamere Reaktionszeit verursacht. Die lineare Regression kann nur zeigen, ob eine Beziehung zwischen Reaktionszeit und Schlafmangel besteht, aber genauso gut könnte das aus Sicht der Regression bedeuten, dass eine langsamere Reaktionszeit Schlafmangel verursacht. 3.4.2 Regressionslinien mit ggplot2 Um eine Regressionslinie durch eine ggplot2-Abbildung zu legen, können wir entweder geom_abline() (s.o.) oder geom_smooth() benutzen. Die erste Funktion bekommt die Argumente slope und intercept, wie Sie in der Abbildung oben sehen können. Die Funktion geom_smooth() hingegen bekommt das Argument method = \"lm\". “lm” steht für linear model, d.h. damit berechnet die Funktion slope und intercept für uns unter der Annahme, dass die Daten in einer linearen Beziehung stehen. Zusätzlich geben wir das Argument se = F an, weil wir hier keine Konfidenzintervalle angezeigt bekommen wollen. So sieht die Regressionslinie im Fall der Queen aus: ggplot(queen) + aes(x = Alter, y = f0) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F, color = &quot;blue&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Der Unterschied zwischen geom_abline() und geom_smooth() ist, dass geom_abline() eine theoretisch unendlich lange, gerade Linie zeichnet (aber wir sehen natürlich nur einen Ausschnitt davon), während sich geom_smooth() nach dem Wertebereich der Daten richtet. geom_smooth() kann zusätzlich auch andere Arten von Regressionslinien zeichnen. 3.5 Lineare Regression mit lm() Nun sind wir bereit, eine lineare Regression mittels der Funktion lm() durchzuführen. Diese Funktion bekommt als Argumente nur eine Formel und den Data Frame. Die Formel lautet y ~ x, d.h. wir wollen die \\(y\\)-Werte (die Grundfrequenz) in Abhängigkeit von den \\(x\\)-Werten (dem Alter) vorhersagen. Die lineare Regression schätzt Intercept und Slope so ein, dass eine Regressionslinie durch die Datenpunkte gelegt werden kann, die den kleinstmöglichen Abstand zu allen Punkten hat (dieses Verfahren heißt auch least squares, mehr dazu später). queen.lm &lt;- lm(f0 ~ Alter, data = queen) queen.lm ## ## Call: ## lm(formula = f0 ~ Alter, data = queen) ## ## Coefficients: ## (Intercept) Alter ## 288.19 -1.07 Die Koeffizienten lassen sich separat auch mit coef() ausgeben: queen.lm %&gt;% coef() ## (Intercept) Alter ## 288.186 -1.074 Wir sehen also, dass das geschätzte Intercept bei 288.2 liegt und die Steigung bei -1.07. Die Steigung wird verwirrenderweise immer wie die \\(x\\)-Variable genannt, in diesem Fall also “Alter”. Die Koeffizienten bedeuten folgendes: Bei einem Alter von Null Jahren (\\(x = 0\\)) liegt die mittlere Grundfrequenz bei ca. 288 Hz, wenn es einen perfekten linearen Zusammenhang zwischen dem Alter und der Grundfrequenz der Queen gibt. Mit jeden weiteren Jahr (\\(x\\) wird um 1 erhöht) sinkt die Grundfrequenz um 1.07 Hz. Indem wir Intercept und Slope in unsere Formel von vorhin einsetzen, können wir nun für alle möglichen Alter die entsprechende Grundfrequenz vorhersagen: x &lt;- c(0, 40, 50) f0_fitted &lt;- coef(queen.lm)[1] + coef(queen.lm)[2] * x f0_fitted ## [1] 288.2 245.2 234.5 Bei einem Alter von Null Jahren lag die geschätzte Grundfrequenz der Queen wie schon gesagt bei 288 Hz. Bei einem Alter von 40 Jahren waren es vermutlich schon nur noch 245 Hz, bei 50 Jahren 234.5 Hz. Wie Sie sehen, lassen sich anhand des von uns “gefitteten” Modells auch \\(y\\)-Werte vorhersagen bzw. schätzen, die nicht im originalen Datensatz enthalten waren (wie z.B. den f0-Wert für die 50-jährige Queen). Alle diese Punkte liegen aber genau auf der Regressionslinie und da die Regressionslinie unendlich lang ist, ergibt die Schätzung nicht zwangsläufig für alle Werte Sinn. Halten Sie es beispielsweise für wahrscheinlich, dass die Grundfrequenz der Queen bei ihrer Geburt bei 288 Hz lag? Normalerweise haben Kinder eine Grundfrequenz von 300 bis 400 Hz. Sie müssen für Ihre Daten immer wissen, ob die Schätzungen sinnvoll sind oder nicht. In R können Sie die Schätzungen mit der Funktion predict() durchführen, die als Argumente das Modell queen.lm und einen Data Frame mit den \\(x\\)-Werten bekommt, für die \\(y\\) geschätzt werden soll. Dabei muss die \\(x\\)-Variable genauso heißen wie in dem ursprünglichen Data Frame, also hier “Alter”: predict(queen.lm, data.frame(Alter = seq(0, 100, by = 10))) ## 1 2 3 4 5 6 7 8 9 ## 288.2 277.4 266.7 256.0 245.2 234.5 223.8 213.0 202.3 ## 10 11 ## 191.6 180.8 3.6 Residuals Wir sehen oben den geschätzten \\(y\\)-Wert für \\(x = 40\\), also den Grundfrequenzwert für die Queen mit 40 Jahren, nämlich ca. 245 Hz. Der tatsächlich gemessene Wert liegt aber weit darunter: queen %&gt;% filter(Alter == 40) %&gt;% pull(f0) ## [1] 228.7 Die Differenzen zwischen den geschätzten und gemessenen \\(y\\)-Werten werden Residuals genannt. Die folgende Abbildung zeigt einen Ausschnitt aus dem vorherigen Plot für die Altersspanne zwischen 30 und 40 Jahren. Die schwarzen Punkte sind die tatsächlich gemessenen f0-Werte, die roten Punkte hingegen sind die geschätzten Werte. Sie liegen genau auf der blauen Regressionslinie. Die vertikalen gestrichelten Linien stellen die Residuals dar. Diese Abbildung verdeutlicht, weshalb Residuals auch als Error bezeichnet werden. Der Abstand zwischen den tatsächlichen und geschätzten Werten wird berechnet als die Summe über die quadrierten Residuals und heißt deshalb auch sum of squares of error (SSE). Das Verfahren, mit dem die Parameter der Regressionslinie geschätzt werden, heißt least squares, weil es versucht, die SSE so klein wie möglich zuhalten. Das führt dazu, dass die Regressionslinie so durch die Daten gelegt wird, dass alle Datenpunkte so nah wie möglich an der Linie sind. Die Residuals können mit resid() ausgegeben werden: queen.lm %&gt;% resid() ## 1 2 3 4 5 6 ## 36.273 25.151 2.743 1.576 6.633 5.814 ## 7 8 9 10 11 12 ## -3.825 5.283 -7.605 -12.949 -22.552 -8.751 ## 13 14 15 16 17 18 ## -16.571 -6.042 -7.350 -11.662 -7.231 -10.822 ## 19 20 21 22 23 24 ## -1.267 -9.038 6.623 6.383 17.037 10.016 ## 25 26 27 28 29 30 ## 1.064 -9.792 11.148 2.725 -6.990 3.978 SSE kann mit deviance() berechnet werden: queen.lm %&gt;% deviance() ## [1] 4412 3.7 Annahmen überprüfen Statistische Modelle wie die lineare Regression basieren auf Annahmen bezüglich der Daten, die erfüllt sein müssen, damit das Ergebnis des Modells überhaupt aussagekräftig ist. Im Falle der linearen Regression beziehen sich die Annahmen auf die Residuals. 3.7.1 Normalität der Residuals Die erste Annahme ist, dass die Residuals normalverteilt sind. Wir überprüfen das mit einem Q-Q-Plot: ggplot(augment(queen.lm)) + aes(sample = .resid) + stat_qq() + stat_qq_line() + ylab(&quot;samples&quot;) + xlab(&quot;theoretical quantiles&quot;) Das sieht okay, aber nicht perfekt aus. Wir erstellen noch eine Wahrscheinlichkeitsverteilung mit überlagerter Normalverteilung: ggplot(augment(queen.lm)) + aes(x = .resid) + geom_density() + xlim(-40, 40) + xlab(&quot;residuals&quot;) + stat_function(fun = dnorm, args = list(mean = mean(augment(queen.lm)$.resid), sd = sd(augment(queen.lm)$.resid)), inherit.aes = F, color = &quot;blue&quot;) Wenn wir uns jetzt immer noch unschlüssig sind, können wir noch einen Shapiro-Wilk Test durchführen: shapiro.test(augment(queen.lm)$.resid) ## ## Shapiro-Wilk normality test ## ## data: augment(queen.lm)$.resid ## W = 0.94, p-value = 0.1 Da der \\(p\\)-Wert hier höher ist als \\(\\alpha = 0.05\\), scheinen die Residuals annähernd normalverteilt zu sein. 3.7.2 Konstante Varianz der Residuals Die zweite Annahme besagt, dass die Varianz der Residuals für alle geschätzten Werte ähnlich sein sollte. Diese Annahme hat auch den schönen Namen Homoskedastizität. Wenn die Annahme nicht erfüllt wird, spricht man von Heteroskedastizität. Um die Varianz visuell darzustellen, plotten wir die Residuals gegen die geschätzten Werte. Da der Mittelwert der Residuals immer bei ungefähr Null liegt (gestrichelte Linie in der Abbildung bei \\(y = 0\\)), können wir die konstante Varianz daran erkennen, dass sich die Punkte gleichmäßig um diesen Mittelwert verteilen. ggplot(augment(queen.lm)) + aes(x = .fitted, y = .resid) + geom_point() + xlab(&quot;geschätzte f0-Werte&quot;) + ylab(&quot;Residuals&quot;) + geom_hline(yintercept = 0, lty = &quot;dashed&quot;) Da wir hier nur sehr wenige Datenpunkte haben, ist es schwierig einzuschätzen, ob es in dem Plot ein erkennbares Muster gibt, das darauf hinweisen würde, dass die Errors keine konstante Varianz hätten. Die zwei Ausreißer oben rechts sind jedenfalls kein gutes Zeichen, der Rest sieht okay aus. Wir gehen deshalb erstmal davon aus, dass die Residuals homoskedastisch sind. Um Ihre Intuition dafür zu schulen, was gute und schlechte Residual Plots sind, empfehle ich die Abbildungen 6.2 und 6.3 in Winter (2020, S. 111f.). 3.8 Alle Ergebnisse von lm() verstehen Die in diesem Abschnitt genannten Details zur Berechnung der verschiedenen Werte finden sich extrem selten in Büchern über Statistik, Statistik-Blogs, R-Vignetten oder sonstigen Informationsquellen. Sie müssen diese Details nicht auswendig lernen; es geht nur darum, dass Sie die Ergebnisse von lm() nachvollziehen können – und dazu gehört mehr als nur der \\(p\\)-Wert. 3.8.1 Geschätzte y-Werte und Residuals Wie Sie vielleicht bemerkt haben, habe ich die Funktion augment() benutzt, um die Ergebnisse des linearen Modells in ggplot2 nutzen zu können. Diese Funktion stammt aus dem Paket broom, das wir anfangs geladen haben und das noch zwei weitere hilfreiche Funktionen bereitstellt: tidy() und glance(). Das Ergebnis dieser Funktionen ist immer eine tibble, also ein Objekt, das wir easy weiterverarbeiten können (im Gegensatz zu den seltsamen Regressionsobjekten): queen.lm %&gt;% class ## [1] &quot;lm&quot; queen.lm %&gt;% augment() %&gt;% class() ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Schauen wir uns zuerst an, was augment() macht: queen.lm %&gt;% augment() ## # A tibble: 30 × 8 ## f0 Alter .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 297. 26 260. 36.3 0.0946 10.5 0.482 ## 2 283. 28 258. 25.2 0.0845 11.7 0.202 ## 3 260. 29 257. 2.74 0.0798 12.8 0.00225 ## 4 258. 30 256. 1.58 0.0753 12.8 0.000694 ## 5 262. 31 255. 6.63 0.0710 12.7 0.0115 ## 6 260. 32 254. 5.81 0.0670 12.7 0.00826 ## 7 249. 33 253. -3.82 0.0632 12.8 0.00334 ## 8 257. 34 252. 5.28 0.0596 12.7 0.00597 ## 9 243. 35 251. -7.60 0.0563 12.7 0.0116 ## 10 236. 37 248. -12.9 0.0503 12.5 0.0297 ## # ℹ 20 more rows ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; Diese Funktion hängt an die originalen Daten (Spalten f0 und Alter) weitere Spalten an, nämlich die gefitteten (also die vom Modell geschätzten) f0-Werte .fitted, die Residuals .resid, und weitere, die erstmal nicht interessant für uns sind. 3.8.2 Regressionskoeffizienten und \\(t\\)-Statistik Die Funktion tidy() gibt eine Tabelle für die geschätzten Regressionskoeffizienten zurück: queen.lm %&gt;% tidy() ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 288. 6.98 41.3 1.23e-26 ## 2 Alter -1.07 0.134 -8.02 9.93e- 9 Die Spalte estimate enthält die Schätzungen für Intercept (erste Zeile) und Slope (zweite Zeile). Die Spalte std.error enthält den sogenannten Standard Error, ein Maß für die Genauigkeit der Schätzung. Hier wollen wir möglichst kleine Werte (in Relation zum Estimate), weil das bedeutet, dass die Schätzungen des Modells für die Regressionskoeffizienten präzise sind. Es folgt eine Spalte mit der Teststatistik statistic. Bisher haben wir noch nicht über die statistische Signifikanz der Regression gesprochen. Hier wird ein \\(t\\)-Test durchgeführt, um herauszufinden, ob sich die Schätzungen der Regressionskoeffizienten signifikant von Null unterscheiden. Wenn die Regressionskoeffizienten nahe Null sind, tragen sie nichts dazu bei, den \\(y\\)-Wert vorherzusagen (erinnern Sie sich an die Formel für die Regressionslinie: Wenn \\(k\\) oder \\(b\\) gleich Null sind, haben sie keinen Einfluss auf die Regressionslinie). Der Wert in der Spalte statistic, der in diesem Fall auch \\(t\\)-Wert genannt wird, wird berechnet als estimate / std.error, so z.B. für die Slope: as.numeric(tidy(queen.lm)[2, &quot;estimate&quot;] / tidy(queen.lm)[2, &quot;std.error&quot;]) ## [1] -8.016 Die \\(t\\)-Statistik hat ihre eigene Wahrscheinlichkeitsdichteverteilung, genannt Student-\\(t\\)-Verteilung oder einfach \\(t\\)-Verteilung, die wir (analog zur Funktion dnorm() für Normalverteilungen) mit der Funktion dt() in ggplot2 zeichnen lassen können. Diese Funktion bekommt ein Argument namens df, das steht in diesem Fall für degrees of freedom (Freiheitsgrade). Die Freiheitsgrade sind üblicherweise die Größe der Stichprobe minus die Anzahl der Koeffizienten, also hier nrow(queen) - 2. ggplot() + xlim(-5, 5) + stat_function(fun = dnorm) + stat_function(fun = dt, args = list(df = 28), col = &quot;orange&quot;) + stat_function(fun = dt, args = list(df = 5), col = &quot;darkgreen&quot;) + xlab(&quot;&quot;) + ylab(&quot;density&quot;) Hier sehen Sie die \\(t\\)-Verteilung in orange gegen die schwarze Normalverteilung mit Mittelwert Null und Standardabweichung Eins; die beiden Verteilungen sind sich sehr ähnlich. Mit abnehmenden Freiheitsgeraden (zum Beispiel 5 Freiheitsgrade bei der dunkelgrünen Verteilung) werden sich die Normal- und \\(t\\)-Verteilung unähnlicher. Wie Sie wissen, ist die Fläche unter diesen Verteilungen immer 1, d.h. auch bei der \\(t\\)-Verteilung können wir mittels einer Funktion berechnen, wie groß die Wahrscheinlichkeit ist, dass ein Wert in einen bestimmten Wertebereich fällt. Der \\(t\\)-Wert für die Slope ist in unserem Beispiel ca. -8.02. Unter der orangen \\(t\\)-Verteilung, die zu unseren Daten passt, ist nur eine sehr sehr kleine Fläche unter der Kurve für den Wertebereich von minus unendlich bis -8.02 (um dies nachzuvollziehen, brauchen Sie ein bisschen Fantasie, da der Wertebereich in der Abbildung oben erst bei -5 beginnt). In Anlehnung an pnorm() heißt die Funktion für die Berechnung der Fläche unter der \\(t\\)-Verteilung pt(). So können wir unter Einsatz des \\(t\\)-Werts und der Freiheitsgrade den \\(p\\)-Wert berechnen: 2 * pt(-8.016248, df = 28) ## [1] 9.929e-09 Weiterführende Infos: two-tailed t-Test Wir müssen den Wahrscheinlichkeitswert, den pt() berechnet, mit 2 multiplizieren, weil es sich bei dem berechneten \\(t\\)-Test nicht um einen one-tailed, sondern um einen two-tailed \\(t\\)-Test handelt. Als tail werden die extremen Enden der Verteilungen bezeichnet; für Normal- und \\(t\\)-Verteilung gilt, dass sehr hohe und sehr niedrige Werte unwahrscheinlich sind (d.h. es ist nur sehr wenig Fläche unter den Verteilungen von minus unendlich bis zu einem sehr niedrigen x-Wert bzw. von einem sehr hohen x-Wert bis plus unendlich). Wenn wir hier mit pt() eine Wahrscheinlichkeit (bzw. Fläche) berechnen, gilt das nur für den lower tail von minus unendlich bis zu dem angegebenen x-Wert. Dieselbe Wahrscheinlichkeit gilt aber für die Fläche von dem ins Positive verkehrten x-Wert (hier: abs(-8.016248)) bis plus unendlich (den upper tail). Daher machen wir es uns hier einfach, indem wir das Ergebnis oben mit 2 multiplizieren. Wir hätten auch schreiben können: pt(-8.016248, df = 28) + pt(abs(-8.016248), df = 28, lower.tail = F) ## [1] 9.929e-09 Etwas Ähnliches haben Sie übrigens schon bei der Berechnung des 95%-Konfidenzintervalls für Normalverteilungen gesehen: Da ging es uns darum, die Fläche von 0.05 gleichmäßig auf beide symmetrischen Hälften der Verteilung aufzuteilen, d.h. wir haben auch da beide tails berücksichtigt, und nicht nur einen von beiden. Der \\(p\\)-Wert kann hier wie folgt interpretiert werden: Wenn wir davon ausgehen, dass die tatsächliche Steigung Null ist (das ist die Null-Hypothese dieses \\(t\\)-Tests), dann ist die hier beobachtete Steigung von -1.07 höchst unerwartet. 3.8.3 Gütekriterien für das Modell und F-Statistik Nun, da wir die Ausgabe von tidy() verstehen, bleibt nur noch glance(). Die Funktion glance() zeigt auf einen Blick ein paar Kriterien, mit denen man die Güte des Modells (auch: goodness-of-fit) einschätzen kann (für bessere Lesbarkeit verwandeln wir den Data Frame ins lange Format): queen.lm %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 6.97e-1 ## 2 adj.r.squared 6.86e-1 ## 3 sigma 1.26e+1 ## 4 statistic 6.43e+1 ## 5 p.value 9.93e-9 ## 6 df 1 e+0 ## 7 logLik -1.17e+2 ## 8 AIC 2.41e+2 ## 9 BIC 2.45e+2 ## 10 deviance 4.41e+3 ## 11 df.residual 2.8 e+1 ## 12 nobs 3 e+1 Der Wert r.squared ist genau das, was der Name sagt: der quadrierte Korrelationswert \\(r\\), den wir oben mit cor() berechnet hatten: cor(queen$Alter, queen$f0)^2 ## [1] 0.6965 \\(R^2\\) beschreibt die Proportion der Varianz in den Daten, die von dem berechneten Modell beschrieben wird. Hier wird also ca. 70% der Varianz in den Daten durch das Modell mit einem Prädiktor (d.h. einer unabhängigen Variable) beschrieben. In der Linguistik sind viel niedrigere \\(R^2\\)-Werte üblicher, weil unser Untersuchungsgegenstand häufig von vielen zufälligen Faktoren beeinflusst wird, die wir nicht erfassen können. Der Wert adj.r.squared ist eine Form des \\(R^2\\), die für die Anzahl der unabhängigen Variablen normalisiert. Das ist wichtig, denn bei einer höheren Zahl von unabhängigen Variablen wird \\(R^2\\) automatisch steigen, selbst wenn eine oder mehrere der Variablen aus statistischer Sicht nichts dazu beitragen, die y-Werte zu erklären bzw. zu schätzen. Der adjusted \\(R^2\\) hingegen bezieht die Anzahl der unabhängigen Variablen in die Berechnung von \\(r^2\\) ein und ist daher verlässlicher als der einfache \\(R^2\\). Da es hier nur eine Variable gibt, sind sich \\(R^2\\) und adjusted \\(R^2\\) sehr ähnlich. Die Spalte sigma enthält den Residual Standard Error, das ist eine Schätzung für die Standardabweichung der Fehlerverteilung. Diesen Wert können wir mit sigma() berechnen (wir werden gleich nochmal auf diesen Wert zurückkommen): queen.lm %&gt;% sigma() ## [1] 12.55 Dann sehen wir wieder eine statistic samt p.value. Dieses Mal handelt es sich um die \\(F\\)-Statistik, d.h. diesmal lesen wir den \\(p\\)-Wert aus der \\(F\\)-Verteilung ab, die mit df() in ggplot2 gezeichnet werden kann (der Funktionsname steht für \\(F\\) distribution, und nicht für degrees of freedom). Diese Verteilung ist von zwei Parametern abhängig, nämlich glance(queen.lm)$df und glance(queen.lm)$df.residual. Dies sind die Freiheitsgrade für das Modell und für die Residuals. In orange sehen Sie wieder die \\(F\\)-Verteilung, die zu unseren Daten passt (nämlich mit einem Freiheitsgrad für das Modell und 28 Freiheitsgraden für die Residuals). Die Verteilung kann nur Werte größer als Null annehmen. In dunkelgrün sehen Sie eine \\(F\\)-Verteilung, bei der die Freiheitsgrade für das Modell verändert wurden, und in schwarz zum Vergleich die Normalverteilung. ggplot(data.frame(x = 0:5)) + aes(x) + stat_function(fun = dnorm) + stat_function(fun = df, args = list(df1 = 1, df2 = 28), col = &quot;orange&quot;) + stat_function(fun = df, args = list(df1 = 7, df2 = 28), col = &quot;darkgreen&quot;) + xlab(&quot;&quot;) + ylab(&quot;density&quot;) ## Warning: Computation failed in `stat_function()`. ## Caused by error in `fun()`: ## ! could not find function &quot;fun&quot; ## Warning: Computation failed in `stat_function()`. ## Caused by error in `fun()`: ## ! could not find function &quot;fun&quot; Wenn wir uns hier die orange Verteilung anschauen, sehen wir schon, dass ein \\(F\\)-Wert von 64.3 (s. statistic) extrem unwahrscheinlich wäre, daher ist der \\(p\\)-Wert auch sehr klein. Die Null-Hypothese im \\(F\\)-Test, der hier ausgeführt wurde, lautet, dass ein Modell ohne Prädiktoren die Daten genauso gut oder besser erklärt, als das Modell mit dem Prädiktor Alter. Das heißt, wenn der \\(p\\)-Wert, der aus dem \\(F\\)-Test resultiert, sehr klein ist, können wir daraus schließen, dass unser Modell mit der unabhängigen Variable Alter die Daten besser erklärt, als ein Modell ohne Prädiktoren. Wir versuchen nun die Werte in den Spalten statistic, df, df.residual und deviance nachzuvollziehen. Letzteren kennen Sie schon unter dem Namen SSE (sum of squared error): SSE &lt;- queen.lm %&gt;% deviance() SSE ## [1] 4412 df ist wie bereits angedeutet die Anzahl der Freiheitsgrade des Modells und wird berechnet als die Anzahl der Regressionskoeffizienten \\(k\\) minus 1: k &lt;- length(queen.lm$coefficients) df.SSR &lt;- k-1 df.SSR ## [1] 1 df.residual ist die Anzahl der Freiheitsgrade der Residuals, die als die Anzahl der Beobachtungen \\(N\\) minus die Anzahl der Regressionskoeffizienten \\(k\\) berechnet wird. N &lt;- queen %&gt;% nrow() df.SSE &lt;- N-k df.SSE ## [1] 28 Um zuletzt noch den \\(F\\)-Wert in der Spalte statistic nachvollziehen zu können, schauen wir uns die Formel dafür an: \\(F = \\frac{MSR}{MSE}\\). Hierbei steht MSR für mean squares of regression und MSE für mean squares of error. Diese beiden Werte beschreiben die Varianz der geschätzten y-Werte und die Varianz der Residuals. Um MSE und MSR berechnen zu können, müssen wir zuerst zwei andere Werte berechnen: sum of squares of Y (SSY), die den Abstand der Datenpunkte zum Mittelwert von \\(y\\) beschreibt, und sum of squares of regression (SSR), die den Unterschied zwischen SSY und SSR beschreibt. Zuerst SSY: SSY &lt;- sum((queen$f0 - mean(queen$f0))^2) SSY ## [1] 14537 In der Praxis ist SSY die Summe aller Abstände zwischen den schwarzen (tatsächlich gemessenen) Datenpunkten und der orangen Linie, die die y-Achse bei mean(queen$f0) schneidet und eine Steigung von Null hat (diese Abbildung zeigt wieder nur einen Ausschnitt aus dem gesamten Wertebereich): Sie können hier schon rein visuell sehen, dass die orange Linie die Daten viel schlechter beschreibt als die blaue Regressionslinie, deshalb ist SSY auch viel größer als SSE. Für SSR müssen wir nun nur noch die Differenz von SSY und SSE bilden: SSR &lt;- SSY - SSE SSR ## [1] 10125 Nun können wir endlich MSE und MSR berechnen: MSE &lt;- SSE/df.SSE MSE ## [1] 157.6 MSR &lt;- SSR/df.SSR MSR ## [1] 10125 …Und aus der Division von MSE und MSR entsteht der \\(F\\)-Wert: F_wert &lt;- MSR / MSE F_wert ## [1] 64.26 Zwischen der \\(t\\)-Statistik und unserer \\(F\\)-Statistik besteht übrigens ein quadratischer Zusammenhang: \\(F = t^2\\) bzw. \\(t = \\sqrt{F}\\) sqrt(F_wert) ## [1] 8.016 Deshalb ist auch der \\(p\\)-Wert bei beiden Statistiken genau derselbe. Zuletzt kommen wir nochmal auf den residual standard error zurück, den wir oben als eine Schätzung für die Standardabweichung der Residuen bezeichnet hatten. Dieser berechnet sich als die Quadratwurzel aus MSE: sqrt(MSE) ## [1] 12.55 Wenn wir die Standardabweichung der Residuen ermitteln, sollte das sehr nah an dem residual standard error liegen (allerdings ist letzterer nur eine Schätzung, daher müssen die Werte nicht gleich sein): queen.lm %&gt;% augment() %&gt;% pull(.resid) %&gt;% sd() ## [1] 12.33 Wenn der residual standard error genau Null ist, dann liegen alle Datenpunkte exakt auf der Regressionslinie, d.h. dann kann jeder y-Wert aus dem Dataset genau durch den dazugehörigen x-Wert mittels eines linearen Modells berechnet werden. 3.9 Ergebnis berichten Nun haben wir alle (für uns relevanten) Ergebnisse aus lm() nachvollzogen. Wir haben dies mittels der broom Funktionen gemacht. Eine traditionellere Art der Übersicht über die Ergebnisse der linearen Regression bietet summary(): queen.lm %&gt;% summary() ## ## Call: ## lm(formula = f0 ~ Alter, data = queen) ## ## Residuals: ## Min 1Q Median 3Q Max ## -22.55 -8.46 -0.10 6.24 36.27 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 288.186 6.977 41.31 &lt; 2e-16 *** ## Alter -1.074 0.134 -8.02 9.9e-09 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.6 on 28 degrees of freedom ## Multiple R-squared: 0.697, Adjusted R-squared: 0.686 ## F-statistic: 64.3 on 1 and 28 DF, p-value: 9.93e-09 Sie sollten hier alle Zahlen wiedererkennen. Es ist außerordentlich wichtig, dass wir unser Ergebnis korrekt berichten. Dafür benötigen wir \\(R^2\\) (oder, weil stabiler: adjusted \\(R^2\\)): 0.69 den \\(F\\)-Wert: 64.3 die Freiheitsgrade für das Modell und die Residuals: 1 und 28 den \\(p\\)-Wert, bzw. das nächst höhere Signifikanzniveau: \\(p\\) &lt; 0.001 Wir berichten: Es besteht eine signifikante lineare Beziehung zwischen dem Alter der Queen und ihrer Grundfrequenz (\\(R^2\\) = 0.69, \\(F\\)[1, 28] = 64.3, \\(p\\) &lt; 0.001). 3.10 Zusammenfassung Bei der linearen Regression werden die Werte der abhängigen Variable \\(y\\) durch die Werte der unabhängigen Variable \\(x\\) geschätzt unter der Annahme, dass zwischen beiden eine lineare Beziehung besteht. Die Regressionslinie ist die gerade Linie, zu der der Abstand der Datenpunkte möglichst gering ist (least squares Verfahren). Die Funktion lm() schätzt die Regressionskoeffizienten y-Achsenabschnitt (intercept) und Steigung (slope). Es wird ein \\(t\\)-Test ausgeführt, um zu testen, ob sich die Regressionskoeffizienten von Null unterscheiden. Wenn \\(p &lt; 0.05\\) im \\(t\\)-Test für \\(x\\), dann unterscheidet sich die Steigung signifikant von Null, d.h. \\(x\\) ist ein guter Prädiktor für \\(y\\). Die Differenzen zwischen den tatsächlichen und den geschätzten y-Werten heißen Residuals oder Error; der residual standard error ist eine Schätzung für die Standardabweichung der Fehlerverteilung. \\(R^2\\) ist das Quadrat des Korrelationswertes \\(r\\) und beschreibt die Proportion der Varianz in der abhängigen Variable \\(y\\), die von dem linearen Modell beschrieben wird. Es wird außerdem ein \\(F\\)-Test ausgeführt, um zu prüfen, ob das lineare Modell erfolgreich einen signifikanten Anteil der Varianz in der abhängigen Variable erklärt. Wenn \\(p &lt; 0.05\\) im \\(F\\)-Test, dann beschreibt das Modell mit dem gewählten Prädiktor besser die empirischen Daten als ein Modell ohne Prädiktoren (Mittelwertmodell). SSE ist die sum of squares of error und beschreibt den Abstand der Datenpunkte zur Regressionslinie; im least squares Verfahren wird versucht, SSE zu minimieren SSR ist die sum of squares of regression und beschreibt, wie gut das Regressionsmodell im Vergleich zum Mittelwertmodell ist (SSY) "],["multiple-lineare-regression.html", "4 Multiple Lineare Regression 4.1 Packages und Daten laden 4.2 Einführung 4.3 Kontinuierliche unabhängige Variablen 4.4 Kategorische unabhängige Variablen 4.5 Mix aus kontinuierlichen und kategorischen unabhängigen Variablen", " 4 Multiple Lineare Regression 4.1 Packages und Daten laden Laden Sie die Pakete und Data Frames: library(broom) library(emmeans) ## Welcome to emmeans. ## Caution: You lose important information if you filter this package&#39;s results. ## See &#39;? untidy&#39; library(tidyverse) library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; faux &lt;- read.table(file.path(url, &quot;faux.txt&quot;), stringsAsFactors = T, header = T) %&gt;% as_tibble() int &lt;- read.table(file.path(url, &quot;dbwort.df.txt&quot;), stringsAsFactors = T) %&gt;% as_tibble() %&gt;% rename(vowel = V, gender = G, subject = Vpn, word = Wort) vlax &lt;- read.table(file.path(url, &quot;vlax.txt&quot;), stringsAsFactors = T) %&gt;% as_tibble() %&gt;% filter(V %in% c(&quot;O&quot;, &quot;I&quot;) &amp; f0 != 0) %&gt;% rename(vowel = V, subject = Vpn, word = Wort, duration = Dauer) 4.2 Einführung Bislang haben wir mit der einfachen linearen Regression gearbeitet, bei der eine Variable (z.B. Grundfrequenz) durch eine weitere Variable (z.B. Alter in Jahren) vorhergesagt wurde. Viel häufiger haben wir aber mehrere Variablen, von denen wir vermuten, dass sie einen Einfluss auf unsere Messwerte haben. Hierfür brauchen wir dann die multiple lineare Regression. Die Formel für die lineare Regression wird so angepasst, dass es für jede unabhängige Variable \\(x_1\\), \\(x_2\\), usw. eine eigene Slope gibt: \\(y = k + b_1x_1 + b_2x_2 + ...\\) Auch hier steht \\(k\\) wieder für den y-Achsenabschnitt, \\(b_1\\), \\(b_2\\) usw. sind Slopes. Im Folgenden zeige ich Ihnen drei Beispiele für multiple lineare Regressionen, je nach der Art der unabhängigen Variablen (kategorial oder kontinuierlich). Aus Gründen der Übersichtlichkeit enthalten alle Beispiele nur zwei unabhängige Variablen; es können aber natürlich generell auch mehr als zwei Prädiktoren in der Regression vorkommen. 4.3 Kontinuierliche unabhängige Variablen Zuerst wollen wir herausfinden, ob die Grundfrequenz f0 im (artifiziell erstellten) Data Frame faux von den beiden kontinuierlichen Variablen dB (Lautstärke) und dur (Wortdauer) abhängig ist. Dafür erstellen wir eine Abbildung von den Daten: ggplot(faux) + aes(x = dB, y = f0, col = dur) + geom_point() Es ist nicht ganz einfach, diesen Plot zu interpretieren. Wenn wir uns erst einmal nur auf die Abhängigkeit der Grundfrequenz von der Lautstärke konzentrieren, sehen wir, dass eine klare positive Korrelation vorliegt, d.h. je lauter die Versuchspersonen sprachen, desto höher wurde ihre Grundfrequenz. Da die Dauer ebenfalls eine kontinuierliche Variable ist, bleibt uns nur, ein Farbkontinuum für die Visualisierung der Variable zu nutzen (ggplot() macht das automatisch durch das Argument col). Es scheint so, als ob die dunkleren Punkte (= niedrige Dauerwerte) mit eher hohen f0-Werten assoziiert sind und die helleren Datenpunkte (= hohe Dauerwerte) eher mit niedrigen f0-Werten. Hier könnte also eine negative Korrelation zwischen f0 und Dauer vorliegen. Mittels cor() können wir diesen Eindruck überprüfen: cor(faux$f0, faux$dB) ## [1] 0.3107 cor(faux$f0, faux$dur) ## [1] -0.53 Da wir es hier mit zwei unabhängigen Variablen zu tun haben, müssen wir auch überlegen, ob eine Interaktion zwischen ihnen vorliegen könnte. Eine Interaktion liegt vor, wenn sich der Effekt einer unabhängigen auf die abhängige Variable für verschiedene (extreme) Werte der zweiten unabhängigen Variable unterscheidet. In unserem Beispiel ist der Effekt von Lautstärke auf f0 im Allgemeinen ein positiver, d.h. je lauter gesprochen wurde, desto höher f0. Dieser Effekt ist jedoch (visuell) stärker ausgeprägt für niedrige als für hohe Dauerwerte. Stellen Sie sich vor, Sie würden zwei Regressionslinien durch die obige Abbildung legen: eine dunkelblaue für einen beispielhaft niedrigen Dauerwert (z.B. 150 ms) und eine hellblaue für eine beispielhaft hohe Dauer (z.B. 450 ms). Wenn keine Interaktion zwischen den Variablen besteht, sind die zwei beispielhaften Regressionslinien parallel zueinander; andernfalls sind sie es nicht. Für die Daten im Data Frame faux habe ich die zwei beispielhaften Regressionslinien eingezeichnet: im linken Plot unter der Annahme, dass keine Interaktion besteht, im rechten unter der Annahme, dass eine Interaktion besteht: Die Regressionslinien im rechten Plot scheinen die Daten besser zu beschreiben als die Daten im linken Plot; d.h. wir gehen davon aus, dass zwischen Lautstärke und Dauer in dem Datensatz eine Interaktion besteht. Wir werden später sehen, wie genau die Regressionslinien für die beiden Plots berechnet wurden. Weiterführende Infos: Regressionslinien bei zwei kontinuierlichen Prädiktoren Es erfordert ein bisschen Rechenarbeit, Regressionslinien für zwei kontinuierliche Variablen zu zeichnen. Wenn Sie diese Arbeit irgendwann vermeiden wollen, können Sie sich aber die Libraries ggiraph und ggiraphExtra installieren. In dieser Vignette finden Sie Informationen über die Funktion ggPredict() aus ggiraphExtra, die Regressionslinien für multiple Prädiktoren zeichnen kann. Wir werden im Folgenden für viele Fragestellungen zuerst eine multiple Regression ohne und dann mit Interaktion berechnen, damit Sie sich mit Interaktionen vertraut machen können. Wenn Sie selbst vor der Entscheidung stehen, ob Sie in Ihrem Datensatz eine Interaktion zwischen unabhängigen Variablen vorliegen könnte, schauen Sie sich Ihren Datensatz in Abbildungen an und überlegen Sie genau, wie eine Interaktion zu interpretieren wäre – bevor Sie die Regression durchführen. 4.3.1 Ohne Interaktion Wenn es in einem Modell keine Interaktionen gibt, werden die unabhängigen Variablen in der lm()-Formel mit einem Plus verbunden. lm1 &lt;- lm(f0 ~ dB + dur, data = faux) lm1 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 169. ## 2 dB 2.89 ## 3 dur -0.372 Versuchen wir zu verstehen, was genau die Schätzungen in der Spalte estimate bedeuten. Die Schätzung für das Intercept ist auch in der multiplen Regression das geschätzte arithmetische Mittel der abhängigen Variable für \\(x_1 = 0\\) und \\(x_2 = 0\\) (und alle möglichen weiteren \\(x\\), wenn es noch mehr unabhängige Variablen gibt). Das heißt, wenn die Lautstärke 0 dB beträgt und die Dauer ebenfalls 0 ms ist, sollte die Grundfrequenz bei 168.6 Hz liegen. Die Slopes messen nun nicht mehr den Effekt jeder einzelnen unabhängigen Variable, sondern den Effekt einer unabhängigen Variable, während alle anderen Variablen konstant bei Null gehalten werden. Jede Erhöhung der Lautstärke um ein Dezibel (bei konstanter Dauer) führt zu einer Erhöhung der Grundfrequenz um 2.9 Hz, jede Erhöhung der Dauer um eine Millisekunde (bei konstanter Lautstärke) führt zu einer Reduktion der Grundfrequenz um 0.4 Hz. Das heißt hier finden wir die erwarteten Zusammenhänge wieder: Lautstärke beeinflusst die Grundfrequenz positiv, Dauer negativ. Rufen wir uns nochmal die Formel für die Regressionslinie bei zwei unabhängigen Variablen in Erinnerung: \\(y = k + b_1x_1 + b_2x_2\\) Die Werte für den y-Achsenabschnitt \\(k\\) und die beiden Slopes \\(b_1\\) und \\(b_2\\) können wir uns aus dem Data Frame holen, den tidy() zurückgibt: k &lt;- lm1 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) k ## [1] 168.6 b_1 &lt;- lm1 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_1 ## [1] 2.892 b_2 &lt;- lm1 %&gt;% tidy() %&gt;% filter(term == &quot;dur&quot;) %&gt;% pull(estimate) b_2 ## [1] -0.3721 Nun können wir uns nach Belieben Werte für \\(x_1\\) (Lautstärke in Dezibel) und \\(x_2\\) (Dauer in Millisekunden) ausdenken, zusammen mit den Regressionskoeffizienten in die Formel einsetzen, und so \\(y\\)-Werte schätzen. Wenn wir beide \\(x\\)-Werte auf Null setzen, kommt genau das Intercept \\(k\\) als geschätzter \\(y\\)-Wert heraus: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur \\\\ &amp;= 168.64 + 2.89 \\cdot 0 + (-0.37 \\cdot 0) \\\\ &amp;= 168.64 \\end{aligned} \\] Wenn Sie diese Werte nicht manuell berechnen möchten, steht Ihnen natürlich die Funktion predict() zur Verfügung. Bei der multiplen linearen Regression muss der Funktion ein Data Frame übergeben werden, der so viele Spalten hat wie es unabhängige Variablen in der Regression gab; die Spalten müssen auch genau wie die unabhängigen Variablen benannt sein. Zu Demonstrationszwecken zeige ich hier, dass die Grundfrequenz f0 laut dem berechneten linearen Modell wirklich bei ca. 168 Hz liegt, wenn die Lautstärke bei 0 dB und die Dauer bei 0 ms liegt. Außerdem zeige ich den geschätzten f0-Wert für db = 1 und dur = 0 (der 2.9 Hz höher ist als das Intercept) und für db = 0 und dur = 1 (der 0.4 Hz niedriger ist als das Intercept). Zuletzt schätzen wir mittels predict() den f0-Wert für die durchschnittliche Lautstärke und Dauer. d1 &lt;- data.frame(dB = c(0, 1, 0, mean(faux$dB)), dur = c(0, 0, 1, mean(faux$dur))) d1 %&lt;&gt;% mutate(estimated_f0 = predict(lm1, d1)) d1 ## dB dur estimated_f0 ## 1 0.0 0.0 168.6 ## 2 1.0 0.0 171.5 ## 3 0.0 1.0 168.3 ## 4 60.1 298.9 231.3 Jetzt können wir auch nachvollziehen, wie die zwei beispielhaften Regressionslinien im linken Plot oben berechnet wurden. Wir wählen 450 ms als beispielhaft hohen und 150 ms als beispielhaft niedrigen Dauerwert (in der Literatur wird als “extremer Wert” für eine unabhängige Variable auch gerne ihr Mittelwert plus/minus 1.5 Standardabweichungen genommen). Anschließend setzen wir diese Werte zusammen mit den Regressionskoeffizienten in unsere Formel für die Regressionslinie ein (hier zuerst für eine Dauer von 450 ms): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-0.37 \\cdot 450) \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-167.43) \\\\ &amp;= 1.21 + 2.89 \\cdot dB \\end{aligned} \\] Dabei kommt heraus, dass das Intercept einer Regressionslinie für eine Dauer von 450 ms bei 1.21 liegen sollte und die Steigung bei 2.89. \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-0.37 \\cdot 150) \\\\ &amp;= 168.64 + 2.89 \\cdot dB + (-55.81) \\\\ &amp;= 112.83 + 2.89 \\cdot dB \\end{aligned} \\] Bei der Regressionslinie für eine Dauer von 150 ms hingegen liegt das Intercept bei 112.83 und die Steigung bei 2.89. Die Slope ist also für die beiden Regressionslinien dieselbe, ergo sind diese Linien parallel zueinander. Die beiden berechneten Intercepts und die Slope können nun für geom_abline() benutzt werden: high_dur &lt;- 450 low_dur &lt;- 150 slope &lt;- b_1 intercept_high_dur &lt;- k + b_1 * 0 + b_2 * high_dur intercept_low_dur &lt;- k + b_1 * 0 + b_2 * low_dur ggplot(faux) + aes(x = dB, y = f0, col = dur) + geom_point() + xlim(0, 95) + ylim(0, 500) + geom_abline(slope = slope, intercept = intercept_high_dur, color = &quot;#56B1F7&quot;, linewidth = 1.2) + geom_abline(slope = slope, intercept = intercept_low_dur, color = &quot;#132B43&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) Wir hätten natürlich auch dur auf die x-Achse setzen und dB durch ein Farbkontinuum darstellen können. Dann müssten wir zwei beispielhaft extreme Lautstärkewerte wählen, wieder alle Werte (Intercept, Slopes, gewählte Beispielwerte für dB) in die Formel einsetzen und kämen auf folgendes Ergebnis: high_dB &lt;- 75 low_dB &lt;- 45 intercept_high_dB &lt;- k + b_1 * high_dB + b_2 * 0 intercept_low_dB &lt;- k + b_1 * low_dB + b_2 * 0 slope &lt;- b_2 ggplot(faux) + aes(x = dur, y = f0, col = dB) + geom_point() + xlim(0, 600) + ylim(0, 500) + geom_abline(slope = slope, intercept = intercept_high_dB, color = &quot;#56B1F7&quot;, linewidth = 1.2) + geom_abline(slope = slope, intercept = intercept_low_dB, color = &quot;#132B43&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) Wenn Sie sich ausreichend Zeit genommen haben, um die Regressionskoeffizienten zu verstehen und zu interpretieren, können wir uns jetzt den Statistiken und Gütekriterien des Modells zuwenden. Mit tidy() schauen wir uns die \\(t\\)-Statistik samt \\(p\\)-Wert an, die aussagen, ob sich die Regressionskoeffizienten signifikant von Null unterscheiden: lm1 %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 169. 3.45 48.8 0 ## 2 dB 2.89 0.0633 45.7 1.91e-306 ## 3 dur -0.372 0.00668 -55.7 0 Die beiden Regressionskoeffizienten für die unabhängigen Variablen unterscheiden sich laut den Tests signifikant von Null, d.h. sowohl die Lautstärke als auch die Dauer scheinen gute Prädiktoren für die Grundfrequenz zu sein. Die Gütekriterien sind wieder die \\(F\\)-Statistik sowie der \\(R^2\\)-Wert: lm1 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 0.661 ## 2 adj.r.squared 0.660 ## 3 sigma 24.0 ## 4 statistic 1815. ## 5 p.value 0 ## 6 df 2 ## 7 logLik -8589. ## 8 AIC 17186. ## 9 BIC 17208. ## 10 deviance 1073556. ## 11 df.residual 1866 ## 12 nobs 1869 Die beiden unabhängigen Variablen beschreiben 66% der Varianz in den Grundfrequenzwerten. Allgemein betrachtet ist das ein relativ hoher Wert. Noch mehr Bedeutung bekommt der Wert jedoch im Vergleich zu den \\(R^2\\)-Werten aus den zwei einfachen linearen Regressionen, die wir mittels der unabhängigen Variablen erstellen können: lm(f0 ~ dB, data = faux) %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.09651 lm(f0 ~ dur, data = faux) %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.2809 Das heißt, ein Modell mit der Lautstärke als einziger unabhängiger Variable beschreibt nur knapp 10% der Varianz in den Grundfrequenzwerten, ein Modell mit der Dauer beschreibt ca. 28% dieser Varianz. Wenn man jedoch beide Prädiktoren in einem Modell (ohne Interaktion) vereint, steigt dieser Anteil auf 66%. Zuletzt sollten wir nicht vergessen, das Ergebnis der Regression zu berichten: Eine multiple lineare Regression mit Lautstärke und Dauer als unabhängige Variablen zeigte einen signifikanten Effekt von Lautstärke (\\(t\\) = 45.7, \\(p\\) &lt; 0.001) und Dauer (\\(t\\) = 55.7, \\(p\\) &lt; 0.001) auf die Grundfrequenz. Das gewählte Modell beschreibt die Daten besser als ein Modell ohne Prädiktoren (\\(R^2\\) = 0.66, \\(F\\)[2, 1866] = 1815, \\(p\\) &lt; 0.001). In einer wissenschaftlichen Veröffentlichung sollten Sie natürlich noch ein paar Zeilen darauf verwenden zu beschreiben, in welche Richtung die signifikanten Effekte gehen (d.h. ob sie die abhängige Variable positiv oder negativ, stark oder schwach beeinflussen) und ob das den Erwartungen (Hypothesen) entspricht, die Sie idealerweise vor der Datenerhebung aufgestellt haben. (Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!) 4.3.2 Mit Interaktion Da wir in unserer ersten Abbildung der Daten in faux festgestellt haben, dass es eine Interaktion zwischen Lautstärke und Dauer geben könnte, wollen wir das Modell jetzt mit Interaktion berechnen. Interaktionen in einem linearen Modell können auf zwei Arten geschrieben werden: entweder mit einem Asterisk dB * dur oder in der ausformulierten Variante dB + dur + dB:dur. Bei drei unabhängigen Variablen A, B und C können sowohl Interaktionen zwischen zwei als auch zwischen allen drei Faktoren vorliegen, also: A * B * C oder A + B + C + A:B + A:C + B:C + A:B:C. Es kann auch sinnvoll sein, eine Interaktion nur zwischen zwei von drei Faktoren zu berechnen, z.B. A * B + C oder A + B + C + A:B. Hier wählen wir die Kurzschreibweise und schauen uns zuerst wieder die Schätzungen der Regressionskoeffizienten an: lm2 &lt;- lm(f0 ~ dB * dur, data = faux) lm2 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 86.0 ## 2 dB 4.30 ## 3 dur -0.0921 ## 4 dB:dur -0.00466 Der y-Achsenabschnitt für dB = 0 und dur = 0 liegt jetzt bei 86 Hz. Die Slopes beschreiben nun den Einfluss einer unabhängigen Variable, wenn alle anderen unabhängigen Variablen konstant bei Null gehalten werden. Die Lautstärke dB hat auch in diesem Modell einen positiven Einfluss auf die Grundfrequenz mit einer Steigung von 4.3 Hz, d.h. die Grundfrequenz steigt mit der Lautstärke (wenn die Dauer konstant bei Null gehalten wird). Die Steigung für dur hingegen ist mit -0.09 Hz negativ, d.h. für höhere Dauerwerte sinkt die Grundfrequenz (wenn die Lautstärke konstant bei Null gehalten wird). Nun gibt es allerdings noch eine weitere Slope, nämlich für die Interaktion dB:dur. Diese Slope findet sich auch in der für Interaktionen angepassten Formel für die Regressionslinie: \\(y = k + b_1x_1 + b_2x_2 + b_3(x_1 \\cdot x_2)\\) Die Slope \\(b_3(x_1 \\cdot x_2)\\) deutet schon an, dass die Interaktion nur wichtig für die Regressionslinie ist, wenn sowohl \\(x_1\\) als auch \\(x_2\\) ungleich Null ist: denn wenn eine von beiden Null ist, ergibt die Multiplikation \\(x_1 \\cdot x_2\\) Null, d.h. die Slope \\(b_3\\) muss mit Null multipliziert werden und fällt somit aus der Formel raus. In unserem Modell ist die Steigung für die Interaktion der beiden Prädiktoren negativ. Das lässt sich so interpretieren, dass die Grundfrequenz sinkt, wenn sowohl die Lautstärke als auch die Dauer steigen. Um diese Zusammenhänge genau nachvollziehen zu können, holen wir uns aus dem obigen Data Frame das Intercept \\(k\\), die beiden Slopes \\(b_1\\) (für die Lautstärke) und \\(b_2\\) (für die Dauer), sowie die Interaktion \\(b_3\\): k &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) k ## [1] 86.02 b_1 &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_1 ## [1] 4.298 b_2 &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;dur&quot;) %&gt;% pull(estimate) b_2 ## [1] -0.09212 b_3 &lt;- lm2 %&gt;% tidy() %&gt;% filter(term == &quot;dB:dur&quot;) %&gt;% pull(estimate) b_3 ## [1] -0.004656 Nun können Sie diese Werte zusammen mit frei gewählten Werten für \\(x_1\\) (Lautstärke) und \\(x_2\\) (Dauer) in die obige Formel einsetzen, um herauszufinden, wie hoch die Grundfrequenz für die gewählten Werte der unabhängigen Variablen sein sollte. Nehmen wir an \\(x_1\\) und \\(x_2\\) sind beide Null, dann gilt wieder, dass der geschätzte y-Wert genau dem Intercept \\(k\\) entspricht: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot 0 + (-0.092 \\cdot 0) + (-0.0047 \\cdot 0 \\cdot 0) \\\\ &amp;= 86.02 \\end{aligned} \\] Genauso kann man anstatt der beiden Nullen verschiedene andere Werte für \\(x_1\\) und \\(x_2\\) einsetzen. Als Beispiel wählen wir hier \\(x_1 = 1\\) und \\(x_2 = 1\\), um zu zeigen, dass dann auch die Slope für die Interaktion der beiden Variablen zum Tragen kommt: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot 1 + (-0.092 \\cdot 1) + (-0.0047 \\cdot 1 \\cdot 1) \\\\ &amp;= 90.22 \\end{aligned} \\] Oder, indem wir die extrahierten Regressionskoeffizienten nutzen: k + b_1 + b_2 + b_3 ## [1] 90.22 Auch hier können wir wieder predict() einsetzen, um uns für verschiedene Kombinationen von dB- und dur-Werten den f0-Wert schätzen zu lassen: d2 &lt;- data.frame(dB = c(0, 1, 0, 1, mean(faux$dB)), dur = c(0, 0, 1, 1, mean(faux$dur))) d2 %&lt;&gt;% mutate(estimated_f0 = predict(lm2, d2)) d2 ## dB dur estimated_f0 ## 1 0.0 0.0 86.02 ## 2 1.0 0.0 90.32 ## 3 0.0 1.0 85.93 ## 4 1.0 1.0 90.22 ## 5 60.1 298.9 233.19 Nun können wir wieder die beispielhaften Regressionslinien nachvollziehen, die wir oben im rechten Plot gesehen haben. Wir wählen 450 ms und 150 ms als beispielhafte Dauerwerte: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-0.092 \\cdot 450) + (-0.0047 \\cdot dB \\cdot 450) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-41.45) + (-2.10 \\cdot dB) \\\\ &amp;= 44.57 + 2.20 \\cdot dB \\end{aligned} \\] Und für 150 ms: \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot dur + b_3(dB \\cdot dur) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-0.092 \\cdot 150) + (-0.0047 \\cdot dB \\cdot 150) \\\\ &amp;= 86.02 + 4.30 \\cdot dB + (-13.82) + (-0.70 \\cdot dB) \\\\ &amp;= 72.20 + 3.60 \\cdot dB \\end{aligned} \\] Dieses Mal unterscheiden sich die Steigungen für die beiden extremen gewählten Dauerwerte, eben weil die Interaktion mit einbezogen wurde. Nun legen wir die berechneten Intercepts und Slopes als Variablen an und setzen diese wieder bei geom_abline() ein, um die Regressionslinien zu zeichnen: high_dur &lt;- 450 low_dur &lt;- 150 intercept_low_dur &lt;- k + b_1 * 0 + b_2 * low_dur + b_3 * 0 * low_dur intercept_low_dur ## [1] 72.2 intercept_high_dur &lt;- k + b_1 * 0 + b_2 * high_dur + b_3 * 0 * high_dur intercept_high_dur ## [1] 44.57 slope_low_dur &lt;- b_1 + b_3 * low_dur slope_low_dur ## [1] 3.6 slope_high_dur &lt;- b_1 + b_3 * high_dur slope_high_dur ## [1] 2.203 ggplot(faux) + aes(x = dB, y = f0, col = dur) + geom_point() + xlim(0, 95) + ylim(0, 500) + geom_abline(slope = slope_low_dur, intercept = intercept_low_dur, color = &quot;#132B43&quot;, linewidth = 1.2) + geom_abline(slope = slope_high_dur, intercept = intercept_high_dur, color = &quot;#56B1F7&quot;, linewidth = 1.2) + geom_vline(xintercept = 0, lty = &quot;dashed&quot;) Nachdem wir die Regressionskoeffizienten verstanden haben, schauen wir uns die \\(t\\)-Statistik an und stellen fest, dass sowohl die Steigung für dB (\\(t\\) = 22.6, \\(p\\) &lt; 0.001) als auch die für dur (\\(t\\) = 2.5, \\(p\\) &lt; 0.05) sich signifikant von Null unterscheiden. Zusätzlich unterscheidet sich auch die Steigung für die Interaktion signifikant von Null (\\(t\\) = 7.8, \\(p\\) &lt; 0.001). lm2 %&gt;% tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 86.0 11.1 7.75 1.50e- 14 ## 2 dB 4.30 0.190 22.6 4.63e-100 ## 3 dur -0.0921 0.0364 -2.53 1.15e- 2 ## 4 dB:dur -0.00466 0.000595 -7.82 8.83e- 15 Das bedeutet, dass auch das gesamte Modell voraussichtlich besser ist als ein Modell ohne Prädiktoren. Wir schauen uns mit glance() wieder die \\(F\\)-Statistik und \\(R^2\\) an: lm2 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 0.671 ## 2 adj.r.squared 0.671 ## 3 sigma 23.6 ## 4 statistic 1270. ## 5 p.value 0 ## 6 df 3 ## 7 logLik -8559. ## 8 AIC 17128. ## 9 BIC 17156. ## 10 deviance 1039480. ## 11 df.residual 1865 ## 12 nobs 1869 Somit können wir berichten: Eine multiple lineare Regression mit Lautstärke und Dauer als unabhängige Variablen mitsamt ihrer Interaktion zeigte einen signifikanten Effekt von Lautstärke (\\(t\\) = 22.6, \\(p\\) &lt; 0.001) und Dauer (\\(t\\) = 2.5, \\(p\\) &lt; 0.05) auf die Grundfrequenz. Zusätzlich war auch die Interaktion signifikant (\\(t\\) = 7.8, \\(p\\) &lt; 0.001). Das gewählte Modell beschreibt die Daten besser als ein Modell ohne Prädiktoren (\\(R^2\\) = 0.67, \\(F\\)[3, 1865] = 1269.6, \\(p\\) &lt; 0.001). (Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!) 4.4 Kategorische unabhängige Variablen Nun wollen wir anhand der Daten in int herausfinden, ob die Lautstärke in Dezibel db vom Vokaltyp vowel und dem Geschlecht der Versuchsperson gender beeinflusst wird. int ## # A tibble: 120 × 5 ## db vowel gender subject word ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 100 a m S1 w1 ## 2 70 a m S2 w1 ## 3 90 a m S3 w1 ## 4 60 a m S4 w1 ## 5 80 a m S5 w1 ## 6 50 a f S6 w1 ## 7 40 a f S7 w1 ## 8 60 a f S8 w1 ## 9 30 a f S9 w1 ## 10 20 a f S10 w1 ## # ℹ 110 more rows Erstmal schauen wir uns in einem Boxplot an, ob überhaupt eine Abhängigkeit zwischen den Variablen vorliegen könnte: ggplot(int) + aes(x = vowel, y = db, fill = gender) + geom_boxplot() Der Plot zeigt, allgemein gesprochen, dass Frauen leiser sprechen als Männer. Dieser Effekt unterscheidet sich aber für die beiden Vokale: Bei /a/ ist der Effekt von Geschlecht viel ausgeprägter als bei /i/. Es könnte also eine Interaktion zwischen Vokal und Geschlecht vorliegen. Genauso können wir die Interaktion der unabhängigen Variablen für den Effekt des Vokals betrachten: Im Vokal /i/ ist die Lautstärke generell niedriger als in /a/ – aber für Männer ist der Effekt stärker als für Frauen (d.h. der Unterschied zwischen den zwei blauen Boxen ist größer als der zwischen den zwei roten Boxen). Obwohl hier wahrscheinlich eine Interaktion zwischen Vokal und Geschlecht vorliegt, schauen wir uns aus didaktischen Gründen erstmal an, wie eine multiple Regression mit zwei kategorialen Prädiktoren (unabhängigen Variablen) ohne Interaktion aussieht. 4.4.1 Ohne Interaktion lm3 &lt;- lm(db ~ vowel + gender, data = int) lm3 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 53.2 ## 2 voweli -20.1 ## 3 genderm 30.1 Wenn wir es mit kontinuierlichen unabhängigen Variablen zu tun haben, ist das Intercept der geschätzte y-Wert für Null in allen unabhängigen Variablen. Was genau ist aber “Null” für kategoriale Variablen? Dies bestimmt R durch das sogenannte Treatment Coding (auch oft mit dem Überbegriff Dummy Coding bezeichnet). Die Variable gender kann als Werte entweder f (female) oder m (male) annehmen. R geht dann alphabetisch vor und bestimmt, dass f als Null und m als Eins interpretiert werden soll. Genauso läuft es bei den Vokalen vowel, wo a als Null und i als Eins interpretiert wird. Dementsprechend ist das Intercept in dieser Regression der geschätzte Dezibel-Mittelwert für das a von Frauen. Dies können wir rechnerisch leicht nachvollziehen: k &lt;- lm3 %&gt;% augment() %&gt;% filter(vowel == &quot;a&quot; &amp; gender == &quot;f&quot;) %&gt;% summarise(m = mean(.fitted)) %&gt;% pull(m) k ## [1] 53.23 Wie Sie sehen, heißen die beiden Slopes in unserem Modell genderm und voweli. Hierbei wird immer zuerst der Faktor (der Name der kategorialen Variable) genannt und dann das Level des Faktors, das nicht im Intercept verarbeitet wurde (also das Level m für gender und das Level i für vowel). Die Steigung genderm beschreibt die Lautstärkeveränderung von Frauen zu Männern für den Vokal /a/. Diese ist mit 30.1 dB positiv, d.h. Männer produzieren das /a/ deutlich lauter als Frauen. Die Steigung vowelibeschreibt die Lautstärkeveränderung von /a/ zu /i/ für Frauen und ist mit -20.12 dB negativ, d.h. Frauen produzieren das /i/ leiser als das /a/. Dies stimmt mit unserem Eindruck aus den Boxplots überein. Wenn wir die Lautstärke-Mittelwerte für alle vier Kombinationen für die Levels von vowel und gender schätzen wollen, brauchen wir wieder die beiden Slopes \\(b_1\\) (für den Vokal /i/) und \\(b_2\\) (für Männer) aus dem Ergebnis von tidy(), sowie das Intercept \\(k\\), das wir oben schon berechnet haben. Erinnern Sie sich außerdem, dass durch das Treatment Coding festgelegt wurde, dass a = 0, i = 1, female = 0 und male = 1. b_1 &lt;- lm3 %&gt;% tidy() %&gt;% filter(term == &quot;voweli&quot;) %&gt;% pull(estimate) b_1 ## [1] -20.12 b_2 &lt;- lm3 %&gt;% tidy() %&gt;% filter(term == &quot;genderm&quot;) %&gt;% pull(estimate) b_2 ## [1] 30.08 Geschätzte Lautstärke für das /a/ (\\(x_1 = 0\\)) von Frauen (\\(x_2 = 0\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 0) + 30.09 \\cdot 0 \\\\ &amp;= 53.23 \\end{aligned} \\] … für das /a/ (\\(x_1 = 0\\)) von Männern (\\(x_2 = 1\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 0) + 30.09 \\cdot 1 \\\\ &amp;= 83.31 \\end{aligned} \\] … für das /i/ (\\(x_1 = 1\\)) von Frauen (\\(x_2 = 0\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 1) + 30.09 \\cdot 0 \\\\ &amp;= 33.11 \\end{aligned} \\] … und für das /i/ (\\(x_1 = 1\\)) von Männern (\\(x_2 = 1\\)): \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot x_1 + b_2 \\cdot x_2 \\\\ &amp;= 53.23 + (-20.12 \\cdot 1) + 30.09 \\cdot 1 \\\\ &amp;= 63.19 \\end{aligned} \\] Die predict()-Funktion kann uns diese Rechenarbeit aber auch abnehmen: d3 &lt;- data.frame(gender = c(&quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;), vowel = c(&quot;a&quot;, &quot;i&quot;, &quot;a&quot;, &quot;i&quot;)) d3 %&lt;&gt;% mutate(estimated_mean_dB = predict(lm3, d3)) d3 ## gender vowel estimated_mean_dB ## 1 f a 53.23 ## 2 f i 33.11 ## 3 m a 83.31 ## 4 m i 63.19 Vergleichen Sie diese geschätzen Mittelwerte auch mit den Boxplots, die wir zuvor gemacht haben oder mit den tatsächlichen Mittelwerten (die Sie mit tidyverse-Funktionen berechnen können). Wir streben natürlich danach, dass die Schätzungen möglichst genau sind, denn das bedeutet, dass das von uns gewählte Modell sehr gut zu den Daten passt. In diesem Fall scheinen die Schätzungen schon recht gut zu sein: int %&gt;% group_by(gender, vowel) %&gt;% summarise(m = mean(db)) ## `summarise()` has grouped output by &#39;gender&#39;. You can ## override using the `.groups` argument. ## # A tibble: 4 × 3 ## # Groups: gender [2] ## gender vowel m ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 f a 48.2 ## 2 f i 38.2 ## 3 m a 88.4 ## 4 m i 58.1 Weiterführende Infos: Dummy Coding Wenn eine der unabhängigen Variablen mehr als zwei Stufen gehabt hätte (also wenn z.B. noch ein dritter Vokal /o/ im Datensatz vorkäme), dann gäbe es eine weitere Slope für diese Stufe (also z.B. vowelo). Es gibt auch noch andere Arten des Dummy Coding, z.B. Sum Coding, wie Sie im Kapitel 7 in Winter (2020) nachlesen können. Jetzt schauen wir uns erst noch die statistischen Ergebnisse unserer multiplen Regression an: lm3 %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 53.2 3.28 16.2 8.86e-32 ## 2 voweli -20.1 3.78 -5.32 5.09e- 7 ## 3 genderm 30.1 3.78 7.95 1.27e-12 Diese Übersicht zeigt, dass es einen signifikanten Einfluss von Geschlecht (\\(t\\) = 8.0, \\(p\\) &lt; 0.001) und von Vokal (\\(t\\) = 5.3, \\(p\\) &lt; 0.001) auf die Lautstärke gab, oder anders gesagt: die Steigungen für Vokal und Gender unterscheiden sich signifikant von Null und sind daher gute Prädiktoren für die Lautstärke. Nun schauen wir uns noch die Gütekriterien an: lm3 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 4.39e- 1 ## 2 adj.r.squared 4.29e- 1 ## 3 sigma 2.07e+ 1 ## 4 statistic 4.58e+ 1 ## 5 p.value 2.06e-15 ## 6 df 2 e+ 0 ## 7 logLik -5.32e+ 2 ## 8 AIC 1.07e+ 3 ## 9 BIC 1.08e+ 3 ## 10 deviance 5.02e+ 4 ## 11 df.residual 1.17e+ 2 ## 12 nobs 1.2 e+ 2 Hier sehen wir, dass durch die beiden unabhängigen Variablen Geschlecht und Vokal ca. 43% der Varianz in den Lautstärkemessungen beschrieben werden kann. Der \\(F\\)-Test zeigt, dass unser Regressionsmodell die Daten erfolgreicher beschreibt als ein Modell ohne Prädiktoren (\\(R^2\\) = 0.43, \\(F\\)[2, 117] = 45.8, \\(p\\) &lt; 0.001). (Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!) 4.4.2 Mit Interaktion Da wir auch in diesem Datensatz eine Interaktion zwischen Vokal und Geschlecht vermuten, binden wir sie in unsere Regression ein: lm4 &lt;- lm(db ~ vowel * gender, data = int) lm4 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 48.2 ## 2 voweli -10.0 ## 3 genderm 40.2 ## 4 voweli:genderm -20.2 Das Intercept beschreibt hier immer noch den geschätzten Dezibel-Mittelwert für /a/ von Frauen, der bei ca. 48 dB liegt. Der Einfluss des Vokals auf die Lautstärke ist wieder negativ, der Einfluss des Geschlechts ist positiv. Nun gibt es zusätzlich die Slope für die Interaktion voweli:genderm, die mit -20.2 dB negativ ist. Wenn der Vokal also /i/ ist und das Geschlecht männlich, dann sinkt die Lautstärke. Im folgenden sehen Sie, wie Sie nun unter Einbeziehung der Interaktion die Lautstärke für die vier Kombinationen aus \\(vowel \\times gender\\) schätzen können (all dies leitet sich wieder aus der Formel für die Regressionslinie mit Interaktion ab): k &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) b_1 &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;genderm&quot;) %&gt;% pull(estimate) b_2 &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;voweli&quot;) %&gt;% pull(estimate) b_3 &lt;- lm4 %&gt;% tidy() %&gt;% filter(term == &quot;voweli:genderm&quot;) %&gt;% pull(estimate) # female-a k ## [1] 48.17 # male-a k + b_1 ## [1] 88.36 # female-i k + b_2 ## [1] 38.16 # male-i k + b_1 + b_2 + b_3 ## [1] 58.14 Die predict()-Funktion kann Ihnen hier aber auch wieder die Rechenarbeit abnehmen: d4 &lt;- data.frame(gender = c(&quot;f&quot;, &quot;f&quot;, &quot;m&quot;, &quot;m&quot;), vowel = c(&quot;a&quot;, &quot;i&quot;, &quot;a&quot;, &quot;i&quot;)) d4 %&lt;&gt;% mutate(estimated_mean_dB = predict(lm4, d4)) d4 ## gender vowel estimated_mean_dB ## 1 f a 48.17 ## 2 f i 38.16 ## 3 m a 88.36 ## 4 m i 58.14 Durch die Einbindung der Interaktion in das Modell sind die Lautstärke-Schätzungen im Vergleich zum Modell vorher präziser geworden! Tatsächlich stimmen die Schätzungen perfekt mit den tatsächlichen Mittelwerten überein (das wird im “echten Leben” so gut wie niemals vorkommen): int %&gt;% group_by(gender, vowel) %&gt;% summarise(m = mean(db)) ## `summarise()` has grouped output by &#39;gender&#39;. You can ## override using the `.groups` argument. ## # A tibble: 4 × 3 ## # Groups: gender [2] ## gender vowel m ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 f a 48.2 ## 2 f i 38.2 ## 3 m a 88.4 ## 4 m i 58.1 Nun schauen wir uns die \\(t\\)-Statistik an, die zeigt, ob unsere Regressionskoeffizienten dazu beitragen, die Varianz in den Dezibelwerten zu erklären: lm4 %&gt;% tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 48.2 3.68 13.1 1.37e-24 ## 2 voweli -10.0 5.21 -1.92 5.69e- 2 ## 3 genderm 40.2 5.21 7.72 4.47e-12 ## 4 voweli:genderm -20.2 7.36 -2.75 7.00e- 3 Offensichtlich unterscheiden sich die Slope für Geschlecht (\\(t\\) = 7.7, \\(p\\) &lt; 0.001) und die Slope für Vokal (\\(t\\) = 1.9, \\(p\\) = 0.06) (fast) signifikant von Null. Zusätzlich gab es eine signifikante Interaktion zwischen den Faktoren (\\(t\\) = 2.8, \\(p\\) &lt; 0.01). Oben haben wir festgestellt, dass unser Modell mit Interaktion die Daten besser beschreibt als das vorherige Modell ohne Interaktion. Dies zeigt sich auch in der \\(F\\)-Statistik: lm4 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 4.73e- 1 ## 2 adj.r.squared 4.60e- 1 ## 3 sigma 2.02e+ 1 ## 4 statistic 3.47e+ 1 ## 5 p.value 4.29e-16 ## 6 df 3 e+ 0 ## 7 logLik -5.29e+ 2 ## 8 AIC 1.07e+ 3 ## 9 BIC 1.08e+ 3 ## 10 deviance 4.71e+ 4 ## 11 df.residual 1.16e+ 2 ## 12 nobs 1.2 e+ 2 Nun werden ca. 46% der Varianz in den Daten durch die beiden Faktoren und ihre Interaktion beschrieben. Das Modell mit der Interaktion beschreibt also mehr Varianz als das Modell ohne Interaktion; d.h. es liefert präzisere Schätzungen der Regressionskoeffizienten. Der \\(F\\)-Test zeigt außerdem, dass das Regressionsmodell die Daten besser beschreibt als das Intercept-only Modell (\\(R^2\\) = 0.46, \\(F\\)[3, 116] = 34.7, \\(p\\) &lt; 0.001). (Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!) 4.4.2.1 Post-hoc Tests mit emmeans Wenn in einer multiplen Regression mit mindestens zwei kategorialen unabhängigen Variablen die Interaktion einen signifikanten Effekt auf die gemessene Variable hat, dann sollte man sogenannte post-hoc Tests ausführen. Dafür eignet sich das Paket emmeans mit der gleichnamigen Funktion außerordentlich gut. Für dieses Paket gibt es mehrere ausführliche Vignetten. Die Funktion emmeans() führt \\(t\\)-Tests auf alle Kombinationen der Werte der kategorialen unabhängigen Variablen aus. So finden wir heraus, welche Kombination(en) dafür sorgen, dass die Interaktion in der Regression signifikant war. Als Argumente bekommt die Funktion das Ergebnis des Regressionsmodells, und dann die Formel pairwise ~ vowel | gender (sprich: pairwise comparisons for vowel given gender). Die Formel bedeutet hier, dass pro Geschlecht der Unterschied zwischen den Vokalen und dessen Signifikanz ermittelt werden soll. emmeans(lm4, pairwise ~ vowel | gender) ## $emmeans ## gender = f: ## vowel emmean SE df lower.CL upper.CL ## a 48.2 3.68 116 40.9 55.5 ## i 38.2 3.68 116 30.9 45.5 ## ## gender = m: ## vowel emmean SE df lower.CL upper.CL ## a 88.4 3.68 116 81.1 95.7 ## i 58.1 3.68 116 50.9 65.4 ## ## Confidence level used: 0.95 ## ## $contrasts ## gender = f: ## contrast estimate SE df t.ratio p.value ## a - i 10.0 5.2 116 1.923 0.0569 ## ## gender = m: ## contrast estimate SE df t.ratio p.value ## a - i 30.2 5.2 116 5.806 &lt;.0001 Das Ergebnis dieser Funktion sind zwei Tabellen, eine namens emmeans und eine namens contrasts. In der Tabelle emmeans, Spalte emmean finden Sie unsere Schätzungen für die Mittelwerte wieder, zuerst für Frauen (gender = f), dann für Männer (gender = m). emmeans steht für estimated marginal means. Sie finden hier zusätzlich den Standard Error SE, die Freiheitsgrade df, sowie das 95%-Konfindenzinterval lower.CL und upper.CL. In der Tabelle contrasts finden Sie die Differenzen zwischen /a/ und /i/, zuerst für Frauen, dann für Männer. Zuerst wird Ihnen hierfür wieder ein estimate angezeigt. Dieser ist ganz einfach die Differenz aus den emmeans für /a/ und /i/ pro Geschlecht. Es folgen Standard Error SE und dann die Freiheitsgrade df, der \\(t\\)-Wert t.ratio und der \\(p\\)-Wert p.value. Hier sehen wir im Einklang mit den Boxplots, die wir ganz zu Anfang dieses Kapitels erstellt haben, dass es keinen signifikanten Lautstärke-Unterschied zwischen /a/ und /i/ für Frauen gibt (\\(t\\)[116] = 1.9, \\(p\\) = 0.06), sehr wohl aber für Männer (\\(t\\)[116] = 5.8, \\(p\\) &lt; 0.001). Nun sollten wir zusätzlich schauen, ob es pro Vokaltyp signifikante Unterschiede zwischen Frauen und Männern gab. Dafür verändern wir einfach die Formel zu pairwise ~ gender | vowel: emmeans(lm4, pairwise ~ gender | vowel) ## $emmeans ## vowel = a: ## gender emmean SE df lower.CL upper.CL ## f 48.2 3.68 116 40.9 55.5 ## m 88.4 3.68 116 81.1 95.7 ## ## vowel = i: ## gender emmean SE df lower.CL upper.CL ## f 38.2 3.68 116 30.9 45.5 ## m 58.1 3.68 116 50.9 65.4 ## ## Confidence level used: 0.95 ## ## $contrasts ## vowel = a: ## contrast estimate SE df t.ratio p.value ## f - m -40.2 5.2 116 -7.721 &lt;.0001 ## ## vowel = i: ## contrast estimate SE df t.ratio p.value ## f - m -20.0 5.2 116 -3.838 0.0002 Hier können wir berichten, dass sowohl für /a/ (\\(t\\)[116] = 7.7, \\(p\\) &lt; 0.001) als auch für /i/ (\\(t\\)[116] = 3.8, \\(p\\) &lt; 0.001) ein signifikanter Unterschied zwischen Männern und Frauen vorliegt. Das stimmt ebenfalls mit unserem visuellen Eindruck aus den anfänglichen Boxplots überein. 4.5 Mix aus kontinuierlichen und kategorischen unabhängigen Variablen Zuletzt beschäftigen wir uns mit der Frage, ob die Grundfrequenzwerte im Data Frame vlax vom Vokal vowel und von der Lautstärke dB beeinflusst werden. Hier haben wir also ein Regressionsmodell mit einer kategorischen und einer kontinuierlichen unabhängigen Variable. Bei einer Abbildung bietet es sich an, die kontinuierliche unabhängige Variable auf die x-Achse zu packen und die Levels der kategorialen Variable durch Farben darzustellen. Hier zeigen wir zusätzlich die Regressionslinien, die geom_smooth() für uns berechnen kann. ggplot(vlax) + aes(x = dB, y = f0, col = vowel) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Im Allgemeinen scheint es auch hier den bekannten positiven linearen Zusammenhang zwischen f0 und dB zu geben. Dieser Zusammenhang scheint allerdings nur für den Vokal /I/, nicht aber für den Vokal /O/ zu existieren. Auch hier liegt also vermutlich eine Interaktion zwischen den beiden unabhängigen Variablen vor, wir berechnen aber zuerst zu Demonstrationszwecken ein Modell ohne Interaktion. 4.5.1 Ohne Interaktion lm5 &lt;- lm(f0 ~ dB + vowel, data = vlax) lm5 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -259. ## 2 dB 6.17 ## 3 vowelO -22.9 Auch hier wird die kategoriale Variable vowel mittels Dummy Coding hinter den Kulissen umgewandelt. Wir sehen, dass die zweite Slope vowelO heißt, das bedeutet, dass der Vokal “I” schon im Intercept verarbeitet wurde. Wenn also der Vokal “I” ist und dB = 0, dann sollte die Grundfrequenz bei -258.9 Hz liegen. Der Einfluss von Lautstärke auf Grundfrequenz ist wie erwartet ein positiver, d.h. mit jeder Steigerung der Lautstärke um 1 dB steigt die Grundfrequenz um 6.2 Hz (dies sehen Sie z.B. im Vergleich der ersten beiden Zeilen in d5). Wenn der Vokal “O” anstatt “I” ist, sinkt die Grundfrequenz um 22.9 Hz (vgl. erste und dritte Zeile in d5). Bei durchschnittlicher Lautstärke liegt die Grundfrequenz bei der Produktion von “I” bei 200 Hz und bei der Prodution von “O” bei 177.1 Hz (s. Zeilen vier und fünf in d5). d5 &lt;- data.frame(dB = c(0, 1, 0, mean(vlax$dB), mean(vlax$dB)), vowel = c(&quot;I&quot;, &quot;I&quot;, &quot;O&quot;, &quot;I&quot;, &quot;O&quot;)) d5 %&lt;&gt;% mutate(estimated_f0 = predict(lm5, d5)) d5 ## dB vowel estimated_f0 ## 1 0.0 I -258.9 ## 2 1.0 I -252.7 ## 3 0.0 O -281.8 ## 4 74.4 I 200.0 ## 5 74.4 O 177.1 Wir können für “I” und “O” jeweils eine Regressionslinie mittels des linearen Models berechnen, indem wir uns die Regressionskoeffizienten holen und wie wie zuvor in die Formel einsetzen: k &lt;- lm5 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) k ## [1] -258.9 b_1 &lt;- lm5 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_1 ## [1] 6.168 b_2 &lt;- lm5 %&gt;% tidy() %&gt;% filter(term == &quot;vowelO&quot;) %&gt;% pull(estimate) b_2 ## [1] -22.93 Für den Vokal “I” (\\(x_2 = 0\\)) liegt der y-Achsenabschnitt bei -258.9 Hz und die Steigung bei 6.17 Hz. \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot x_2 \\\\ &amp;= -258.87 + (6.17 \\cdot dB) + (-22.93 \\cdot 0) \\\\ &amp;= -258.87 + (6.17 \\cdot dB) \\end{aligned} \\] Für den Vokal “O” (\\(x_2 = 1\\)) liegt der y-Achsenabschnitt bei -281.8 Hz, die Steigung aber ebenfalls bei 6.17 Hz. \\[ \\begin{aligned} y &amp;= k + b_1 \\cdot dB + b_2 \\cdot x_2 \\\\ &amp;= -258.87 + (6.17 \\cdot dB) + (-22.93 \\cdot 1) \\\\ &amp;= -281.79 + (6.17 \\cdot dB) \\end{aligned} \\] Die hier berechneten Regressionslinien können nicht denen in der Abbildung der Daten entsprechen, denn die hier berechneten Linien haben dieselbe Steigung und verlaufen dementsprechend parallel zueinander, während die von geom_smooth() gezeichneten Linien einander schneiden. Daraus können wir schließen, dass dieses Modell ohne Interaktion nicht besonders gut zu den Daten passt. Trotzdem zeigt die \\(t\\)-Statistik, dass sich die Regressionskoeffizienten signifikant von Null unterscheiden (Slope für Lautstärke: \\(t\\) = 8.1, \\(p\\) &lt; 0.001; Slope für Vokal: \\(t\\) = 2.6, \\(p\\) &lt; 0.01). Das heißt, beide Koeffizienten sind gute Prädiktoren für die Grundfrequenz in diesem Modell. lm5 %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -259. 56.3 -4.60 8.25e- 6 ## 2 dB 6.17 0.761 8.11 8.74e-14 ## 3 vowelO -22.9 8.75 -2.62 9.59e- 3 Wie immer werfen wir auch noch einen Blick auf die Gütekriterien für das Modell und stellen fest, dass durch die beiden unabhängigen Variablen Vokal und Lautstärke ca. 26.6% der Varianz in den Grundfrequenzwerten beschrieben werden kann. Der \\(F\\)-Test zeigt, dass unser Regressionsmodell die Daten erfolgreicher beschreibt als ein Modell ohne Prädiktoren (\\(R^2\\) = 0.27, \\(F\\)[2, 175] = 33.0, \\(p\\) &lt; 0.001). lm5 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 2.74e- 1 ## 2 adj.r.squared 2.66e- 1 ## 3 sigma 4.28e+ 1 ## 4 statistic 3.30e+ 1 ## 5 p.value 6.81e-13 ## 6 df 2 e+ 0 ## 7 logLik -9.20e+ 2 ## 8 AIC 1.85e+ 3 ## 9 BIC 1.86e+ 3 ## 10 deviance 3.21e+ 5 ## 11 df.residual 1.75e+ 2 ## 12 nobs 1.78e+ 2 (Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!) 4.5.2 Mit Interaktion Nun berechnen wir das Modell mitsamt der Interaktion zwischen den unabhängigen Variablen: lm6 &lt;- lm(f0 ~ dB * vowel, data = vlax) lm6 %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -319. ## 2 dB 6.98 ## 3 vowelO 615. ## 4 dB:vowelO -8.34 Das Dummy Coding bleibt gleich, d.h. der Vokal “I” wird als Null und der Vokal “O” als Eins gewertet. Das Intercept liegt bei -318.5 Hz. Der Einfluss von Lautstärke auf Grundfrequenz ist immer noch positiv mit ca. 7 Hz. Auffällig ist, dass sich der Koeffizient vowelO im Vergleich zum Modell ohne Interaktion stark verändert hat, von -22.9 zu +615.2 Hz. Das ergibt hier aber auch Sinn, wenn wir uns nochmal die Abbildung anschauen (hier haben wir die x-Achse so verändert, dass sie bei Null beginnt): ggplot(vlax) + aes(x = dB, y = f0, col = vowel) + geom_point() + xlim(0, 90) + geom_smooth(method = &quot;lm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Wenn Sie die blaue Regressionslinie bis zur y-Achse verfolgen, kommen Sie bei knapp unter 300 Hz heraus – das entspricht dem Intercept (db = 0 und vowel = \"I\") plus der Slope für vowelO. Dasselbe gilt für die rote Linie, die die y-Achse an einem Punkt schneidet, der hier nicht mehr abgebildet wird, nämlich bei ca. -311 Hz – das entspricht dem Intercept plus der Slope für dB. Wir können diese Berechnungen wie folgt nachvollziehen (oder Sie setzen die Regressionskoeffizienten manuell in die Formel ein, wie oben gezeigt): k &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;(Intercept)&quot;) %&gt;% pull(estimate) b_1 &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;dB&quot;) %&gt;% pull(estimate) b_2 &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;vowelO&quot;) %&gt;% pull(estimate) b_3 &lt;- lm6 %&gt;% tidy() %&gt;% filter(term == &quot;dB:vowelO&quot;) %&gt;% pull(estimate) # &quot;I&quot; bei 0 dB k ## [1] -318.5 # &quot;I&quot; bei 1 dB k + b_1 ## [1] -311.5 # &quot;O&quot; bei 0 dB k + b_2 ## [1] 296.6 # &quot;O&quot; bei 1 dB k + b_1 + b_2 + b_3 ## [1] 295.3 Die predict()-Funktion kann Ihnen hier wieder die Rechenarbeit abnehmen: d6 &lt;- data.frame(dB = c(0, 1, 0, 1), vowel = c(&quot;I&quot;, &quot;I&quot;, &quot;O&quot;, &quot;O&quot;)) d6 %&lt;&gt;% mutate(estimated_f0 = predict(lm6, d6)) d6 ## dB vowel estimated_f0 ## 1 0 I -318.5 ## 2 1 I -311.5 ## 3 0 O 296.6 ## 4 1 O 295.3 Wenn Sie sich jetzt den Unterschied zwischen Zeile 1 und 2 in d6 anschauen, sehen Sie, dass die Lautstärke für den Vokal “I” positiv mit der Grundfrequenz korreliert ist, denn mit steigender Lautstäke (von 0 zu 1 dB) steigt auch die geschätzte Grundfrequenz. Für den Vokal “O” hingegen sinkt die Grundfrequenz mit steigender Lautstärke (vgl. Zeilen 3 und 4 in d6), d.h. die Korrelation zwischen Lautstärke und Grundfrequenz ist negativ für den Vokal “O” – das entspricht auch den Regressionslinien im obigen Plot sowie den wie folgt berechneten Korrelationswerten \\(r\\): vlax %&gt;% group_by(vowel) %&gt;% summarise(r = cor(dB, f0)) ## # A tibble: 2 × 2 ## vowel r ## &lt;fct&gt; &lt;dbl&gt; ## 1 I 0.591 ## 2 O -0.115 Nun schauen wir uns die \\(t\\)-Statistik an, die zeigt, ob unsere Regressionskoeffizienten dazu beitragen, die Grundfrequenzwerte zuverlässig zu schätzen: lm6 %&gt;% tidy() ## # A tibble: 4 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -319. 57.6 -5.53 1.17e- 7 ## 2 dB 6.98 0.779 8.96 4.93e-16 ## 3 vowelO 615. 192. 3.21 1.58e- 3 ## 4 dB:vowelO -8.34 2.50 -3.33 1.05e- 3 Die Slope für Laustärke (\\(t\\) = 9.0, \\(p\\) &lt; 0.001) und die Slope für Vokal (\\(t\\) = 3.2, \\(p\\) &lt; 0.01) unterscheiden sich signifikant von Null und eignen sich dementsprechend gut als Prädiktoren für die Grundfrequenz. Zusätzlich gab es eine signifikante Interaktion zwischen den Faktoren (\\(t\\) = 3.3, \\(p\\) &lt; 0.001). Die \\(F\\)-Statistik zeigt, dass das Regressionsmodell mit den beiden Prädiktoren und ihrer Interaktion die Daten besser beschreibt als ein Mittelwertmodell (\\(R^2\\) = 0.31, \\(F\\)[3, 174] = 27.0, \\(p\\) &lt; 0.001). Anhand des \\(R^2\\)-Werts sehen wir, dass das Modell nun auch mehr Varianz in den Grundfrequenzwerten beschreibt als im Modell ohne Interaktion, d.h. die Güte des Modells ist durch die Hinzunahme der Interaktion gestiegen. lm6 %&gt;% glance() %&gt;% pivot_longer(cols = c(r.squared:nobs)) ## # A tibble: 12 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 r.squared 3.18e- 1 ## 2 adj.r.squared 3.06e- 1 ## 3 sigma 4.17e+ 1 ## 4 statistic 2.70e+ 1 ## 5 p.value 2.21e-14 ## 6 df 3 e+ 0 ## 7 logLik -9.14e+ 2 ## 8 AIC 1.84e+ 3 ## 9 BIC 1.85e+ 3 ## 10 deviance 3.02e+ 5 ## 11 df.residual 1.74e+ 2 ## 12 nobs 1.78e+ 2 Für dieses Modell berechnen wir trotz der signifikanten Interaktion zwischen den unabhängigen Variablen keine post-hoc Tests, weil eine der unabhängigen Variablen kontinuierlich ist. Die post-hoc Tests mit emmeans ergeben nur Sinn für mehrere kategoriale Variablen. (Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!) "],["gemischte-lineare-regression.html", "5 Gemischte Lineare Regression 5.1 Packages und Daten laden 5.2 Mixed Models (LMERs): Einführung 5.3 Random Intercepts vs. Random Slopes 5.4 LMER in R 5.5 Konvergenzprobleme und Modell vereinfachen 5.6 Ergebnisse berichten 5.7 Gütekriterien für Mixed Models", " 5 Gemischte Lineare Regression 5.1 Packages und Daten laden Laden Sie die Pakete und Data Frames: library(emmeans) library(lmerTest) ## Loading required package: lme4 ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step library(MuMIn) ## Registered S3 methods overwritten by &#39;MuMIn&#39;: ## method from ## nobs.multinom broom ## nobs.fitdistr broom library(tidyverse) library(magrittr) url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; df &lt;- read.table(file.path(url, &quot;int_new.txt&quot;), stringsAsFactors = T, header = T) %&gt;% mutate(subject = factor(subject, levels = paste0(&quot;S&quot;, 1:10))) %&gt;% as_tibble() 5.2 Mixed Models (LMERs): Einführung Mit einem Linear Mixed Effect Model (auch: Linear Mixed Effect Regression oder LMER) wird geprüft, ob eine gemessene abhängige Variable von einer oder mehreren unabhängigen Variablen beeinflusst wird. Diese Art von Modellen wird als “gemischt” (mixed) bezeichnet, weil sowohl Variablen, deren Einfluss auf die abhängige Variable für die Forschenden interessant ist (Fixed Effects), als auch Variablen, deren Einfluss uninteressant ist (Random Effects), Teil des Modells sein können. Die Fixed Effects in einem Mixed Model können sowohl kategorial als auch kontinuierlich sein, die Random Effects ausschließlich kategorial. Damit erweitern wir die einfache und multiple lineare Regression, die nur Fixed Effects als Prädiktoren genutzt haben. In der Phonetik und Linguistik werden häufig die StudienteilnehmerInnen und Items oder Wörter als Random Effects im LMER verwendet. Das liegt daran, dass wir eine zufällige Auswahl an Probanden und/oder Wörtern einer bestimmten Sprache auswählen und hoffen, dass sich die Ergebnisse auf andere Probanden und/oder Wörter derselben Sprache übertragen lassen. Dieser zufällige Auswahlprozess hat zur Folge, dass wir noise in unser Experiment bringen, das wir dann mittels LMER wieder “herausrechnen” können. Während wir also für Fixed Effects einen vorhersagbaren Einfluss auf die abhängige Variable vermuten, ist der Einfluss der Random Effects nicht vorhersagbar, d.h. random. Schauen wir uns den Data Frame df an, den wir bereits aus früheren Kapiteln kennen (für dieses Kapitel wurde er leicht verändert): df ## # A tibble: 120 × 5 ## db vowel gender subject word ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 100 a m S1 w1 ## 2 70 a m S2 w1 ## 3 90 a m S3 w1 ## 4 60 a m S4 w1 ## 5 80 a m S5 w1 ## 6 50 a f S6 w1 ## 7 40 a f S7 w1 ## 8 60 a f S8 w1 ## 9 30 a f S9 w1 ## 10 20 a f S10 w1 ## # ℹ 110 more rows Hier haben wir die SprecherInnen in der Spalte subject und die produzierten Wörter in der Spalte word. Uns interessiert, ob der Vokal vowel und das Geschlecht gender einen Einfluss auf die Lautstärke db haben. Für diese Fragestellung schauen wir uns die Daten in einer Abbildung an: ggplot(df) + aes(x = vowel, y = db, fill = gender) + geom_boxplot() Wir sehen (wie schon im vorherigen Kapitel), dass Frauen die Vokale leiser produzierten als Männer und dass /i/ etwas leiser produziert wurde als /a/. Es ist außerdem zu vermuten, dass eine Interaktion zwischen Geschlecht und Vokal vorliegt, denn der Effekt von Geschlecht ist stärker ausgeprägt für /a/ als für /i/ (bzw. umgekehrt: der Effekt von Vokal ist stärker ausgeprägt für Männer als für Frauen). Nun wissen wir aber, dass wir mehrere ProbandInnen und verschiedene Wörter aufgenommen haben, um die Fragestellung zu untersuchen. Dabei interessiert uns nicht der genaue Einfluss der Individuen oder Wörter auf die Lautstärke, im Gegenteil: Wir wollen über die ProbandInnen und Wörter eine allgemeine Aussage ableiten, die (theoretisch) für alle möglichen ProbandInnen und Wörter gilt. Sollte es nennenswerte Lautstärke-Unterschiede zwischen den individuellen ProbandInnen und/oder den individuellen Wörtern geben, haben wir durch unsere zufällige Auswahl der ProbandInnen und Wörter noise verursacht. Mit Boxplots schauen wir, ob es nennenswerte Variationen der Dezibelwerte zwischen den Sprechern und/oder zwischen den Wörtern gibt: ggplot(df) + aes(x = subject, y = db) + geom_boxplot() ggplot(df) + aes(x = word, y = db) + geom_boxplot() Da es hier z.T. große Lautstärke-Unterschiede zwischen einzelnen SprecherInnen und zwischen einzelnen Wörtern gibt, ist es unbedingt notwendig, dass wir in dem Mixed Model, das wir für unsere Fragestellung konstruieren werden, mit Random Effects für diese beiden Variablen arbeiten. Ein zweiter äußerst wichtiger Grund für Random Effects ist, dass wir häufig sogenannte repeated measure Experimente durchführen, bei denen z.B. mehrere Datenpunkte von demselben Probanden stammen und/oder bei denen Wörter mehrfach wiederholt wurden. Das bedeutet, dass die Datenpunkte nicht mehr unabhängig von einander sind (zum Beispiel: Datenpunkte von demselben Probanden werden häufig näher zusammenliegen als solche von unterschiedlichen Probanden). Dies ist jedoch eine wichtige Annahme in Regressionsmodellen, über die wir bislang nicht gesprochen haben. Wenn wir also überlegen, welches statistische Modell zu den Daten und unserer Fragestellung passen könnte, muss einbezogen werden, ob die Datenpunkte unabhängig voneinander sind oder nicht (in früheren Kapiteln haben wir diese Annahme aus didaktischen Gründen z.T. ignoriert). Wenn die Datenpunkte nicht als unabhängig voneinander betrachtet werden können, muss ein Mixed Model verwendet werden, da sich die Random Effects der Abhängigkeiten zwischen Datenpunkten annehmen. Ein Unterschied zwischen der Unabhängigkeitsannahme und den beiden anderen Annahmen, die Sie hier bisher kennengelernt haben, ist, dass sich die Unabhängigkeitsannahme direkt auf die gemessenen Daten bezieht, während die Normalitäts- und Varianz-Annahmen sich auf die Residuals beziehen. Bei der einfachen und multiplen linearen Regression wird ein Intercept und dann eine Slope pro unabhängiger Variable (pro Fixed Effect) geschätzt. Bei den gemischten Modellen wird das ebenfalls gemacht, aber wir können jetzt erlauben, dass das Intercept und die Slope(s) sich für die Levels der Random Effects unterscheiden. Um die Konzepte Random Intercept und Random Slope zu erklären, konzentrieren wir uns hier erstmal auf subject als Random Effect. 5.3 Random Intercepts vs. Random Slopes 5.3.1 Random Intercepts Sie erinnern sich, dass mittels Treatment Coding der Vokal /a/ als Null und der Vokal /i/ als Eins interpretiert wird, genauso wird das Level f (female) der Variable gender als Null und m (male) als Eins interpretiert. Die Schätzung des Lautstärke-Mittelwerts für /a/ von Frauen ist folglich in unserem Mixed Model das Intercept (genauso wie vorher in der multiplen Regression). Wir haben mittels der Boxplots oben festgestellt, dass sich die Versuchspersonen hinsichtlich der Lautstärke deutlich voneinander unterscheiden. Interessant ist jetzt für uns, ob sich Versuchspersonen desselben Geschlechts auch im Lautstärke-Mittelwert für /a/ (dem im “allgemeinen” Intercept verarbeiteten Level von vowel) unterscheiden. Denn wenn dem so ist, dann können wir mittels eines Random Intercepts für die Variable subject verlangen, dass das Intercept von Person zu Person variieren darf (deshalb spricht man manchmal auch von varying intercepts). In der folgenden Abbildung sehen Sie die Dezibelwerte für die Vokale /a/ und /i/, getrennt nach Versuchsperson. Die Personen S1-S5 sind männlich, die Personen S6-S10 sind weiblich. Die Kreuze in orange markieren die Dezibel-Mittelwerte pro Person und Vokal. Wenn wir uns jetzt die Mittelwerte für /a/ pro Versuchsperson innerhalb derselben Zeile anschauen (das jeweils linke orange Kreuz pro Panel), gibt es große Unterschiede, z.B. zwischen S1 und S4 oder zwischen S8 und S10. Das ist ein Indikator dafür, dass es sinnvoll ist, pro SprecherIn ein Random Intercept zu berechnen. ggplot(df) + aes(x = vowel, y = db) + geom_point() + facet_wrap(~subject, nrow = 2) + geom_point(data = df %&gt;% group_by(subject, vowel) %&gt;% summarise(db = mean(db)), color = &quot;orange&quot;, size = 3, stroke = 2, shape = 4) + geom_line(data = df %&gt;% group_by(subject, vowel) %&gt;% summarise(db = mean(db)), aes(x = vowel, y = db, group = subject), lty = &quot;dashed&quot;) ## `summarise()` has grouped output by &#39;subject&#39;. You can ## override using the `.groups` argument. ## `summarise()` has grouped output by &#39;subject&#39;. You can ## override using the `.groups` argument. In der R Notation wird ein Random Intercept so geschrieben: (1 | subject). Hierbei steht 1 für Intercept, ausgesprochen bedeutet die Formel so viel wie “schätze sprecherspezifische Intercepts”. In der Praxis berechnet ein Mixed Model mit einem solchen Random Intercept allerdings nicht ein Intercept pro Person, sondern die Standardabweichung der Versuchspersonen-Intercepts vom geschätzten “allgemeinen” Intercept über alle Datenpunkte. Das ist wichtig, weil die Berechnung des Mixed Models ewig lang dauern und wahrscheinlich Fehler werfen würde, wenn wirklich pro Versuchsperson ein Intercept geschätzt würde. 5.3.2 Random Slopes In der obigen Abbildung sehen Sie neben den Mittelwerten für /a/ auch die für /i/ sowie eine gestrichelte Linie, die beide verbindet. Diese stellt hier sozusagen die ideale Regressionslinie pro Versuchsperson dar. Da die Linien ganz unterschiedlich steil verlaufen, können wir erahnen, dass die einzelnen Versuchspersonen unterschiedliche Slopes haben. Versuchsperson S7 macht z.B. kaum einen Lautstärke-Unterschied zwischen /a/ und /i/ (die Slope ist ca. 0), während Versuchsperson S6 das /a/ deutlich lauter produziert als das /i/ (die Slope ist negativ). Wir vergleichen die Slopes auch hier wieder nur innerhalb einer Geschlechtsgruppe, d.h. für S1-S5 und für S6-S10. Hier scheint es sinnvoll zu sein, auch sprecherspezifische Random Slopes zu berechnen, weil der Effekt des Vokals auf die Lautstärke unterschiedlich für die Versuchspersonen ist. Allgemein wird davon ausgegangen, dass auch ein Random Intercept geschätzt werden soll, wenn wir Random Slopes schätzen. Daher ist die Formel für Random Slopes (1 + vowel | subject), also “schätze sprecherspezifische Intercepts und sprecherspezifische Slopes in Relation zum Vokal”. Häufig wird anstatt dieser Notation allerdings die Kurzform (vowel | subject) verwendet, da für die Funktion dann klar ist, dass sowohl Random Slope als auch Random Intercept berechnet werden soll. Auch hier wird wieder die Abweichung von der “allgemeinen” Slope geschätzt, und nicht wirklich eine Slope pro Proband. Wenn wirklich nur die sprecherspezifische Slope, nicht aber das varying intercept geschätzt werden soll, lautet die Formel (0 | subject). Vielleicht fragen Sie sich jetzt, warum der Fixed Effect gender im Vergleich zu vowel hier eine irgendwie untergeordnete Rolle zu spielen scheint. Warum z.B. interessieren wir uns nicht für (1 + gender | subject) oder (1 + vowel + gender | subject)? Das liegt daran, dass die Levels der Variable gender nicht pro Versuchsperson variieren, denn die Versuchspersonen sind hier entweder männlich oder weiblich, wie die folgende Tabelle zeigt: table(df$subject, df$gender) ## ## f m ## S1 0 12 ## S2 0 12 ## S3 0 12 ## S4 0 12 ## S5 0 12 ## S6 12 0 ## S7 12 0 ## S8 12 0 ## S9 12 0 ## S10 12 0 Hier gibt es also gar keine Variation, die wir durch die entsprechende Random Slope herausrechnen könnten. Für die Stufen der Variable vowel hingegen liegen jeweils Werte für die Versuchspersonen vor (sie haben sowohl /a/ als auch /i/ produziert). Und das wiederum bedeutet, dass sich der Effekt von Vokal auf die Lautstärke von Versuchsperson zu Versuchsperson unterscheiden kann (der Effekt von Geschlecht auf Lautstärke hingegen kann sich nicht pro Versuchsperson unterscheiden). table(df$subject, df$vowel) ## ## a i ## S1 6 6 ## S2 6 6 ## S3 6 6 ## S4 6 6 ## S5 6 6 ## S6 6 6 ## S7 6 6 ## S8 6 6 ## S9 6 6 ## S10 6 6 5.3.3 Random Effects Struktur für word bestimmen Da wir jetzt wissen, wie der Random Effect für subject aussieht, gehen wir dieselbe Prozedur für die Variable word durch. Dieses Mal fangen wir damit an, herauszufinden, ob pro Wort jeweils Messwerte von beiden Geschlechtern und von beiden Vokalen vorliegen: table(df$word, df$gender) ## ## f m ## w1 10 10 ## w2 10 10 ## w3 10 10 ## w4 10 10 ## w5 10 10 ## w6 10 10 table(df$word, df$vowel) ## ## a i ## w1 20 0 ## w2 20 0 ## w3 20 0 ## w4 0 20 ## w5 0 20 ## w6 0 20 Das scheint für Geschlecht, nicht aber für Vokal der Fall zu sein, d.h. der Effekt von Geschlecht auf die Lautstärke kann sich pro Wort unterscheiden. Der Effekt von Vokal auf die Lautstärke kann sich jedoch nicht von Wort zu Wort unterscheiden, weil in jedem Wort entweder /a/ oder /i/ vorkam. Die maximale Random Effects Struktur, die wir also für die Variable word erstellen könnten, ist (1 + gender | word) (sowohl Random Intercept als auch Random Slope in Relation zum Geschlecht). Zuerst schauen wir, ob wir überhaupt ein Random Intercept für word benötigen (wobei die wortspezifischen Boxplots oben schon ein guter Hinweis darauf sind, dass wir ein Random Intercept benötigen). Wir nutzen wieder einen ähnlichen Plot wie zuvor, sodass wir pro Wort die Lautstärke-Mittelwerte für Frauen vergleichen können (f war das Level der Variable gender, das im Intercept verarbeitet wird). Natürlich könnten Sie diese Mittelwerte aber auch mittels tidyverse-Funktionen berechnen. Die Wörter in der oberen Zeile der Abbildung enthalten /a/, die in der unteren Zeile enthalten /i/. ggplot(df) + aes(x = gender, y = db) + geom_point() + facet_wrap(~word) + geom_point(data = df %&gt;% group_by(word, gender) %&gt;% summarise(db = mean(db)), color = &quot;orange&quot;, size = 3, stroke = 2, shape = 4) + geom_line(data = df %&gt;% group_by(word, gender) %&gt;% summarise(db = mean(db)), aes(x = gender, y = db, group = word), lty = &quot;dashed&quot;) ## `summarise()` has grouped output by &#39;word&#39;. You can ## override using the `.groups` argument. ## `summarise()` has grouped output by &#39;word&#39;. You can ## override using the `.groups` argument. Der Lautstärke-Mittelwert für Frauen (jeweils das linke orange Kreuz pro Panel) unterscheidet sich für verschiedene Wörter. Wort w1 hat zum Beispiel einen deutlich niedrigeren Lautstärke-Mittelwert als w3, wenn es von Frauen produziert wurde. Auch die Mittelwerte für Frauen in Wörtern, die /i/ enthalten (w4-w6) unterscheiden sich z.T. stark voneinander. Dementsprechend ist die Berechnung eines Random Intercepts für word angebracht: (1 | word). Interessanterweise sind die gestrichelten Linien, die die orangen Kreuze verbinden, quasi parallel für die jeweils drei Wörter pro Zeile in der Abbildung. Das heißt, der Effekt von Geschlecht auf die Wörter w1, w2 und w3 ist gleich (oder zumindest sehr ähnlich). Gleiches gilt für die Wörter w4, w5 und w6, die sich in der Steigung der gestrichelten Linie kaum unterscheiden. Innerhalb einer Vokalgruppe ist der Effekt von Geschlecht auf die verschiedenen Wörter also gleich. Das bedeutet, dass eine wortspezifische Random Slope in Relation zum Geschlecht (1 + gender | word) für diesen Datensatz nicht angebracht wäre. Es bleibt bei (1 | word) als Random Effect Struktur für die Variable word. 5.4 LMER in R Das klassische Paket, das für LMERs benutzt wird, heißt lme4. Wir nutzen hier stattdessen lmerTest, das aber ein Wrapper für lme4 ist. Die Funktion für das Berechnen eines Mixed Models heißt lmer(). Unsere vollständige Formel enthält den üblichen ersten Teil für die Fixed Effects (inklusive Interaktion, wenn nötig), und dann die Random Effects für subject und word. Dieses Mal schauen wir uns die Ergebnisse des Mixed Models mit der Funktion summary() an. Die Funktion bekommt optional das Argument corr = F, um die Anzeige einer bestimmten Korrelationstabelle zu unterdrücken, die für uns keine Bedeutung hat. Weiterführende Infos: LMER Ergebnisse anzeigen Anstelle von summary() können Sie sich auch das Paket broom.mixed herunterladen, das die Funktionen tidy(), augment() und glance() für LMERs bereitstellt. Die Funktion lmer() bekommt die Formel, die wir uns mühsam erarbeitet haben, mit der bekannten Fixed Effects Struktur db ~ gender * vowel und dann, durch Pluszeichen verbunden, die Random Effects. Zusätzlich wird der Data Frame mit dem Argument data angegeben. In diesem Fall spezifizieren wir außerdem REML = F. REML steht für Restricted Maximum Likelihood. Indem wir der Funktion das Argument REML = F gegeben haben, wird nun stattdessen eine echte Maximum Likelihood Schätzung der gewünschten Parameter ausgeführt (mehr dazu später). df.lmer &lt;- lmer(db ~ gender * vowel + (1 + vowel | subject) + (1 | word), data = df, REML = F) df.lmer %&gt;% summary(corr = F) ## Linear mixed model fit by maximum likelihood . ## t-tests use Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: ## db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## Data: df ## ## AIC BIC logLik -2*log(L) df.resid ## 850.7 875.8 -416.4 832.7 111 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.286 -0.506 0.018 0.496 2.023 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 215.8 14.69 ## voweli 110.0 10.49 -0.47 ## word (Intercept) 193.1 13.90 ## Residual 28.2 5.31 ## Number of obs: 120, groups: subject, 10; word, 6 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 48.17 10.41 12.49 4.63 ## genderm 40.19 9.39 9.53 4.28 ## voweli -10.01 12.35 7.93 -0.81 ## genderm:voweli -20.21 6.91 9.17 -2.92 ## Pr(&gt;|t|) ## (Intercept) 0.00053 *** ## genderm 0.00180 ** ## voweli 0.44140 ## genderm:voweli 0.01658 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Hier wird uns zuerst angezeigt, was genau für ein Modell berechnet und welche Formel dafür verwendet wurde. Es folgt eine Aufzählung von Maßen für die model selection, nämlich AIC (Akaike information criterion), BIC (Bayesian information criterion) und logLik (logarithmierte Likelihood). Diese absoluten Werte haben keine Bedeutung; wenn sie jedoch verglichen werden mit Werten aus anderen Modellen für dieselben Daten, stehen niedrigere Werte von AIC und BIC sowie höhere Werte von logLik für eine bessere Passgenauigkeit des Modells in Bezug auf die Daten. df.resid steht für residual degrees of freedom; das ist die Anzahl der Datenpunkte minus die Anzahl der geschätzten Parameter. Der Data Frame df hat 120 Beobachtungen (Zeilen, Datenpunkte), und es wurden neun Parameter von unserem Modell geschätzt: für die Fixed Effects: (Intercept), genderm, voweli, genderm:voweli für die Random Effects: subject (Intercept), subject voweli, word (Intercept), Residual, Corr für die sprecherspezifische Slope in Relation zum Vokal. Es folgen die üblichen Kennzahlen für die Residuals. Neu für uns ist die Tabelle für die Random Effects, altbekannt ist die Tabelle für die Fixed Effects. Diese beiden Tabellen schauen wir uns im Folgenden genauer an. 5.4.1 Fixed Effects Die Tabelle für die Fixed Effects kommt uns bekannt vor. Wir sehen die Schätzung für das Intercept und dann die Schätzungen für die Slopes genderm, voweli und die Interaktion genderm:voweli. Der geschätzte dB-Mittelwert für das /a/ von Frauen liegt bei 48.2 dB. Männer sprechen offenbar deutlich lauter, denn die Slope für genderm ist hoch und positiv. Das heißt das /a/ von Männern liegt bei ca. \\(48.2 + 40.2 = 88.4\\) dB. Der Vokal /i/ wird hingegen leiser produziert als /a/, denn die Slope ist negativ mit -10 dB, d.h. das /i/ von Frauen wurde mit ca. \\(48.2 - 10.0 = 38.2\\) dB produziert. Zuletzt sehen wir, dass für das /i/ von Männern nochmal 20.2 dB abgezogen werden müssen, das heißt der geschätzte Mittelwert für das /i/ von Männern liegt bei \\(48.2 + 40.2 + (-10.0) + (-20.2) = 58.2\\) dB. Dazu werden uns die Standard Errors gegeben und die Ergebnisse der \\(t\\)-Statistik, die testet, ob sich die Regressionskoeffizienten signifikant von Null unterscheiden. Laut dieser Statistik hatte Geschlecht einen signifikanten Einfluss auf die Lautstärke (\\(t\\)[9.5] = 4.3, \\(p\\) &lt; 0.01) und die Interaktion zwischen Geschlecht und Vokal war ebenfalls signifikant (\\(t\\)[9.2] = 2.9, \\(p\\) &lt; 0.05). Der Vokal hatte hingegen keinen signifikanten Einfluss auf die Lautstärke. Wie Sie sehen, wird die \\(t\\)-Statistik hier mitsamt der Freiheitsgrade aus der Spalte df berichtet, denn die Freiheitsgrade sind der bestimmende Parameter für die Student-\\(t\\)-Verteilung. Die Freiheitsgrade sind hier ebenfalls eine Schätzung, deshalb sind die Werte häufig Dezimalzahlen. Da es eine signifikante Interaktion zwischen den kategorialen unabhängigen Variablen gibt, werden wir später noch post-hoc Tests mit emmeans ausführen. 5.4.2 Random Effects Nun schauen wir uns die Random Effects genauer an. Wie vorhin angedeutet, werden hier nicht z.B. pro Versuchsperson je ein Intercept und eine Slope berechnet, sondern es wird die Standardabweichung der personenspezifischen Intercepts und Slopes vom geschätzten “allgemeinen” Intercept und der geschätzten “allgemeinen” Slope geschätzt. Jeder Wert in der Spalte Std.Dev ist deshalb ein vom Mixed Model geschätzter Parameter. Die Standardabweichung für die sprecherspezifischen Intercepts liegt hier bei 14.7 dB, d.h. die Sprechervariation um das “allgemeine” Intercept von 48.2 dB liegt bei \\(\\pm 14.7\\) dB, was in Relation recht viel ist. Wir können hier die 68-95-99.7 Regel anwenden: 95% der sprecherspezifischen Intercepts sollten im Bereich von \\(Intercept \\pm 2 \\cdot Std.Dev\\) liegen, also zwischen 77.6 dB und 18.8 dB. Dieser große Wertebereich zeigt, dass es tatsächlich große Unterschiede in den Intercepts der verschiedenen Versuchspersonen gab. Die Sprechervariation um die “allgemeine” Slope für voweli von -10 dB liegt bei 10.5 dB, was eine sehr große Standardabweichung ist: das heißt, das 95% der sprecherspezifischen Slopes im Bereich von -31 dB bis 11 dB liegen, was wiederum bedeutet, dass der Vokaleffekt pro Sprecher äußerst unterschiedlich war. Das rechtfertigt nochmal die Berechnung der sprecherspezifischen Random Slope. Zusätzlich steht in der Spalte Corr der Korrelationswert \\(r\\) für die Korrelation zwischen dem sprecherspezifischen Random Intercept und der sprecherspezifischen Random Slope. Da die Korrelation hier negativ ist, bedeutet das, dass Probanden mit einem höheren Intercept eine steilere negative Slope für /i/ haben. Wenn also jemand das /a/ besonders laut produzierte, produzierte die Person auch ein leiseres /i/ (Achtung: wir beschreiben hier keine Kausalität, sondern eine Korrelation!). Die Korrelation Corr gilt als ein weiterer Parameter, der vom Modell geschätzt wurde. 5.5 Konvergenzprobleme und Modell vereinfachen Beim Berechnen von LMERs treten regelmäßig sogenannte Konvergenzprobleme auf. Der häufigste Fehler ist folgender: Dieser Fehler bedeutet, grob gesagt, dass das gewünschte Modell nicht geschätzt werden konnte. Das liegt meist daran, dass man unnötig komplexe Random Effect Strukturen und/oder insgesamt zu viele unabhängige Variablen und Interaktionen eingebaut hat. Man sollte sich also wirklich gut überlegen, welche Formel man für das Mixed Model verwendet; jeder Fixed Effect, jede Interaktion und jeder Random Effect sollte sinnvoll sein für die Daten und für die Fragestellung. Zuvor haben wir festgestellt, dass es für die Daten in df nicht sinnvoll wäre, eine Random Slope für Geschlecht gegeben Wort (1 + gender | word) schätzen zu lassen. Das machen wir jetzt trotzdem, um zu zeigen, wie man mit dem auftretenden Fehler umgehen sollte. df.wrong &lt;- lmer(db ~ gender * vowel + (1 + vowel | subject) + (1 + gender | word), data = df, REML = F) ## boundary (singular) fit: see help(&#39;isSingular&#39;) df.wrong %&gt;% summary(corr = F) ## Linear mixed model fit by maximum likelihood . ## t-tests use Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: ## db ~ gender * vowel + (1 + vowel | subject) + (1 + gender | word) ## Data: df ## ## AIC BIC logLik -2*log(L) df.resid ## 854.6 885.3 -416.3 832.6 109 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2670 -0.5051 0.0023 0.4921 2.0442 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 215.7933 14.690 ## voweli 110.0413 10.490 -0.47 ## word (Intercept) 197.0661 14.038 ## genderm 0.0809 0.284 -1.00 ## Residual 28.1476 5.305 ## Number of obs: 120, groups: subject, 10; word, 6 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 48.17 10.48 12.26 4.60 ## genderm 40.19 9.39 9.53 4.28 ## voweli -10.01 12.46 7.78 -0.80 ## genderm:voweli -20.21 6.92 9.18 -2.92 ## Pr(&gt;|t|) ## (Intercept) 0.00058 *** ## genderm 0.00180 ** ## voweli 0.44562 ## genderm:voweli 0.01660 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) Durch die Einführung der wortspezifischen Random Slope ist tatsächlich ein Konvergenzproblem entstanden – und trotzdem wird uns das Ergebnis angezeigt. Dieses Ergebnis ist jedoch nicht vertrauenswürdig und darf unter keinen Umständen berichtet werden. Schauen wir uns den Random Effect word an. Hier wurde zuerst ein Random Intercept geschätzt, für das eine Standardabweichung von 14 dB geschätzt wurde. So weit so gut. Dann sehen wir aber, dass die Random Slope eine extrem kleine Standardabweichung hat mit nur 0.3 dB. Das bedeutet, dass das Wort gar nicht mit dem Geschlecht der Versuchspersonen variiert (das hatten wir zuvor mittels einer Abbildung festgestellt). Das Mixed Model hat aber trotzdem versucht, die Random Slope und die Korrelation zwischen Random Intercept und Random Slope zu schätzen – und ist gescheitert. Das sehen Sie daran, dass \\(r = -1\\) (auch bei \\(r = 1\\)) in der Spalte Corr, denn eine perfekte Korrelation gibt es nicht. Die 1 oder -1 zeigt, dass die Korrelation nicht geschätzt werden konnte. Für solche Fälle gibt es die step() Funktion aus dem lmerTest Paket. Diese Funktion schaut sich alle Fixed und Random Effects Strukturen im Modell an und berechnet, welche davon signifikant sind und damit etwas Essentielles zum Modell beitragen und welche nicht. Die Variablen, die nichts beitragen, werden eliminiert. So bleibt am Ende ein Modell, das nur die statistisch relevanten Variablen mit einbezieht. Das heißt aber im übrigen nicht, dass Sie nicht trotzdem Variablen im Modell behalten können, die nicht signifikant sind (solange das Modell dann noch konvergiert)! Hier sehen Sie das Ergebnis von step(): df.step &lt;- df.wrong %&gt;% step() ## Warning: the &#39;nobars&#39; function has moved to the reformulas package. Please update your imports, or ask an upstream package maintainter to do so. ## This warning is displayed once per session. ## Warning: the &#39;findbars&#39; function has moved to the reformulas package. Please update your imports, or ask an upstream package maintainter to do so. ## This warning is displayed once per session. ## boundary (singular) fit: see help(&#39;isSingular&#39;) df.step ## Backward reduced random-effect table: ## ## Eliminated npar logLik ## &lt;none&gt; 11 -416 ## gender in (1 + gender | word) 1 9 -416 ## vowel in (1 + vowel | subject) 0 7 -442 ## (1 | word) 0 8 -512 ## AIC LRT Df ## &lt;none&gt; 855 ## gender in (1 + gender | word) 851 0.1 2 ## vowel in (1 + vowel | subject) 897 50.6 2 ## (1 | word) 1040 191.0 1 ## Pr(&gt;Chisq) ## &lt;none&gt; ## gender in (1 + gender | word) 0.96 ## vowel in (1 + vowel | subject) 1e-11 *** ## (1 | word) &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Backward reduced fixed-effect table: ## Degrees of freedom method: Satterthwaite ## ## Eliminated Sum Sq Mean Sq NumDF DenDF ## gender:vowel 0 241 241 1 9.17 ## F value Pr(&gt;F) ## gender:vowel 8.55 0.017 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Model found: ## db ~ gender + vowel + (1 + vowel | subject) + (1 | word) + gender:vowel Es kommt vor, dass die Funktion mehrfach den obigen Fehler wirft, und zwar immer dann, wenn sie ein neues Modell mit einer angepassten Formel berechnet und der Fehler immer noch da ist. Zuerst sehen wir im Ergebnis die Backward reduced random-effect table. Die backward reduction ist ein Prozess, bei dem zuerst das komplexeste Modell ausprobiert wird und dann werden nach und nach die nicht signifikanten Terme entfernt. Hier sehen wir, dass die Random Slope gender in dem Random Effect (1 + gender | word) nicht signifikant ist, und deshalb entfernt wurde. Es bleibt daher nur noch das Random Intercept (1 | word) übrig. Auch dieses wurde nochmal evaluiert, in Zeile 3, und für wichtig befunden. Auch die Random Slope vowel in (1 + vowel | subject) kann bleiben. Es folgt dieselbe Prozedur für die Fixed Effects. Zuletzt zeigt uns step() an, welches Modell es schließlich ausgewählt hat, nämlich: db ~ gender + vowel + (1 + vowel | subject) + (1 | word) + gender:vowel Unsere Fixed Effects werden hier ausgeschrieben als gender + vowel + ... gender:vowel. Random Intercept und Slope für Versuchspersonen sind unverändert, und für den Random Factor word wird jetzt nur noch ein Intercept geschätzt. Also erhalten wir hier genau das Modell, das wir zuvor schon berechnet haben. Mit der Funktion get_model(), angewendet auf das Ergebnis der step()-Funktion, können wir uns die Ergebnisse des vereinfachten Modells anzeigen lassen: df.lmer.new &lt;- df.step %&gt;% get_model() df.lmer.new %&gt;% summary(corr = F) ## Linear mixed model fit by maximum likelihood . ## t-tests use Satterthwaite&#39;s method [lmerModLmerTest] ## Formula: ## db ~ gender + vowel + (1 + vowel | subject) + (1 | word) + gender:vowel ## Data: df ## ## AIC BIC logLik -2*log(L) df.resid ## 850.7 875.8 -416.4 832.7 111 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.286 -0.506 0.018 0.496 2.023 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 215.8 14.69 ## voweli 110.0 10.49 -0.47 ## word (Intercept) 193.1 13.90 ## Residual 28.2 5.31 ## Number of obs: 120, groups: subject, 10; word, 6 ## ## Fixed effects: ## Estimate Std. Error df t value ## (Intercept) 48.17 10.41 12.49 4.63 ## genderm 40.19 9.39 9.53 4.28 ## voweli -10.01 12.35 7.93 -0.81 ## genderm:voweli -20.21 6.91 9.17 -2.92 ## Pr(&gt;|t|) ## (Intercept) 0.00053 *** ## genderm 0.00180 ** ## voweli 0.44140 ## genderm:voweli 0.01658 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.6 Ergebnisse berichten Wir kennen die Ergebnisse der \\(t\\)-Tests für die Regressionskoeffizienten und haben festgestellt, dass die Interaktion zwischen den beiden kategorialen unabhängigen Variablen signifikant war. Deshalb führen wir hier der Vollständigkeit halber die paarweisen Vergleiche mit emmeans aus: emmeans(df.lmer.new, pairwise ~ vowel | gender)$contrasts ## gender = f: ## contrast estimate SE df t.ratio p.value ## a - i 10.0 14.4 10 0.696 0.5024 ## ## gender = m: ## contrast estimate SE df t.ratio p.value ## a - i 30.2 14.4 10 2.101 0.0619 ## ## Degrees-of-freedom method: kenward-roger emmeans(df.lmer.new, pairwise ~ gender | vowel)$contrasts ## vowel = a: ## contrast estimate SE df t.ratio p.value ## f - m -40.2 10.10 10.9 -3.965 0.0023 ## ## vowel = i: ## contrast estimate SE df t.ratio p.value ## f - m -20.0 9.32 10.8 -2.144 0.0557 ## ## Degrees-of-freedom method: kenward-roger Wenn Sie Ihre Ergebnisse berichten, beschreiben Sie auch das Modell ausführlich; in einer wissenschaftlichen Publikation verwenden Sie zusätzlich einige Zeilen darauf, die Richtung der Fixed Effects zu beschreiben (z.B. “Das Mixed Model zeigte, dass /a/ lauter produziert wurde als /i/, insbesondere von männlichen Probanden”) und ob das mit Ihren Erwartungen/Hypothesen übereinstimmt. Hier ist ein Bespiel für einen kurzen Ergebnisbericht zu der Fragestellung: Wird die Lautstärke von dem Geschlecht und dem Vokal beeinflusst? Es wurde eine Linear Mixed Effect Regression durchgeführt mit der Lautstärke in Dezibel als abhängiger Variable, sowie mit Fixed Effects für Geschlecht und Vokal inklusive der Interaktion zwischen den unabhängigen Variablen, und den Random Effects für Versuchspersonen und Wörter. Für Versuchspersonen wurden sowohl Random Intercept als auch Random Slope und für Wörter nur das Random Intercept geschätzt. Das Modell ergab einen signifikanten Einfluss von Geschlecht (\\(t\\)[9.5] = 4.3, \\(p\\) &lt; 0.01) auf die Lautstärke. Es gab zusätzlich eine signifikante Interaktion zwischen Geschlecht und Vokal (\\(t\\)[9.2] = 2.9, \\(p\\) &lt; 0.05). Post-hoc \\(t\\)-Tests zeigten eine signifikante Differenz zwischen Frauen und Männern für /a/ (\\(t\\)[10.8] = 4.0, \\(p\\) &lt; 0.01), jedoch nicht für /i/. 5.7 Gütekriterien für Mixed Models 5.7.1 Marginal und Conditional \\(R^2\\) Sie haben vielleicht bemerkt, dass uns die summary() Funktion weder \\(R^2\\) noch die \\(F\\)-Statistik ausgegeben hat, wie das zuvor bei den Regressionen mit lm() der Fall war. Im Fall von \\(R^2\\) liegt das daran, dass jetzt separat für Fixed und Random Effects evaluiert werden muss, wie viel Varianz in den Daten sie jeweils beschreiben. Das Paket MuMIn stellt die Funktion r.squaredGLMM() bereit, die wir auf unser Mixed Model anwenden können: df.lmer %&gt;% r.squaredGLMM() ## R2m R2c ## [1,] 0.4586 0.9637 Hier werden uns jetzt folgerichtig zwei \\(R^2\\)-Werte angezeigt. Der erste, R2m (das steht für marginal \\(R^2\\)), ist die Proportion der Varianz in den gemessenen Lautstärkewerten, die durch die Fixed Effects beschrieben wird, hier also 46%. Der zweite, R2c (für conditional \\(R^2\\)), ist die Proportion der Varianz in den gemessenen Lautstärkewerten, die durch die Fixed Effects und Random Effects zusammen beschrieben wird. Mit 96.4% haben wir also quasi die gesamte Varianz in den Daten beschrieben! (Solch hohe Werte werden Sie in echten linguistischen Studien niemals sehen). Hieraus können wir ableiten, dass die Random Effects \\(96.4 - 45.9 = 50.5\\) Prozent der Varianz beschreiben. Es war also für den Datensatz df sehr wichtig, dass wir die Random Effects im Modell hatten. 5.7.2 Likelihood Ratio Tests Im Ergebnis des Modells fehlte außer dem \\(R^2\\)-Wert ein Indikator für die Güte des Modells, wie es die \\(F\\)-Statistik bei lm() war. Bei LMERs wird die Güte mit Likelihood Ratio Tests evaluiert. Dabei wird das verwendete Mixed Model mit einem anderen Mixed Model verglichen, bei dem ein Fixed oder Random Effect eliminiert wurde. So kann eingeschätzt werden, ob durch das Weglassen eines Faktors die Güte des Modells sinkt oder gleich bleibt. An dieser Stelle sollten wir uns mit dem Begriff likelihood beschäftigen, vor allem im Unterschied zu probability (denn beide werden im Deutschen leider mit “Wahrscheinlichkeit” übersetzt). Im statistischen Kontext ist probability die Wahrscheinlichkeit eines Ergebnisses, wenn die Parameter (z.B. Regressionskoeffizienten) gegeben sind. Die \\(p\\)-Werte beschreiben diese Art von Wahrscheinlichkeit. Die likelihood hingegen ist die Wahrscheinlichkeit von Parametern, wenn die Daten gegeben sind. Wie wahrscheinlich (likely) ist die Schätzung für das Intercept (hier: 48.2 dB) für die Daten in df? Oder wie wahrscheinlich ist die Schätzung für die Slope for Geschlecht (40.2 dB) für die Daten in df? Mixed Models führen diese Schätzungen nach dem Prinzip der Maximum Likelihood aus (nachdem wir REML = F in lmer() angegeben haben), d.h. das Ziel des Mixed Models ist es, die Regressionskoeffizienten zu finden, die am wahrscheinlichsten (likely) für die gegebenen Daten sind. Im Ergebnis eines Mixed Models mit lmer() finden wir unter anderem die drei Werte AIC, BIC und log Likelihood. Letztere ist die logarithmierte maximierte Likelihood; dieser Wert ist immer negativ; je näher er jedoch an Null ist, desto besser ist das Modell. Der Likelihood Ratio Test vergleicht die log Likelihoods zweier Mixed Models und findet heraus, ob das gewählte Modell die Daten signifikant besser beschreibt als ein Modell, bei dem eine Variable weggelassen wird. Hier vergleichen wir unser Modell df.lmer mit verschiedenen anderen Modellen; zuerst mit einem Modell, bei dem die Variable gender weggelassen wurde. df.gender &lt;- lmer(db ~ vowel + (1 + vowel | subject) + (1 | word), data = df, REML = F) Die Funktion anova() testet den Vergleich der Modelle auf Signifikanz mithilfe eines \\(\\chi^2\\)-Tests (sprich: chi /kai/ squared, oder Chi-Quadrat). “Anova” steht eigentlich für analysis of variance; der default der anova()-Funktion ist allerdings der \\(\\chi^2\\)-Test durch das Argument test = \"Chisq\". Ansonsten bekommt diese Funktion nur die Namen der beiden Modelle übergeben. Der \\(\\chi^2\\)-Test prüft, ob sich die logarithmierte Likelihood der beiden Modelle signifikant voneinander unterscheidet. anova(df.lmer, df.gender) ## Data: df ## Models: ## df.gender: db ~ vowel + (1 + vowel | subject) + (1 | word) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.gender 7 857 877 -422 843 ## df.lmer 9 851 876 -416 833 10.6 2 ## Pr(&gt;Chisq) ## df.gender ## df.lmer 0.005 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Im Ergebnis dieses Tests sehen wir die beiden Modelle, die miteinander verglichen wurden. In der ersten Spalte der Tabelle, npar, steht die Anzahl an Parametern, die für das Modell geschätzt wurde. Anschließend sehen wir die Informationskriterien AIC, BIC, logLik sowie deviance, die wir schon aus den Ergebnissen von lmer() kennen. In der oberen Zeile stehen die Werte für das restriktive Modell, in der unteren Zeile die unseres Modells. Für unser Modell wird zusätzlich der \\(\\chi^2\\)-Wert in der Spalte Chisq angegeben. Die \\(\\chi^2\\)-Verteilung wird durch den Parameter Df (Freiheitsgrade) beschrieben. Die Freiheitsgrade sind hier die Anzahl an Regressionskoeffizienten, die im restriktiven Modell nicht geschätzt wurden (hier: Fixed Effect für gender und Interaktion zwischen gender und vowel; auch berechenbar aus der Differenz der beiden Werte in Spalte npar). In der Spalte Pr(&gt;Chisq) findet sich schließlich der \\(p\\)-Wert, der aus der \\(\\chi^2\\)-Verteilung mit zwei Freiheitsgraden für den Wert 10.59 abgelesen wurde. Da der \\(p\\)-Wert unter 0.05 liegt, unterscheiden sich die beiden Modelle signifikant; da die log Likelihood für unser Modell höher ist als die des restriktiven Modells (und AIC sowie BIC niedriger sind für unser als für das restriktive Modell), passt unser Modell besser zu den Daten als ein Modell ohne die Variable gender. Die Formel für den \\(\\chi^2\\)-Wert ist: \\[ \\begin{aligned} LR &amp;= -2ln \\cdot \\left( \\frac{L_{m_1}}{L_{m_2}} \\right) \\\\ &amp;= 2 \\cdot (log(L_{m_2}) - log(L_{m_1})) \\end{aligned} \\] \\(LR\\) steht hier für likelihood ratio, deshalb enthält die Formel auch eine Division, nämlich die der Likelihood \\(L\\) für das restriktive Modell \\(m_1\\) geteilt durch die Likelihood für unser volles berechnetes Modell \\(m_2\\). \\(ln\\) steht für den natürlichen Logarithmus. Diese Formel lässt sich jedoch so umformen, dass wir anstatt den uns unbekannten Likelihoods unsere log Likelihoods aus dem Ergebnis von lmer() verwenden können. Für den Likelihood Ratio Test oben kann man den \\(\\chi^2\\)-Wert also manuell wie folgt berechnen: 2 * (-416.36 - (-421.65)) ## [1] 10.58 Weiterführende Infos: \\(\\chi^2\\)-Verteilung Genau wie Sie das schon für die Normal-, \\(t\\)- und \\(F\\)-Verteilung kennengelernt haben, geben Ihnen die Funktionen dchisq(), pchisq() und qchisq() die Möglichkeit, selbst mit der \\(\\chi^2\\)-Verteilung zu arbeiten. Sie können sich also selber, wenn gewünscht, eine \\(\\chi^2\\)-Verteilung mit den gewünschten Freiheitsgraden zeichnen oder den \\(p\\)-Wert für einen bestimmten \\(\\chi^2\\)-Wert berechnen. Siehe auch: ?dchisq Zuletzt berichten wir das Ergebnis des Likelihood Ratio Tests: Ein Likelihood Ratio Test ergab, dass das gewählte Modell bessere Schätzungen für die Modellparameter abgab als ein vergleichbares Mixed Model ohne die Variable gender (\\(\\chi^2\\)[2] = 10.6, \\(p\\) &lt; 0.01)… Nun können wir dasselbe für einige weitere Vergleiche machen: df.vowel &lt;- lmer(db ~ gender + (1 | subject) + (1 | word), data = df, REML = F) anova(df.lmer, df.vowel) ## Data: df ## Models: ## df.vowel: db ~ gender + (1 | subject) + (1 | word) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.vowel 5 939 953 -465 929 ## df.lmer 9 851 876 -416 833 96.7 4 ## Pr(&gt;Chisq) ## df.vowel ## df.lmer &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 …und als ein vergleichbares Mixed Model ohne die Variable vowel (\\(\\chi^2\\)[4] = 96.7, \\(p\\) &lt; 0.001)… df.subject &lt;- lmer(db ~ gender * vowel + (1 | word), data = df, REML = F) anova(df.lmer, df.subject) ## Data: df ## Models: ## df.subject: db ~ gender * vowel + (1 | word) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.subject 6 1018 1035 -503 1006 ## df.lmer 9 851 876 -416 833 174 3 ## Pr(&gt;Chisq) ## df.subject ## df.lmer &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 … und als ein vergleichbares Mixed Model ohne die Variable subject (\\(\\chi^2\\)[3] = 173.6, \\(p\\) &lt; 0.001)… df.word &lt;- lmer(db ~ gender * vowel + (1 + vowel | subject), data = df, REML = F) anova(df.lmer, df.word) ## Data: df ## Models: ## df.word: db ~ gender * vowel + (1 + vowel | subject) ## df.lmer: db ~ gender * vowel + (1 + vowel | subject) + (1 | word) ## npar AIC BIC logLik -2*log(L) Chisq Df ## df.word 8 1040 1062 -512 1024 ## df.lmer 9 851 876 -416 833 191 1 ## Pr(&gt;Chisq) ## df.word ## df.lmer &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 … und schließlich als ein vergleichbares Mixed Model ohne die Variable word (\\(\\chi^2\\)[1] = 191.0, \\(p\\) &lt; 0.001). "],["logistische-regression.html", "6 Logistische Regression 6.1 Pakete und Daten laden 6.2 Von der linearen zur logistischen Regression 6.3 Die Sigmoidal-Funktion und der Umkipppunkt 6.4 Umkipppunkte in Perzeptionstests 6.5 Kategorialer unabhängiger Faktor", " 6 Logistische Regression 6.1 Pakete und Daten laden Laden Sie die folgenden Pakete und Data Frames: library(magrittr) library(tidyverse) url &lt;- &quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot; ovokal &lt;- read.table(file.path(url, &quot;ovokal.txt&quot;)) %&gt;% as_tibble() pvp &lt;- read.table(file.path(url, &quot;pvp.txt&quot;)) %&gt;% as_tibble() sz &lt;- read.table(file.path(url, &quot;sz.txt&quot;)) %&gt;% as_tibble() 6.2 Von der linearen zur logistischen Regression Die logistische Regression ist (genau wie die lineare Regression) ein statistischer Test, der prüft, ob eine abhängige Variable von einem unabhängigen Faktor beeinflusst wird. Der Unterschied zur linearen Regression ist, dass die abhängige Variable in der logistischen Regression immer kategorial und binär ist und die unabhängige Variable entweder numerisch (kontinuierlich) oder kategorial sein kann. Mit der logistischen Regression können wir unter der Annahme eines Zusammenhangs zwischen abhängiger und unabhängiger Variable die Wahrscheinlichkeit eines bestimmten Wertes schätzen. Beispiele: Inwiefern wird die Vokalisierung von einem finalen /l/ im Englischen (feel vs. ‘feeu’) vom Dialekt beeinflusst? Abhängige Variable: Vokalisierung (kategorial mit zwei Stufen: ja, nein) Unabhängige Variable: Dialekt (kategorial mit zwei oder mehr Stufen) Wird “passt” in Augsburg im Vergleich zu München eher mit /ʃ/ produziert? Abhängige Variable: Frikativ (kategorial mit zwei Stufen: /s/, /ʃ/) Unabhängige Variable: Dialekt (kategorial mit zwei Stufen: Augsburg, München) Der Vokal /a/ in /lam/ wird mit unterschiedlichen Dauern synthetisiert und HörerInnen vorgespielt. Hören die ProbandInnen eher “lahm” (langes /a:/) als “Lamm” (kurzes /a/) mit zunehmender Dauer des Vokals? Abhängige Variable: Vokal (kategorial mit zwei Stufen: /a/, /a:/) Unabhängige Variable: Dauer (kontinuierlich) Da die abhängige Variable in der logistischen Regression immer ein Faktor mit zwei Stufen ist, kann man diese Stufen auch als 1 und 0 kodieren und fragen, was auf der Grundlage der gegebenen Daten die Wahrscheinlichkeit \\(P\\) ist, dass die abhängige Variable \\(y\\) den Wert 1 annimmt: \\(P(y = 1)\\). Genauso können wir nach der Wahrscheinlichkeit \\(Q\\) fragen, dass \\(y\\) den Wert 0 annimmt: \\(1 - P(y = 1)\\). Für das dritte Beispiel oben würde das folgendes bedeuten: \\(P\\): Wahrscheinlichkeit, dass ProbandInnen mit steigender Vokaldauer “lahm” hören (“Erfolg”, denn auf der Grundlage unseres Wissens oder unserer bisherigen Erkenntnisse z.B. aus anderen Experimenten gehen wir davon aus, dass die Probanden bei steigender Vokaldauer “lahm” hören sollten) \\(Q\\): Wahrscheinlichkeit, dass ProbandInnen mit steigender Vokaldauer “Lamm” hören (“Misserfolg”, denn wieder auf der Grundlage unserer bisherigen Erkenntnisse zu diesem Sachverhalt gehen wir davon aus, dass es seltsam wäre, wenn die Probanden bei steigender Vokaldauer “Lamm” hören würden) Die Division (das Verhältnis) von \\(P\\) und \\(Q\\) wird als Odds (Gewinnchancen) bezeichnet: \\(Odds = \\frac{P(y = 1)}{1 - P(y = 1)} = \\frac{P}{Q}\\) Die Gewinnchancen liegen immer in einem Wertebereich von 0 bis unendlich. Nun könnte man überlegen, einfach die Odds als abhängige Variable in einer linearen Regression zu verwenden, denn jetzt handelt es sich ja nicht mehr um eine kategoriale, binäre abhängige Variable. Das Problem ist, dass lm() nicht weiß, dass die Odds nur Werte von Null bis Unendlich annehmen können und daher auch Werte außerhalb dieses Bereichs vorhersagen würde. Außerdem sagt das Verhältnis von \\(P\\) und \\(Q\\) nichts darüber aus, wie viele Beobachtungen in die Berechnung dieses Verhältnisses eingeflossen sind (je mehr Beobachtungen, desto aussagekräftiger ist die berechnete Gewinnchance). Wir brauchen also eine Funktion, die uns die Odds in etwas umwandelt, dass zum einen in den Wertebereich \\(\\pm\\)unendlich fällt und zum anderen die Proportionen anhand der Anzahl der Beobachtungen gewichtet. Diese Funktion heißt allgemein Linkfunktion (link function) und ist im Fall der logistischen Regression die Logit Transformation der Odds. Der Logit ist der Logarithmus der Gewinnchancen und wird deshalb auch als log odds bezeichnet: \\(log(\\frac{P}{Q})\\) 6.2.1 Ein Beispiel für \\(P\\), \\(Q\\) und Logit Zwischen 1950 und 2005 sollen Wörter wie lost in einer aristokratischen Form des Englischen (Received Pronunciation) immer weniger mit einem hohen Vokal /lost/ und zunehmend mit einem tiefen Vokal /lɔst/ produziert worden sein. Für diese Vermutung haben wir Daten im Data Frame ovokal: head(ovokal) ## # A tibble: 6 × 3 ## Jahr Vokal Vpn ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1950 hoch S1 ## 2 1950 hoch S2 ## 3 1950 hoch S3 ## 4 1950 hoch S4 ## 5 1950 hoch S5 ## 6 1950 hoch S6 Unsere Forschungsfrage lautet: Wird die Aussprache des Vokals (hoch vs. tief = abhängige Variable) vom Jahr (1950… 2005 = unabhängige numerische Variable) beeinflusst? Wir wollen \\(P\\) (die Wahrscheinlichkeit, dass der Vokal tief produziert wurde) und \\(Q\\) (die Wahrscheinlichkeit, dass der Vokal hoch produziert wurde) pro Jahr berechnen. Unseren bisherigen Erkenntnissen nach ist die Richtung der Veränderung von der hohen hin zur tiefen Aussprache des Vokals /o/, also bezeichnen wir es als “Erfolg”, wenn der Vokal tief produziert wurde, und als “Misserfolg”, wenn er hoch produziert wurde. Wir kodieren nun als ersten Schritt in der Berechnung von \\(P\\) und \\(Q\\) tiefe und hohe Aussprache als 1 und 0, bzw. als TRUE und FALSE: ovokal %&lt;&gt;% mutate(Erfolg = ifelse(Vokal == &quot;tief&quot;, TRUE, FALSE), Misserfolg = !Erfolg) head(ovokal) ## # A tibble: 6 × 5 ## Jahr Vokal Vpn Erfolg Misserfolg ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1950 hoch S1 FALSE TRUE ## 2 1950 hoch S2 FALSE TRUE ## 3 1950 hoch S3 FALSE TRUE ## 4 1950 hoch S4 FALSE TRUE ## 5 1950 hoch S5 FALSE TRUE ## 6 1950 hoch S6 FALSE TRUE Dann nehmen wir uns das erste Jahr vor, 1950, und berechnen hierfür \\(P\\) und \\(Q\\), indem wir zählen, wie viele “Erfolge” bzw. “Misserfolge” wir für dieses Jahr haben: P &lt;- ovokal %&gt;% filter(Jahr == 1950) %&gt;% summarise(P = sum(Erfolg)) %&gt;% pull(P) P ## [1] 5 Q &lt;- ovokal %&gt;% filter(Jahr == 1950) %&gt;% summarise(Q = sum(Misserfolg)) %&gt;% pull(Q) Q ## [1] 30 Das heißt im Jahr 1950 wurde der Vokal /o/ in Wörtern wie lost nur 5 Mal tief, aber 30 Mal hoch produziert. So müssten wir das jetzt für jede Stufe der unabhängigen Variable (für jede Jahreszahl) machen… Aber das wäre sehr umständlich. Deshalb gruppieren wir die Daten nach Jahr: df &lt;- ovokal %&gt;% group_by(Jahr) %&gt;% summarise(P = sum(Erfolg), Q = sum(Misserfolg)) df ## # A tibble: 6 × 3 ## Jahr P Q ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1950 5 30 ## 2 1960 21 18 ## 3 1971 26 15 ## 4 1980 20 13 ## 5 1993 32 4 ## 6 2005 34 2 Mit \\(P\\) und \\(Q\\) können wir nun die log odds (den Logit) berechnen: df$log_odds &lt;- log(df$P/df$Q) df ## # A tibble: 6 × 4 ## Jahr P Q log_odds ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1950 5 30 -1.79 ## 2 1960 21 18 0.154 ## 3 1971 26 15 0.550 ## 4 1980 20 13 0.431 ## 5 1993 32 4 2.08 ## 6 2005 34 2 2.83 Schauen wir uns die Verteilung der log odds über die Jahrzehnte an: ggplot(df) + aes(x = Jahr, y = log_odds) + geom_point() Es sind diese log odds, durch die wir mittels der logistischen Regression eine Regressionslinie legen werden. Diese Regressionslinie wird genauso definiert wie die lineare Regressionslinie, sie schätzt aber eben die log odds: \\(log(\\frac{P}{Q}) = bx + k\\) Hier gilt wieder: \\(b\\) ist die Steigung \\(x\\) ist ein Wert auf der x-Achse \\(k\\) ist der y-Achsenabschnitt Hier können wir \\(b\\) und \\(k\\) jedoch nicht so leicht berechnen wie bei der linearen Regression, d.h. wir lassen sie uns direkt schätzen. 6.2.2 Die logistische Regressionslinie Bei der linearen Regression haben wir uns die Regressionskoeffizienten mit der Funktion lm() schätzen lassen, die dafür das least squares Verfahren benutzt. Die logistische Regressionslinie wird hingegen mit dem maximum likelihood Verfahren angenähert, das dafür sorgt, dass die geschätzten Datenpunkte des logistischen Modells so ähnlich wie möglich zu den tatsächlichen Werten sind. Zur Schätzung der Regressionskoeffizienten nutzen wir die Funktion glm(), das steht für Generalized Linear Model. Die Funktion bekommt neben der Formel y ~ x und dem Data Frame das Argument family = binomial, das der Funktion mitteilt, dass die Logit-Transformation durchgeführt werden soll. Die abhängige Variable muss ein Faktor sein; ggf. müssen Sie die Variable also mit as.factor() noch in einen Faktor verwandeln: class(ovokal$Vokal) # kein Faktor ## [1] &quot;character&quot; lreg &lt;- glm(as.factor(Vokal) ~ Jahr, family = binomial, data = ovokal) Die Zusammenfassung dieses Modells schauen wir uns etwas später an. Zuerst zeigen wir hier noch die Alternative zu der obigen Anwendung von glm() auf den originalen Daten Frame. glm() kann auch auf \\(P\\) und \\(Q\\) aus dem zusammengefassten Data Frame df ausgeführt werden, indem \\(P\\) und \\(Q\\) durch cbind() verbunden und als abhängige Variable verwendet werden: lreg2 &lt;- glm(cbind(P, Q) ~ Jahr, family = binomial, data = df) Wir können wieder coef() verwenden, um uns die Regressionskoeffizienten (Intercept und Slope) anzeigen zu lassen: coefs &lt;- coef(lreg) coefs ## (Intercept) Jahr ## -138.11742 0.07026 # oder mit $coefficients lreg$coefficients ## (Intercept) Jahr ## -138.11742 0.07026 Mithilfe dieser Parameter kann die gerade Regressionslinie auf die Daten im Logit-Raum überlagert werden. Dazu nutzen wir wieder die beiden Möglichkeiten aus ggplot2. Wenn geom_smooth() benutzt wird, muss method = \"glm\" verwendet werden, und bei geom_abline() benutzen wir die geschätzten coefs für intercept und slope. # mit geom_smooth(): ggplot(df) + aes(x = Jahr, y = log_odds) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # mit geom_abline(): ggplot(df) + aes(x = Jahr, y = log_odds) + geom_point() + geom_abline(intercept = coefs[1], slope = coefs[2], color = &quot;blue&quot;) Die Werte, die durch die logistische Regression geschätzt werden, sind die log odds. Wir können wieder die Funktion predict() verwenden, um uns die geschätzten log odds anzeigen zu lassen: log_odds_estimate &lt;- predict(lreg) log_odds_estimate ## 1 2 3 4 5 6 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 7 8 9 10 11 12 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 13 14 15 16 17 18 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 19 20 21 22 23 24 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 25 26 27 28 29 30 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 ## 31 32 33 34 35 36 ## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -0.4017 ## 37 38 39 40 41 42 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 43 44 45 46 47 48 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 49 50 51 52 53 54 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 55 56 57 58 59 60 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 61 62 63 64 65 66 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 67 68 69 70 71 72 ## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 ## 73 74 75 76 77 78 ## -0.4017 -0.4017 0.3712 0.3712 0.3712 0.3712 ## 79 80 81 82 83 84 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 85 86 87 88 89 90 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 91 92 93 94 95 96 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 97 98 99 100 101 102 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 103 104 105 106 107 108 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 109 110 111 112 113 114 ## 0.3712 0.3712 0.3712 0.3712 0.3712 0.3712 ## 115 116 117 118 119 120 ## 0.3712 1.0036 1.0036 1.0036 1.0036 1.0036 ## 121 122 123 124 125 126 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 127 128 129 130 131 132 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 133 134 135 136 137 138 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 139 140 141 142 143 144 ## 1.0036 1.0036 1.0036 1.0036 1.0036 1.0036 ## 145 146 147 148 149 150 ## 1.0036 1.0036 1.0036 1.0036 1.9170 1.9170 ## 151 152 153 154 155 156 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 157 158 159 160 161 162 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 163 164 165 166 167 168 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 169 170 171 172 173 174 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 175 176 177 178 179 180 ## 1.9170 1.9170 1.9170 1.9170 1.9170 1.9170 ## 181 182 183 184 185 186 ## 1.9170 1.9170 1.9170 1.9170 2.7601 2.7601 ## 187 188 189 190 191 192 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 193 194 195 196 197 198 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 199 200 201 202 203 204 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 205 206 207 208 209 210 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 211 212 213 214 215 216 ## 2.7601 2.7601 2.7601 2.7601 2.7601 2.7601 ## 217 218 219 220 ## 2.7601 2.7601 2.7601 2.7601 Der Output von predict() besteht in diesem Fall aus 220 Zahlen, eine Zahl pro Zeile im originalen Data Frame ovokal. Wie Sie sehen, wiederholen sich die geschätzten log odds. Das liegt daran, dass ein log odd Wert pro Stufe (bzw. Wert) der unabhängigen Variable berechnet wird, in diesem Fall gibt es also sechs einzigartige log odd Werte, einen pro Jahreszahl: unique(log_odds_estimate) ## [1] -1.1043 -0.4017 0.3712 1.0036 1.9170 2.7601 Wir können diese vorhergesagten Werte wieder in Rot in unserem Plot von oben einzeichnen und stellen fest, dass die vorhergesagten Werte genau auf der Regressionslinie liegen (wir verwenden hier geom_abline()): ggplot(df) + aes(x = Jahr, y = log_odds) + geom_point() + geom_abline(intercept = coefs[1], slope = coefs[2], color = &quot;blue&quot;) + geom_point(data = data.frame(x = unique(ovokal$Jahr), y = unique(log_odds_estimate)), mapping = aes(x, y), color = &quot;red&quot;) Genau wie bei der linearen Regression können wir predict() auch benutzen, um uns die log odds Werte vorhersagen zu lassen für x-Werte, die nicht im originalen Datensatz vorkommen. Wenn wir uns zum Beispiel die Logit-Werte für die Jahre 2000 bis 2020 schätzen lassen wollen, funktioniert das wie folgt: predict(lreg, data.frame(Jahr = 2000:2020)) ## 1 2 3 4 5 6 7 8 9 ## 2.409 2.479 2.549 2.620 2.690 2.760 2.830 2.901 2.971 ## 10 11 12 13 14 15 16 17 18 ## 3.041 3.111 3.182 3.252 3.322 3.393 3.463 3.533 3.603 ## 19 20 21 ## 3.674 3.744 3.814 6.2.3 Regression mit glm() Das Ergebnis der Funktion glm() ist ein Objekt mit den Klassen “glm” und “lm”: class(lreg) ## [1] &quot;glm&quot; &quot;lm&quot; Wir wenden die summary()-Funktion auf das Ergebnis der logistischen Regression lreg an und schauen uns das Ergebnis wieder Zeile für Zeile an: summary(lreg) ## ## Call: ## glm(formula = as.factor(Vokal) ~ Jahr, family = binomial, data = ovokal) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -138.1174 20.9988 -6.58 4.8e-11 *** ## Jahr 0.0703 0.0107 6.59 4.3e-11 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 290.57 on 219 degrees of freedom ## Residual deviance: 229.45 on 218 degrees of freedom ## AIC: 233.5 ## ## Number of Fisher Scoring iterations: 4 Die Zusammenfassung beginnt wieder mit dem Call, also der Funktion, die verwendet wurde. 6.2.3.1 Coefficients Es folgt wieder die Tabelle der Regressionskoeffizienten: In der ersten Zeile dieser Tabelle stehen die Kennzahlen für Intercept, in der zweiten für Slope. In der ersten Spalte finden sich wieder die Estimates (Schätzungen) für die Regressionskoeffizienten, die mittels eines maximum likelihood Verfahrens ermittelt wurden. In der zweiten Spalte steht der Standard Error, der beschreibt, wie verlässlich die Schätzungen sind (je kleiner desto besser). Auf die beiden Schätzungen wurde ein Wald Test durchgeführt, der prüft, ob sich die Schätzungen signifikant von Null unterscheiden. Das Ergebnis dieses Tests ist der \\(z\\)-value, der sich auch aus der Division von Estimate und Standard Error berechnen lässt. Hierbei interessiert uns vor allem die zweite Zeile, deren \\(z\\)-Wert und \\(p\\)-Wert zeigen, ob die unabhängige Variable Jahr in signifikantem Maß dazu beiträgt, die log odds Werte zu erklären. Wenn der \\(p\\)-Wert, der in der vierten Spalte steht, kleiner ist als 0.05 (siehe auch die Signifikanzniveau-Sternchen), dann unterscheidet sich der Koeffizient signifikant von Null. Im Fall der abhängigen Variable sehen wir, dass der \\(p\\)-Wert kleiner ist als 0.001, d.h. die Variable ist ein guter Prädiktor für die log odds. Standardmäßig wird nach der Coefficients-Tabelle ein Statement über den Dispersion parameter gedruckt. Das können wir ignorieren. 6.2.3.2 Deviances und AIC In den folgenden zwei Zeilen stehen die null deviance und die residual deviance sowie das AIC (Akaike Information Criterion): Die null deviance beschreibt, wie gut ein Modell ohne unabhängige Variablen die Daten erklären würde. Ein Modell ohne unabhängige Variablen wird nur durch das Intercept charakterisiert. Für sich alleine genommen ist die null deviance schwierig zu interpretieren. Deshalb steht gleich darunter die residual deviance, die beschreibt, wie gut das tatsächlich verwendete Modell die Daten erklären würde. Aus der Differenz zwischen null und residual Deviance lässt sich also erkennen, wie hilfreich unsere unabhängige Variable in dem Modell ist. Die Freiheitsgrade berechnen sich übrigens aus der Anzahl der Beobachtungen im Data Frame minus die Anzahl der Parameter im Modell. Bei der null deviance gibt es nur einen Parameter (Intercept), bei der residual deviance gibt es zwei (Intercept und unabhängige Variable). Je kleiner die Deviances (also die Abweichungen zwischen den tatsächlichen und den geschätzten Werten) sind, desto besser. AIC steht für Akaike Information Criterion und ist vor allem hilfreich, wenn man verschiedene Regressionsmodelle für dasselbe Dataset vergleichen will (wenn man also z.B. für ovokal noch ein Modell mit mehr als einer unabhängigen Variable berechnen würde). Je kleiner AIC, desto besser beschreibt das Modell die Varianz in den Daten. Da wir hier nur das eine Modell haben, ist AIC für uns uninteressant. 6.2.3.3 Fisher Scoring Iterations Bei der logistischen Regression berechnet ein iterativer Algorithmus die Regressionsparameter und die Fisher Scoring iterations geben Auskunft darüber, wie viele Iterationen benötigt wurden. Das ist ebenfalls uninteressant für uns. 6.2.3.4 Deviance Residuals Die Deviance Residuals sind die Unterschiede zwischen den tatsächlichen und den geschätzten Werten. Sie werden uns zwar im Ergebnis der logistischen Regression nicht angezeigt, aber wir können sie uns mit resid() anzeigen lassen und die üblichen deskriptiven Werte mittels summary() berechnen: summary(resid(lreg)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.3755 -0.7566 0.3503 0.0586 0.7903 1.6677 6.2.4 Der \\(\\chi^2\\)-Test &amp; Ergebnisse berichten Bei der linearen Regression haben wir als Prüfstatistik einen \\(F\\)-Test durchgeführt. Anstelle dessen führen wir bei der logistischen Regression einen \\(\\chi^2\\)-Test durch, der prüft, ob das Modell die Daten besser erfasst als ein Intercept-only Modell. Sie kennen diesen Test schon von den gemischten Modellen; er vergleicht zwei Modelle miteinander. Da wir oben in der Coefficients-Tabelle schon gesehen haben, dass der Wald Test für unsere unabhängige Variable signifikant war, wird wahrscheinlich auch das gesamte Modell besser sein als ein Modell ohne unabhängige Variablen. Wir verwenden hierfür wieder die Funktion anova(): anova(lreg) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: as.factor(Vokal) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 219 291 ## Jahr 1 61.1 218 230 5.4e-15 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Das Ergebnis des \\(\\chi^2\\)-Tests hat zwei Zeilen, eine für das Null-Modell (nur Intercept) und das andere für das Modell mit der unabhängigen Variable Jahr. In der Spalte Resid. Df finden sich die Freiheitsgrade für die null und residual deviance, die wiederum in der Spalte Resid. Dev. angegeben werden. Diese Werte wurden in der Zusammenfassung des logistischen Modells in Kurzform berichtet. Uns interessiert aus dem Ergebnis des \\(\\chi^2\\)-Tests vor allem der Wert in der Spalte Pr(&gt;Chi), die den \\(p\\)-Wert enthält. Wenn dieser Wert kleiner ist als 0.05 (siehe auch Signifikanz-Sternchen), dann passt das Model mit der Variable Jahr besser zu den Daten als das Null-Modell. Unsere Ausgangsfrage war: Wird die Aussprache des Vokals (hoch vs. tief) vom Jahr beeinflusst? Nun können wir also berichten: Jahr hatte einen signifikanten Einfluss auf die Proportion von ‘lost’ mit tiefem/hohem Vokal (\\(\\chi^2\\)[1] = 61.1, \\(p\\) &lt; 0.001). 6.3 Die Sigmoidal-Funktion und der Umkipppunkt Die Ergebnisse einer logistischen Regression haben wir oben im Logit-Raum präsentiert. Man kann jedoch auch den y-Achsenabschnitt und die Steigung verwenden, um statt der log odds auf der y-Achse Proportionen abzubilden. In diesem Fall ist die Regressionslinie nicht mehr gerade, sondern sigmoidal (s-förmig). Die Formel für die Sigmoid-Funktion lautet: \\(f(x) = \\frac{e^{bx+k}}{1 + e^{bx+k}}\\) In dieser Formel ist \\(e\\) ist die Exponentialfunktion, \\(b\\) und \\(k\\) sind die Steigung und das Intercept. Je größer die Steigung \\(b\\) ist (in der Abbildung: 1, 0.5, 0.25), desto steiler kippt die Sigmoid-Kurve (schwarz, rot, grün): Wenn die Steigung Null ist, bekommt man eine gerade Linie um den Wert 0.5 auf der y-Achse. Wenn man bei einer Steigung von \\(b = 0\\) den y-Achsenabschnitt \\(k\\) verändert (in der Abbildung: 0, 1, -1), führt das dazu, dass die gerade horizontale Linie nach oben oder unten verschoben wird (schwarz, rot, grün): 6.3.1 Der Umkipppunkt Der Umkipppunkt ist der Punkt, zu dem die Sigmoid-Kurve am steilsten ist. An diesem Punkt ist der Wert auf der y-Achse (die Proportion) immer 0.5 (unten als horizontale Linie). Den x-Wert des Umkipppunkts berechnet man mit \\(\\frac{-k}{b}\\). Für \\(k = 4\\) und \\(b = 0.8\\) wäre das zum Beispiel \\(-4/0.8 = -5\\) (gestrichelte Linie): 6.3.2 Proportionen abbilden Für unser Beispiel oben wollen wir nun Proportionen abbilden und anschließend eine sigmoidale Regressionskurve durch unsere Daten legen. Wir nehmen unseren zusammengefassten Data Frame df und berechnen die Proportion von \\(P\\) (den Anteil von “Erfolgen”) pro Jahr: df$Proportion &lt;- df$P / (df$P + df$Q) df ## # A tibble: 6 × 5 ## Jahr P Q log_odds Proportion ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1950 5 30 -1.79 0.143 ## 2 1960 21 18 0.154 0.538 ## 3 1971 26 15 0.550 0.634 ## 4 1980 20 13 0.431 0.606 ## 5 1993 32 4 2.08 0.889 ## 6 2005 34 2 2.83 0.944 Für das Jahr 1950 liegt der Anteil an “Erfolgen” (wo also der Vokal /o/ tief produziert wurde) bei 14.3%, für das Jahr 1960 dann schon bei 53.8% usw. Diese Proportionen in der neu angelegten Spalte df$Proportion können wir jetzt plotten und dann mittels geom_smooth() eine sigmoidale Regressionslinie durch die Daten legen. Dafür verwenden wir die Argumente method = \"glm\" (generalized linear model), se = F (keinen Standard Error anzeigen) und zusätzlich method.args = list(family = \"quasibinomial\"), damit die Funktion weiß, dass wir Proportionen auf der y-Achse plotten. ggplot(df) + aes(x = Jahr, y = Proportion) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F, method.args = list(family = &quot;quasibinomial&quot;)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; In dieser Abbildung ist das vollständige “S” der sigmoidalen Kurve nicht zu erkennen, weil unser Ausschnitt auf der x-Achse zu klein ist. Wir können uns aber einfach weitere Proportionswerte mit predict() berechnen. Wie wir vorhin gesehen haben, gibt uns predict() aber die log odds aus, und nicht die Proportionen. Die Proportionen erhalten wir, wenn wir in predict() das Argument type = \"response\" nutzen: predict(lreg, type = &quot;response&quot;) ## 1 2 3 4 5 6 7 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 8 9 10 11 12 13 14 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 15 16 17 18 19 20 21 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 22 23 24 25 26 27 28 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 29 30 31 32 33 34 35 ## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 ## 36 37 38 39 40 41 42 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 43 44 45 46 47 48 49 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 50 51 52 53 54 55 56 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 57 58 59 60 61 62 63 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 64 65 66 67 68 69 70 ## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 ## 71 72 73 74 75 76 77 ## 0.4009 0.4009 0.4009 0.4009 0.5917 0.5917 0.5917 ## 78 79 80 81 82 83 84 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 85 86 87 88 89 90 91 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 92 93 94 95 96 97 98 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 99 100 101 102 103 104 105 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 106 107 108 109 110 111 112 ## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 ## 113 114 115 116 117 118 119 ## 0.5917 0.5917 0.5917 0.7318 0.7318 0.7318 0.7318 ## 120 121 122 123 124 125 126 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 127 128 129 130 131 132 133 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 134 135 136 137 138 139 140 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 141 142 143 144 145 146 147 ## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 ## 148 149 150 151 152 153 154 ## 0.7318 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 155 156 157 158 159 160 161 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 162 163 164 165 166 167 168 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 169 170 171 172 173 174 175 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 176 177 178 179 180 181 182 ## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 ## 183 184 185 186 187 188 189 ## 0.8718 0.8718 0.9405 0.9405 0.9405 0.9405 0.9405 ## 190 191 192 193 194 195 196 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 197 198 199 200 201 202 203 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 204 205 206 207 208 209 210 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 211 212 213 214 215 216 217 ## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 ## 218 219 220 ## 0.9405 0.9405 0.9405 Das sind jetzt wieder die geschätzten Werte für alle 220 Beobachtungen im originalen Data Frame. Wir wollen nun ein paar Schätzungen für die Jahre vor 1950 und nach 2010. Also geben wir der predict()-Funktion auch noch einen Data Frame mit den gewünschten Jahreszahlen: more_props &lt;- predict(lreg, data.frame(Jahr = c(1910, 1920, 1930, 1940, 2020, 2030)), type = &quot;response&quot;) more_props ## 1 2 3 4 5 6 ## 0.01955 0.03871 0.07519 0.14101 0.97842 0.98919 Wir bauen uns nun einen Data Frame, der nur Jahr und Proportionen enthält, und zwar aus dem originalen Data Frame und den soeben geschätzten Werten: df_new &lt;- data.frame(Jahr = c(df$Jahr, 1910, 1920, 1930, 1940, 2020, 2030), Proportion = c(df$Proportion, more_props)) ggplot(df_new) + aes(x = Jahr, y = Proportion) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F, method.args = list(family = &quot;quasibinomial&quot;)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Wir können für diese Daten auch noch den Umkipppunkt berechnen und zwar aus den oben bereits gespeicherten coefs: -coefs[1] / coefs[2] ## (Intercept) ## 1966 Das Jahr, in dem sich die Aussprache von /o/ in der Received Pronunciation von “hoch” in “tief” wandelt, ist also laut unserem Modell ungefähr das Jahr 1965. 6.4 Umkipppunkte in Perzeptionstests Umkipppunkte werden häufig in Perzeptionstests verwendet, die wie folgt konstruiert werden: Wir haben ein 11-stufiges Kontinuum zwischen /pUp/ und /pYp/ synthetisiert. Phonetisch betrachtet ist der Unterschied zwischen /U/ und /Y/ der zweite Formant, der bei /U/ niedrig und bei /Y/ hoch ist. Diesen F2-Wert haben wir im Kontinuum also in 11 Schritten langsam verändert. Das erste und letzte Token aus diesem Kontinuum klingen sehr eindeutig wie PUPP oder PÜPP, dazwischen kann es aber schwierig für Hörer sein, zwischen PUPP und PÜPP zu unterscheiden. Nun wurde einigen Probanden jedes Token aus dem Kontinuum in randomisierter Reihenfolge vorgespielt und der Proband musste entscheiden, ob es sich um PUPP oder PÜPP handelte. Uns interessiert, ab welchem F2-Wert die Wahrnehmung der Probanden von PUPP auf PÜPP umschaltet. Anders gesagt: Uns interessiert hier der Umkipppunkt. Daten aus einem solchen Perzeptionsexperiment haben wir im Data Frame pvp gespeichert: head(pvp) ## # A tibble: 6 × 3 ## Vpn F2 Urteil ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 VP18 1239 Y ## 2 VP18 1088 Y ## 3 VP18 803 U ## 4 VP18 956 U ## 5 VP18 1328 Y ## 6 VP18 861 U unique(pvp$Urteil) ## [1] &quot;Y&quot; &quot;U&quot; unique(pvp$F2) ## [1] 1239 1088 803 956 1328 861 989 1121 808 1310 ## [11] 1436 Wir erwarten, dass die Probanden mit steigendem F2-Wert eher /Y/ als /U/ hören, also kodieren wir das Urteil /Y/ als Erfolg und /U/ als Misserfolg: pvp %&lt;&gt;% mutate(Erfolg = ifelse(Urteil == &quot;Y&quot;, T, F), Misserfolg = !Erfolg) head(pvp) ## # A tibble: 6 × 5 ## Vpn F2 Urteil Erfolg Misserfolg ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 VP18 1239 Y TRUE FALSE ## 2 VP18 1088 Y TRUE FALSE ## 3 VP18 803 U FALSE TRUE ## 4 VP18 956 U FALSE TRUE ## 5 VP18 1328 Y TRUE FALSE ## 6 VP18 861 U FALSE TRUE Nun berechnen wir \\(P\\) und \\(Q\\) für die Steps des F2-Kontinuums: df &lt;- pvp %&gt;% group_by(F2) %&gt;% summarise(P = sum(Erfolg), Q = sum(Misserfolg)) df ## # A tibble: 11 × 3 ## F2 P Q ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 803 0 10 ## 2 808 0 10 ## 3 861 0 10 ## 4 956 0 10 ## 5 989 0 10 ## 6 1088 2 8 ## 7 1121 4 6 ## 8 1239 9 1 ## 9 1310 9 1 ## 10 1328 10 0 ## 11 1436 10 0 Anschließend berechnen wir die Proportionen von \\(P\\) und \\(Q\\) und plotten die sigmoidale Regressionslinie in den Daten: df$Proportionen &lt;- df$P / (df$P + df$Q) df ## # A tibble: 11 × 4 ## F2 P Q Proportionen ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 803 0 10 0 ## 2 808 0 10 0 ## 3 861 0 10 0 ## 4 956 0 10 0 ## 5 989 0 10 0 ## 6 1088 2 8 0.2 ## 7 1121 4 6 0.4 ## 8 1239 9 1 0.9 ## 9 1310 9 1 0.9 ## 10 1328 10 0 1 ## 11 1436 10 0 1 ggplot(df) + aes(x = F2, y = Proportionen) + geom_point() + geom_smooth(method = &quot;glm&quot;, se = F, method.args = list(family = &quot;quasibinomial&quot;)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Um den Umkipppunkt dieser Sigmoid-Kurve zu ermitteln, berechnen wir das Generalized Linear Model: pvp.glm &lt;- glm(as.factor(Urteil) ~ F2, family = binomial, data = pvp) Mit den geschätzten Regressionskoeffizienten können wir jetzt den perzeptiven Umkipppunkt der Probanden berechnen: coefs &lt;- coef(pvp.glm) -coefs[1] / coefs[2] ## (Intercept) ## 1151 Das heißt ab einem F2-Wert von ca. 1151 Hz hören die Probanden eher PÜPP als PUPP. Zuletzt wollen wir noch herausfinden, ob die Urteile der Probanden tatsächlich durch F2 beeinflusst wurden. Dafür nutzen wir den \\(\\chi^2\\)-Test: anova(pvp.glm) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: as.factor(Urteil) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 109 148.1 ## F2 1 109 108 39.1 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Wir berichten: Die Proportion von pUp/pYp-Antworten wurde signifikant von F2 beeinflusst (\\(\\chi^2\\)[1] = 109.0, \\(p\\) &lt; 0.001). 6.5 Kategorialer unabhängiger Faktor Die logistische Regression kann auf eine ähnliche Weise verwendet werden, wenn die unabhängige Variable kategorial ist. Der wesentliche Unterschied ist, dass kein Umkipppunkt berechnet und kein Sigmoid abgebildet werden muss. Im Data Frame sz haben wir Informationen darüber abgespeichert, wie 20 Versuchspersonen das Wort “Sonne” aussprechen: entweder mit initialem [s] (stimmlos) oder initialem [z] (stimmhaft). Von den 20 Versuchspersonen kamen 9 aus Bayern und 11 aus Schleswig-Holstein: head(sz) ## # A tibble: 6 × 3 ## Frikativ Dialekt Vpn ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 z SH S1 ## 2 z SH S2 ## 3 z SH S3 ## 4 z SH S4 ## 5 s SH S5 ## 6 s SH S6 Unsere Frage ist nun: Wird die Stimmhaftigkeit (zwei Stufen: s, z) vom Dialekt (zwei Stufen: BY, SH) beeinflusst? Da beide Variablen in diesem Fall kategorial sind, können wir einen Barplot erstellen, um einen Eindruck von den Daten zu gewinnen: ggplot(sz) + aes(fill = Frikativ, x = Dialekt) + geom_bar(position = &quot;fill&quot;) Es sieht sehr danach aus, dass der initiale Frikativ deutlich häufiger stimmlos in Bayern als in Schleswig-Holstein produziert wird. Nun wenden wir, genau wie zuvor, eine logistische Regression mit anschließendem \\(\\chi^2\\)-Test auf die Daten an: sz.glm &lt;- glm(as.factor(Frikativ) ~ Dialekt, family = binomial, data = sz) anova(sz.glm) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: as.factor(Frikativ) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 19 27.7 ## Dialekt 1 5.3 18 22.4 0.021 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Der \\(\\chi^2\\)-Test zeigt: Die Verteilung von stimmhaftem und stimmlosem /s/ in Worten wie Sonne wurde signifikant vom Dialekt beeinflusst (\\(\\chi^2\\)[1] = 5.3, \\(p\\) &lt; 0.05). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
