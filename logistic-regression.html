<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Logistic Regression | Statistics in R: An Introduction for Phoneticians</title>
  <meta name="description" content="An introduction to statistics in R focussing on linear regressions. This introduction was written as study material for the students of the Institute of Phonetics and Speech Processing at the University of Munich." />
  <meta name="generator" content="bookdown 0.46 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Logistic Regression | Statistics in R: An Introduction for Phoneticians" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introduction to statistics in R focussing on linear regressions. This introduction was written as study material for the students of the Institute of Phonetics and Speech Processing at the University of Munich." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Logistic Regression | Statistics in R: An Introduction for Phoneticians" />
  
  <meta name="twitter:description" content="An introduction to statistics in R focussing on linear regressions. This introduction was written as study material for the students of the Institute of Phonetics and Speech Processing at the University of Munich." />
  

<meta name="author" content="Johanna Cronenberg" />


<meta name="date" content="2025-12-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mixed-linear-regression.html"/>

<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Statistics in R: An Introduction</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="setup.html"><a href="setup.html"><i class="fa fa-check"></i><b>1</b> Setup</a>
<ul>
<li class="chapter" data-level="1.1" data-path="setup.html"><a href="setup.html#installation"><i class="fa fa-check"></i><b>1.1</b> Installation</a></li>
<li class="chapter" data-level="1.2" data-path="setup.html"><a href="setup.html#r-projects"><i class="fa fa-check"></i><b>1.2</b> R Projects</a></li>
<li class="chapter" data-level="1.3" data-path="setup.html"><a href="setup.html#packages-and-r-version"><i class="fa fa-check"></i><b>1.3</b> Packages and R Version</a></li>
<li class="chapter" data-level="1.4" data-path="setup.html"><a href="setup.html#sessions"><i class="fa fa-check"></i><b>1.4</b> Sessions</a></li>
<li class="chapter" data-level="1.5" data-path="setup.html"><a href="setup.html#types-of-documents"><i class="fa fa-check"></i><b>1.5</b> Types of Documents</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="setup.html"><a href="setup.html#r-scripts"><i class="fa fa-check"></i><b>1.5.1</b> R Scripts</a></li>
<li class="chapter" data-level="1.5.2" data-path="setup.html"><a href="setup.html#r-markdown"><i class="fa fa-check"></i><b>1.5.2</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="setup.html"><a href="setup.html#help"><i class="fa fa-check"></i><b>1.6</b> Help</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="setup.html"><a href="setup.html#introduction-to-programming-in-r"><i class="fa fa-check"></i><b>1.6.1</b> Introduction to Programming in R</a></li>
<li class="chapter" data-level="1.6.2" data-path="setup.html"><a href="setup.html#recognizing-errors"><i class="fa fa-check"></i><b>1.6.2</b> Recognizing Errors</a></li>
<li class="chapter" data-level="1.6.3" data-path="setup.html"><a href="setup.html#ask-the-community"><i class="fa fa-check"></i><b>1.6.3</b> Ask the Community</a></li>
<li class="chapter" data-level="1.6.4" data-path="setup.html"><a href="setup.html#help-with-ggplot2"><i class="fa fa-check"></i><b>1.6.4</b> Help with <code>ggplot2</code></a></li>
<li class="chapter" data-level="1.6.5" data-path="setup.html"><a href="setup.html#statistics-in-r-literature"><i class="fa fa-check"></i><b>1.6.5</b> Statistics in R: Literature</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="setup.html"><a href="setup.html#acknowledgements"><i class="fa fa-check"></i><b>1.7</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html"><i class="fa fa-check"></i><b>2</b> Introduction to Inferential Statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#load-packages-and-data"><i class="fa fa-check"></i><b>2.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#basic-terminology"><i class="fa fa-check"></i><b>2.2</b> Basic Terminology</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#normal-distribution"><i class="fa fa-check"></i><b>2.3</b> Normal Distribution</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#testing-for-normal-distribution"><i class="fa fa-check"></i><b>2.3.1</b> Testing for Normal Distribution</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-inferential-statistics.html"><a href="introduction-to-inferential-statistics.html#rule-confidence-intervals"><i class="fa fa-check"></i><b>2.3.2</b> 68–95–99.7 Rule &amp; Confidence Intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#load-packages-and-data-1"><i class="fa fa-check"></i><b>3.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#introduction"><i class="fa fa-check"></i><b>3.2</b> Introduction</a></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#correlation"><i class="fa fa-check"></i><b>3.3</b> Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#the-regression-line"><i class="fa fa-check"></i><b>3.4</b> The Regression Line</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#theoretical-information"><i class="fa fa-check"></i><b>3.4.1</b> Theoretical Information</a></li>
<li class="chapter" data-level="3.4.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-lines-with-ggplot2"><i class="fa fa-check"></i><b>3.4.2</b> Regression Lines with <code>ggplot2</code></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#linear-regression-with-lm"><i class="fa fa-check"></i><b>3.5</b> Linear Regression with <code>lm()</code></a></li>
<li class="chapter" data-level="3.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#residuals"><i class="fa fa-check"></i><b>3.6</b> Residuals</a></li>
<li class="chapter" data-level="3.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#testing-assumptions"><i class="fa fa-check"></i><b>3.7</b> Testing Assumptions</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#normal-distribution-of-residuals"><i class="fa fa-check"></i><b>3.7.1</b> Normal Distribution of Residuals</a></li>
<li class="chapter" data-level="3.7.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#constant-variance-of-the-residuals"><i class="fa fa-check"></i><b>3.7.2</b> Constant Variance of the Residuals</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#understanding-all-results-of-lm"><i class="fa fa-check"></i><b>3.8</b> Understanding all Results of <code>lm()</code></a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#estimated-y-values-and-residuals"><i class="fa fa-check"></i><b>3.8.1</b> Estimated <span class="math inline">\(y\)</span>-Values and Residuals</a></li>
<li class="chapter" data-level="3.8.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-coefficients-and-t-statistic"><i class="fa fa-check"></i><b>3.8.2</b> Regression Coefficients and <span class="math inline">\(t\)</span>-Statistic</a></li>
<li class="chapter" data-level="3.8.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#quality-criteria-for-the-model-and-f-statistic"><i class="fa fa-check"></i><b>3.8.3</b> Quality Criteria for the Model and <span class="math inline">\(F\)</span>-Statistic</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#reporting-the-result"><i class="fa fa-check"></i><b>3.9</b> Reporting the Result</a></li>
<li class="chapter" data-level="3.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#load-packages-and-data-2"><i class="fa fa-check"></i><b>4.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#introduction-1"><i class="fa fa-check"></i><b>4.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#continuous-independent-variables"><i class="fa fa-check"></i><b>4.3</b> Continuous Independent Variables</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#without-interaction"><i class="fa fa-check"></i><b>4.3.1</b> Without Interaction</a></li>
<li class="chapter" data-level="4.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#with-interaction"><i class="fa fa-check"></i><b>4.3.2</b> With Interaction</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#categorical-independent-variables"><i class="fa fa-check"></i><b>4.4</b> Categorical Independent Variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#without-interaction-1"><i class="fa fa-check"></i><b>4.4.1</b> Without Interaction</a></li>
<li class="chapter" data-level="4.4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#with-interaction-1"><i class="fa fa-check"></i><b>4.4.2</b> With Interaction</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#mix-of-continuous-and-categorical-variables"><i class="fa fa-check"></i><b>4.5</b> Mix of Continuous and Categorical Variables</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#without-interaction-2"><i class="fa fa-check"></i><b>4.5.1</b> Without Interaction</a></li>
<li class="chapter" data-level="4.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#with-interaction-2"><i class="fa fa-check"></i><b>4.5.2</b> With Interaction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html"><i class="fa fa-check"></i><b>5</b> Mixed Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#load-packages-and-data-3"><i class="fa fa-check"></i><b>5.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="5.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#mixed-models-lmers-introduction"><i class="fa fa-check"></i><b>5.2</b> Mixed Models (LMERs): Introduction</a></li>
<li class="chapter" data-level="5.3" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-intercepts-vs.-random-slopes"><i class="fa fa-check"></i><b>5.3</b> Random Intercepts vs. Random Slopes</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-intercepts"><i class="fa fa-check"></i><b>5.3.1</b> Random Intercepts</a></li>
<li class="chapter" data-level="5.3.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-slopes"><i class="fa fa-check"></i><b>5.3.2</b> Random Slopes</a></li>
<li class="chapter" data-level="5.3.3" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#determining-the-random-effects-structure-for-word"><i class="fa fa-check"></i><b>5.3.3</b> Determining the Random Effects Structure for <code>word</code></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#lmer-in-r"><i class="fa fa-check"></i><b>5.4</b> LMER in R</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#fixed-effects"><i class="fa fa-check"></i><b>5.4.1</b> Fixed Effects</a></li>
<li class="chapter" data-level="5.4.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#random-effects"><i class="fa fa-check"></i><b>5.4.2</b> Random Effects</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#convergence-problems-and-simplifying-the-model"><i class="fa fa-check"></i><b>5.5</b> Convergence Problems and Simplifying the Model</a></li>
<li class="chapter" data-level="5.6" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#reporting-results"><i class="fa fa-check"></i><b>5.6</b> Reporting Results</a></li>
<li class="chapter" data-level="5.7" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#quality-criteria-for-mixed-models"><i class="fa fa-check"></i><b>5.7</b> Quality Criteria for Mixed Models</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#marginal-and-conditional-r2"><i class="fa fa-check"></i><b>5.7.1</b> Marginal and Conditional <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="5.7.2" data-path="mixed-linear-regression.html"><a href="mixed-linear-regression.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>5.7.2</b> Likelihood Ratio Tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>6</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="logistic-regression.html"><a href="logistic-regression.html#load-packages-and-data-4"><i class="fa fa-check"></i><b>6.1</b> Load Packages and Data</a></li>
<li class="chapter" data-level="6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#from-linear-to-logistic-regression"><i class="fa fa-check"></i><b>6.2</b> From Linear to Logistic Regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#an-example-for-p-q-and-logit"><i class="fa fa-check"></i><b>6.2.1</b> An Example for <span class="math inline">\(P\)</span>, <span class="math inline">\(Q\)</span>, and Logit</a></li>
<li class="chapter" data-level="6.2.2" data-path="logistic-regression.html"><a href="logistic-regression.html#the-logistic-regression-line"><i class="fa fa-check"></i><b>6.2.2</b> The Logistic Regression Line</a></li>
<li class="chapter" data-level="6.2.3" data-path="logistic-regression.html"><a href="logistic-regression.html#regression-with-glm"><i class="fa fa-check"></i><b>6.2.3</b> Regression with <code>glm()</code></a></li>
<li class="chapter" data-level="6.2.4" data-path="logistic-regression.html"><a href="logistic-regression.html#the-chi2-test-reporting-results"><i class="fa fa-check"></i><b>6.2.4</b> The <span class="math inline">\(\chi^2\)</span>-Test &amp; Reporting Results</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="logistic-regression.html"><a href="logistic-regression.html#the-sigmoid-function-and-tipping-point"><i class="fa fa-check"></i><b>6.3</b> The Sigmoid Function and Tipping Point</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#the-tipping-point"><i class="fa fa-check"></i><b>6.3.1</b> The Tipping Point</a></li>
<li class="chapter" data-level="6.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#plotting-proportions"><i class="fa fa-check"></i><b>6.3.2</b> Plotting Proportions</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="logistic-regression.html"><a href="logistic-regression.html#tipping-points-in-perceptual-studies"><i class="fa fa-check"></i><b>6.4</b> Tipping Points in Perceptual Studies</a></li>
<li class="chapter" data-level="6.5" data-path="logistic-regression.html"><a href="logistic-regression.html#categorical-independent-factor"><i class="fa fa-check"></i><b>6.5</b> Categorical Independent Factor</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
			<a class="btn pull-right js-toolbar-action" href="logistische-regression.html"><i class="fa fa-language"></i></a>
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistics in R: An Introduction for Phoneticians</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Logistic Regression<a href="logistic-regression.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="load-packages-and-data-4" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Load Packages and Data<a href="logistic-regression.html#load-packages-and-data-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Please load the following packages and data frames:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="logistic-regression.html#cb339-1" tabindex="-1"></a><span class="fu">library</span>(magrittr)</span>
<span id="cb339-2"><a href="logistic-regression.html#cb339-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb339-3"><a href="logistic-regression.html#cb339-3" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">&quot;http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf&quot;</span></span>
<span id="cb339-4"><a href="logistic-regression.html#cb339-4" tabindex="-1"></a>ovowel <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="fu">file.path</span>(url, <span class="st">&quot;ovokal.txt&quot;</span>)) <span class="sc">%&gt;%</span> <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb339-5"><a href="logistic-regression.html#cb339-5" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">vowel =</span> Vokal, <span class="at">year =</span> Jahr, <span class="at">subject =</span> Vpn) <span class="sc">%&gt;%</span> </span>
<span id="cb339-6"><a href="logistic-regression.html#cb339-6" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">vowel =</span> <span class="fu">ifelse</span>(vowel <span class="sc">==</span> <span class="st">&quot;hoch&quot;</span>, <span class="st">&quot;high&quot;</span>, <span class="st">&quot;low&quot;</span>))</span>
<span id="cb339-7"><a href="logistic-regression.html#cb339-7" tabindex="-1"></a>pvp <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="fu">file.path</span>(url, <span class="st">&quot;pvp.txt&quot;</span>)) <span class="sc">%&gt;%</span> <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb339-8"><a href="logistic-regression.html#cb339-8" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">response =</span> Urteil)</span>
<span id="cb339-9"><a href="logistic-regression.html#cb339-9" tabindex="-1"></a>sz <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="fu">file.path</span>(url, <span class="st">&quot;sz.txt&quot;</span>)) <span class="sc">%&gt;%</span> <span class="fu">as_tibble</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb339-10"><a href="logistic-regression.html#cb339-10" tabindex="-1"></a>  <span class="fu">rename</span>(<span class="at">fricative =</span> Frikativ, <span class="at">dialect =</span> Dialekt)</span></code></pre></div>
</div>
<div id="from-linear-to-logistic-regression" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> From Linear to Logistic Regression<a href="logistic-regression.html#from-linear-to-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression (just like linear regression) is a statistical test that examines whether a dependent variable is influenced by an independent factor. In contrast to the linear regression, the dependent variable in a logistic regression is always <strong>categorical and binary</strong>, while the independent variable can be <strong>either numeric (continuous) or categorical</strong>. Logistic regression allows us to estimate the probability of a particular value, assuming a relationship exists between the dependent and independent variables.</p>
<p>Examples:</p>
<ul>
<li>To what extent is the vocalization of a final /l/ in English (<em>feel</em> vs. ‘<em>feeu</em>’) influenced by dialect?
<ul>
<li>Dependent variable: Vocalization (categorical with two levels: yes, no)</li>
<li>Independent variable: Dialect (categorical with two or more levels)</li>
</ul></li>
<li>Is “passt” more likely to be produced with /ʃ/ in Augsburg compared to Munich?
<ul>
<li>Dependent variable: Fricative (categorical with two levels: /s/, /ʃ/)</li>
<li>Independent variable: Dialect (categorical with two levels: Augsburg, Munich)</li>
</ul></li>
<li>The vowel /a/ in /lam/ is synthesized with different durations and played back to listeners. Do the participants hear “lahm” (long /a:/) more often than “Lamm” (short /a/) as the vowel duration increases?
<ul>
<li>Dependent variable: Vowel (categorical with two levels: /a/, /a:/)</li>
<li>Independent variable: Duration (continuous)</li>
</ul></li>
</ul>
<p>Since the dependent variable in logistic regression is always a factor with two levels, these levels can also be coded as 1 and 0, and we can ask what the probability <span class="math inline">\(P\)</span> is that the dependent variable <span class="math inline">\(y\)</span> takes the value 1 based on the given data: <span class="math inline">\(P(y = 1)\)</span>. Similarly, we can ask for the probability <span class="math inline">\(Q\)</span> that <span class="math inline">\(y\)</span> takes the value 0: <span class="math inline">\(1 - P(y = 1)\)</span>. For the third example above, this would mean the following:</p>
<ul>
<li><span class="math inline">\(P\)</span>: Probability that subjects hear “lahm” with increasing vowel duration (“success,” because based on our knowledge or previous findings, e.g., from other experiments, we assume that subjects should hear “lahm” with increasing vowel duration)</li>
<li><span class="math inline">\(Q\)</span>: Probability that subjects hear “Lamm” with increasing vowel duration (“failure,” because again, based on our previous knowledge of this phenomenon, we assume that it would be strange if subjects heard “Lamm” with increasing vowel duration)</li>
</ul>
<p>The division (ratio) of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is called the <strong>odds</strong> (probability of winning):</p>
<p><span class="math inline">\(Odds = \frac{P(y = 1)}{1 - P(y = 1)} = \frac{P}{Q}\)</span></p>
<p>The odds of winning always lie within a range of 0 to infinity. One might consider simply using the odds as the dependent variable in a linear regression, since it’s no longer a categorical, binary dependent variable. The problem is that <code>lm()</code> doesn’t know that the odds can only take values from zero to infinity and would therefore also predict values outside this range. Furthermore, the ratio of <span class="math inline">\(P\)</span> to <span class="math inline">\(Q\)</span> says nothing about how many observations were used in calculating this ratio (the more observations, the more meaningful the calculated odds). So we need a function that transforms the odds into something that, firstly, falls within the range <span class="math inline">\(\pm\)</span>infinity and, secondly, weights the proportions based on the number of observations. This function is generally called a <strong>link function</strong> and, in the case of logistic regression, is the <strong>logit transformation of the odds</strong>. The logit is the logarithm of the odds of winning and is therefore also referred to as <strong>log odds</strong>:</p>
<p><span class="math inline">\(log(\frac{P}{Q})\)</span></p>
<div id="an-example-for-p-q-and-logit" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> An Example for <span class="math inline">\(P\)</span>, <span class="math inline">\(Q\)</span>, and Logit<a href="logistic-regression.html#an-example-for-p-q-and-logit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Between 1950 and 2005, words like <em>lost</em> in an aristocratic form of English (<em>Received Pronunciation</em>) were increasingly produced low vowel /lɔst/ instead of with the high vowel /lost/. We have data in the <code>ovowel</code> data frame to support this hypothesis:</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="logistic-regression.html#cb340-1" tabindex="-1"></a><span class="fu">head</span>(ovowel)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##    year vowel subject
##   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  
## 1  1950 high  S1     
## 2  1950 high  S2     
## 3  1950 high  S3     
## 4  1950 high  S4     
## 5  1950 high  S5     
## 6  1950 high  S6</code></pre>
<p>Our research question is: <em>Is the pronunciation of the vowel (high vs. low = dependent variable) influenced by the year (1950…2005 = independent numerical variable)?</em></p>
<p>We want to calculate <span class="math inline">\(P\)</span> (the probability that the vowel was low) and <span class="math inline">\(Q\)</span> (the probability that the vowel was high) for each year. Based on our current understanding, the direction of change in the pronunciation of the vowel is from high to low, so we define it as “success” if the vowel was produced low and as “failure” if it was produced high. As a first step in calculating <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, we code low and high pronunciation as 1 and 0, respectively, as <code>TRUE</code> and <code>FALSE</code>:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="logistic-regression.html#cb342-1" tabindex="-1"></a>ovowel <span class="sc">%&lt;&gt;%</span> <span class="fu">mutate</span>(<span class="at">success =</span> <span class="fu">ifelse</span>(vowel <span class="sc">==</span> <span class="st">&quot;low&quot;</span>, <span class="cn">TRUE</span>, <span class="cn">FALSE</span>),</span>
<span id="cb342-2"><a href="logistic-regression.html#cb342-2" tabindex="-1"></a>                   <span class="at">failure =</span> <span class="sc">!</span>success)</span>
<span id="cb342-3"><a href="logistic-regression.html#cb342-3" tabindex="-1"></a><span class="fu">head</span>(ovowel)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 5
##    year vowel subject success failure
##   &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;lgl&gt;   &lt;lgl&gt;  
## 1  1950 high  S1      FALSE   TRUE   
## 2  1950 high  S2      FALSE   TRUE   
## 3  1950 high  S3      FALSE   TRUE   
## 4  1950 high  S4      FALSE   TRUE   
## 5  1950 high  S5      FALSE   TRUE   
## 6  1950 high  S6      FALSE   TRUE</code></pre>
<p>Then we take the first year, 1950, and calculate <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> by counting how many “successes” and “failures” we have for that year:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="logistic-regression.html#cb344-1" tabindex="-1"></a>P <span class="ot">&lt;-</span> ovowel <span class="sc">%&gt;%</span></span>
<span id="cb344-2"><a href="logistic-regression.html#cb344-2" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">1950</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb344-3"><a href="logistic-regression.html#cb344-3" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">P =</span> <span class="fu">sum</span>(success)) <span class="sc">%&gt;%</span> </span>
<span id="cb344-4"><a href="logistic-regression.html#cb344-4" tabindex="-1"></a>  <span class="fu">pull</span>(P)</span>
<span id="cb344-5"><a href="logistic-regression.html#cb344-5" tabindex="-1"></a>P</span></code></pre></div>
<pre><code>## [1] 5</code></pre>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="logistic-regression.html#cb346-1" tabindex="-1"></a>Q <span class="ot">&lt;-</span> ovowel <span class="sc">%&gt;%</span></span>
<span id="cb346-2"><a href="logistic-regression.html#cb346-2" tabindex="-1"></a>  <span class="fu">filter</span>(year <span class="sc">==</span> <span class="dv">1950</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb346-3"><a href="logistic-regression.html#cb346-3" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">Q =</span> <span class="fu">sum</span>(failure)) <span class="sc">%&gt;%</span> </span>
<span id="cb346-4"><a href="logistic-regression.html#cb346-4" tabindex="-1"></a>  <span class="fu">pull</span>(Q)</span>
<span id="cb346-5"><a href="logistic-regression.html#cb346-5" tabindex="-1"></a>Q</span></code></pre></div>
<pre><code>## [1] 30</code></pre>
<p>This means that in 1950, the vowel /o/ in words like <em>lost</em> was only produced 5 times as a low vowel, but 30 times as a high vowel. We would have to do this for every level of the independent variable (for every year)… But that would be very cumbersome. So we group the data frame by year:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="logistic-regression.html#cb348-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> ovowel <span class="sc">%&gt;%</span></span>
<span id="cb348-2"><a href="logistic-regression.html#cb348-2" tabindex="-1"></a>  <span class="fu">group_by</span>(year) <span class="sc">%&gt;%</span></span>
<span id="cb348-3"><a href="logistic-regression.html#cb348-3" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">P =</span> <span class="fu">sum</span>(success), <span class="at">Q =</span> <span class="fu">sum</span>(failure))</span>
<span id="cb348-4"><a href="logistic-regression.html#cb348-4" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##    year     P     Q
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1  1950     5    30
## 2  1960    21    18
## 3  1971    26    15
## 4  1980    20    13
## 5  1993    32     4
## 6  2005    34     2</code></pre>
<p>Using <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> we can now calculate the log odds (the logit):</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="logistic-regression.html#cb350-1" tabindex="-1"></a>df<span class="sc">$</span>log_odds <span class="ot">&lt;-</span> <span class="fu">log</span>(df<span class="sc">$</span>P<span class="sc">/</span>df<span class="sc">$</span>Q)</span>
<span id="cb350-2"><a href="logistic-regression.html#cb350-2" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>## # A tibble: 6 × 4
##    year     P     Q log_odds
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;
## 1  1950     5    30   -1.79 
## 2  1960    21    18    0.154
## 3  1971    26    15    0.550
## 4  1980    20    13    0.431
## 5  1993    32     4    2.08 
## 6  2005    34     2    2.83</code></pre>
<p>Let’s look at the distribution of log odds over the decades:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="logistic-regression.html#cb352-1" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb352-2"><a href="logistic-regression.html#cb352-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> year, <span class="at">y =</span> log_odds) <span class="sc">+</span> </span>
<span id="cb352-3"><a href="logistic-regression.html#cb352-3" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-142-1.svg" width="672" /></p>
<p>It is these log odds that we will use to construct a regression line using logistic regression. This regression line is defined in the same way as the linear regression line, but it estimates the log odds:</p>
<p><span class="math inline">\(log(\frac{P}{Q}) = bx + k\)</span></p>
<p>Once again,</p>
<ul>
<li><span class="math inline">\(b\)</span> is the slope</li>
<li><span class="math inline">\(x\)</span> is a value on the x-axis</li>
<li><span class="math inline">\(k\)</span> is the y-intercept</li>
</ul>
<p>Here, however, we cannot calculate <span class="math inline">\(b\)</span> and <span class="math inline">\(k\)</span> as easily as in linear regression; that is, we have them estimated directly.</p>
</div>
<div id="the-logistic-regression-line" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> The Logistic Regression Line<a href="logistic-regression.html#the-logistic-regression-line" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For linear regression, we estimated the regression coefficients using the <code>lm()</code> function, which employs the least squares method. The <strong>logistic regression line</strong>, on the other hand, is approximated using the <strong>maximum likelihood</strong> method, which ensures that the estimated data points of the logistic model are as close as possible to the actual values. To estimate the regression coefficients, we use the function <code>glm()</code>, which stands for <strong>Generalized Linear Model</strong>. In addition to the formula <code>y ~ x</code> and the data frame, the function receives the argument <code>family = binomial</code>, which tells the function to perform the logit transformation. The dependent variable must be a factor; if necessary, you must convert the variable into a factor using <code>as.factor()</code>:</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="logistic-regression.html#cb353-1" tabindex="-1"></a><span class="fu">class</span>(ovowel<span class="sc">$</span>vowel) <span class="co"># not a factor</span></span></code></pre></div>
<pre><code>## [1] &quot;character&quot;</code></pre>
<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb355-1"><a href="logistic-regression.html#cb355-1" tabindex="-1"></a>lreg <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">as.factor</span>(vowel) <span class="sc">~</span> year, <span class="at">family =</span> binomial, <span class="at">data =</span> ovowel)</span></code></pre></div>
<p>We’ll look at the summary of this model later. First, we’ll show an alternative to the above application of <code>glm()</code> to the original data frame. <code>glm()</code> can also be executed on <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> from the combined data frame <code>df</code> by concatenating <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> using <code>cbind()</code> and using them as dependent variables:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="logistic-regression.html#cb356-1" tabindex="-1"></a>lreg2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">cbind</span>(P, Q) <span class="sc">~</span> year, <span class="at">family =</span> binomial, <span class="at">data =</span> df)</span></code></pre></div>
<p>We can use <code>coef()</code> to display the regression coefficients:</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="logistic-regression.html#cb357-1" tabindex="-1"></a>coefs <span class="ot">&lt;-</span> <span class="fu">coef</span>(lreg)</span>
<span id="cb357-2"><a href="logistic-regression.html#cb357-2" tabindex="-1"></a>coefs</span></code></pre></div>
<pre><code>## (Intercept)        year 
##  -138.11742     0.07026</code></pre>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="logistic-regression.html#cb359-1" tabindex="-1"></a><span class="co"># or with $coefficients</span></span>
<span id="cb359-2"><a href="logistic-regression.html#cb359-2" tabindex="-1"></a>lreg<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>## (Intercept)        year 
##  -138.11742     0.07026</code></pre>
<p>Using these parameters, the straight regression line can be superimposed onto the data in the logit space with either <code>geom_smooth()</code> (which receives <code>method = "glm"</code> as an argument) or <code>geom_abline()</code> (with the estimated <code>coefs</code>).</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="logistic-regression.html#cb361-1" tabindex="-1"></a><span class="co"># with geom_smooth():</span></span>
<span id="cb361-2"><a href="logistic-regression.html#cb361-2" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb361-3"><a href="logistic-regression.html#cb361-3" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> year, <span class="at">y =</span> log_odds) <span class="sc">+</span> </span>
<span id="cb361-4"><a href="logistic-regression.html#cb361-4" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb361-5"><a href="logistic-regression.html#cb361-5" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="at">se =</span> F)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-146-1.svg" width="672" /></p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="logistic-regression.html#cb363-1" tabindex="-1"></a><span class="co"># with geom_abline():</span></span>
<span id="cb363-2"><a href="logistic-regression.html#cb363-2" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb363-3"><a href="logistic-regression.html#cb363-3" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> year, <span class="at">y =</span> log_odds) <span class="sc">+</span> </span>
<span id="cb363-4"><a href="logistic-regression.html#cb363-4" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb363-5"><a href="logistic-regression.html#cb363-5" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> coefs[<span class="dv">1</span>], <span class="at">slope =</span> coefs[<span class="dv">2</span>], <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-146-2.svg" width="672" /></p>
<p>The values estimated by logistic regression are the <em>log odds</em>. We can again use the <code>predict()</code> function to display the estimated <em>log odds</em>:</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="logistic-regression.html#cb364-1" tabindex="-1"></a>log_odds_estimate <span class="ot">&lt;-</span> <span class="fu">predict</span>(lreg)</span>
<span id="cb364-2"><a href="logistic-regression.html#cb364-2" tabindex="-1"></a>log_odds_estimate</span></code></pre></div>
<pre><code>##       1       2       3       4       5       6 
## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 
##       7       8       9      10      11      12 
## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 
##      13      14      15      16      17      18 
## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 
##      19      20      21      22      23      24 
## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 
##      25      26      27      28      29      30 
## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 
##      31      32      33      34      35      36 
## -1.1043 -1.1043 -1.1043 -1.1043 -1.1043 -0.4017 
##      37      38      39      40      41      42 
## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 
##      43      44      45      46      47      48 
## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 
##      49      50      51      52      53      54 
## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 
##      55      56      57      58      59      60 
## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 
##      61      62      63      64      65      66 
## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 
##      67      68      69      70      71      72 
## -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 -0.4017 
##      73      74      75      76      77      78 
## -0.4017 -0.4017  0.3712  0.3712  0.3712  0.3712 
##      79      80      81      82      83      84 
##  0.3712  0.3712  0.3712  0.3712  0.3712  0.3712 
##      85      86      87      88      89      90 
##  0.3712  0.3712  0.3712  0.3712  0.3712  0.3712 
##      91      92      93      94      95      96 
##  0.3712  0.3712  0.3712  0.3712  0.3712  0.3712 
##      97      98      99     100     101     102 
##  0.3712  0.3712  0.3712  0.3712  0.3712  0.3712 
##     103     104     105     106     107     108 
##  0.3712  0.3712  0.3712  0.3712  0.3712  0.3712 
##     109     110     111     112     113     114 
##  0.3712  0.3712  0.3712  0.3712  0.3712  0.3712 
##     115     116     117     118     119     120 
##  0.3712  1.0036  1.0036  1.0036  1.0036  1.0036 
##     121     122     123     124     125     126 
##  1.0036  1.0036  1.0036  1.0036  1.0036  1.0036 
##     127     128     129     130     131     132 
##  1.0036  1.0036  1.0036  1.0036  1.0036  1.0036 
##     133     134     135     136     137     138 
##  1.0036  1.0036  1.0036  1.0036  1.0036  1.0036 
##     139     140     141     142     143     144 
##  1.0036  1.0036  1.0036  1.0036  1.0036  1.0036 
##     145     146     147     148     149     150 
##  1.0036  1.0036  1.0036  1.0036  1.9170  1.9170 
##     151     152     153     154     155     156 
##  1.9170  1.9170  1.9170  1.9170  1.9170  1.9170 
##     157     158     159     160     161     162 
##  1.9170  1.9170  1.9170  1.9170  1.9170  1.9170 
##     163     164     165     166     167     168 
##  1.9170  1.9170  1.9170  1.9170  1.9170  1.9170 
##     169     170     171     172     173     174 
##  1.9170  1.9170  1.9170  1.9170  1.9170  1.9170 
##     175     176     177     178     179     180 
##  1.9170  1.9170  1.9170  1.9170  1.9170  1.9170 
##     181     182     183     184     185     186 
##  1.9170  1.9170  1.9170  1.9170  2.7601  2.7601 
##     187     188     189     190     191     192 
##  2.7601  2.7601  2.7601  2.7601  2.7601  2.7601 
##     193     194     195     196     197     198 
##  2.7601  2.7601  2.7601  2.7601  2.7601  2.7601 
##     199     200     201     202     203     204 
##  2.7601  2.7601  2.7601  2.7601  2.7601  2.7601 
##     205     206     207     208     209     210 
##  2.7601  2.7601  2.7601  2.7601  2.7601  2.7601 
##     211     212     213     214     215     216 
##  2.7601  2.7601  2.7601  2.7601  2.7601  2.7601 
##     217     218     219     220 
##  2.7601  2.7601  2.7601  2.7601</code></pre>
<p>The output of <code>predict()</code> in this case consists of 220 numbers, one number per row in the original data frame <code>ovowel</code>. As you can see, the estimated log odds are repeated. This is because one log odd value is calculated for each level (or value) of the independent variable; in this case, there are six unique log odd values, one for each year:</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="logistic-regression.html#cb366-1" tabindex="-1"></a><span class="fu">unique</span>(log_odds_estimate)</span></code></pre></div>
<pre><code>## [1] -1.1043 -0.4017  0.3712  1.0036  1.9170  2.7601</code></pre>
<p>We can plot these predicted values in red on our plot from above and find that the predicted values lie exactly on the regression line (we use <code>geom_abline()</code> here):</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="logistic-regression.html#cb368-1" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb368-2"><a href="logistic-regression.html#cb368-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> year, <span class="at">y =</span> log_odds) <span class="sc">+</span> </span>
<span id="cb368-3"><a href="logistic-regression.html#cb368-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb368-4"><a href="logistic-regression.html#cb368-4" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> coefs[<span class="dv">1</span>], <span class="at">slope =</span> coefs[<span class="dv">2</span>], <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb368-5"><a href="logistic-regression.html#cb368-5" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">unique</span>(ovowel<span class="sc">$</span>year), <span class="at">y =</span> <span class="fu">unique</span>(log_odds_estimate)),</span>
<span id="cb368-6"><a href="logistic-regression.html#cb368-6" tabindex="-1"></a>             <span class="at">mapping =</span> <span class="fu">aes</span>(x, y),</span>
<span id="cb368-7"><a href="logistic-regression.html#cb368-7" tabindex="-1"></a>             <span class="at">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-149-1.svg" width="672" /></p>
<p>Just like with linear regression, we can also use <code>predict()</code> to predict the <em>log odds</em> values for x-values that are not present in the original dataset. For example, if we want to estimate the logit values for the years 2000 to 2020, it works as follows:</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="logistic-regression.html#cb369-1" tabindex="-1"></a><span class="fu">predict</span>(lreg, <span class="fu">data.frame</span>(<span class="at">year =</span> <span class="dv">2000</span><span class="sc">:</span><span class="dv">2020</span>))</span></code></pre></div>
<pre><code>##     1     2     3     4     5     6     7     8     9 
## 2.409 2.479 2.549 2.620 2.690 2.760 2.830 2.901 2.971 
##    10    11    12    13    14    15    16    17    18 
## 3.041 3.111 3.182 3.252 3.322 3.393 3.463 3.533 3.603 
##    19    20    21 
## 3.674 3.744 3.814</code></pre>
</div>
<div id="regression-with-glm" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Regression with <code>glm()</code><a href="logistic-regression.html#regression-with-glm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The result of the function <code>glm()</code> is an object of the classes “glm” and “lm”:</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="logistic-regression.html#cb371-1" tabindex="-1"></a><span class="fu">class</span>(lreg)</span></code></pre></div>
<pre><code>## [1] &quot;glm&quot; &quot;lm&quot;</code></pre>
<p>We apply the <code>summary()</code> function to the result of the logistic regression <code>lreg</code> and look at the result again line by line:</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="logistic-regression.html#cb373-1" tabindex="-1"></a><span class="fu">summary</span>(lreg)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = as.factor(vowel) ~ year, family = binomial, data = ovowel)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -138.1174    20.9988   -6.58  4.8e-11 ***
## year           0.0703     0.0107    6.59  4.3e-11 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 290.57  on 219  degrees of freedom
## Residual deviance: 229.45  on 218  degrees of freedom
## AIC: 233.5
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The summary begins again with the <em>call</em>, i.e., the function that was used.</p>
<div id="coefficients" class="section level4 hasAnchor" number="6.2.3.1">
<h4><span class="header-section-number">6.2.3.1</span> Coefficients<a href="logistic-regression.html#coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The table of regression coefficients follows:</p>
<p><img src="img/glm1.png" style="width:60.0%" /></p>
<p>The first row of this table contains the values for the intercept, the second for slope. The first column again shows the estimates for the regression coefficients, which were determined using a maximum likelihood method. The second column shows the standard error, which describes how reliable the estimates are (the smaller the better). A <strong>Wald test</strong> was performed on the two estimates, which checks whether the estimates differ significantly from zero. The result of this test is the <strong><span class="math inline">\(z\)</span>-value</strong>, which can also be calculated by dividing the estimate by the standard error. We are particularly interested in the second row, whose <span class="math inline">\(z\)</span>-value and <span class="math inline">\(p\)</span>-value show whether the independent variable <code>year</code> contributes significantly to explaining the log odds values. If the <span class="math inline">\(p\)</span>-value, which is in the fourth column, is less than 0.05 (see also the significance level asterisks), then the coefficient differs significantly from zero. In the case of the dependent variable, we see that the <span class="math inline">\(p\)</span>-value is less than 0.001, i.e., the variable is a good predictor for the log odds.</p>
<p>By default, a statement about the <strong>dispersion parameter</strong> is printed after the coefficients table. We can ignore this.</p>
</div>
<div id="deviances-and-aic" class="section level4 hasAnchor" number="6.2.3.2">
<h4><span class="header-section-number">6.2.3.2</span> Deviances and AIC<a href="logistic-regression.html#deviances-and-aic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The following two lines contain the <strong>null deviance</strong> and the <strong>residual deviance</strong> as well as the <strong>AIC</strong> (<em>Akaike Information Criterion</em>):</p>
<p><img src="img/glm2.png" style="width:60.0%" /></p>
<p>The null deviance describes how well a model without independent variables would explain the data. A model without independent variables is characterized solely by the intercept. On its own, the null deviance is difficult to interpret. Therefore, the residual deviance is listed directly below it, describing how well our full model explains the data. The difference between null and residual deviance reveals how helpful our independent variable is in the model. The degrees of freedom are calculated by subtracting the number of parameters in the model from the number of observations in the data frame. With null deviance, there is only one parameter (the intercept), while with residual deviance, there are two (the intercept and the independent variable). The smaller the deviations (i.e., the discrepancies between the actual and the estimated values), the better.</p>
<p><strong>AIC</strong> stands for <strong>Akaike Information Criterion</strong> and is particularly helpful when comparing different regression models for the same dataset (for example, if you were to calculate another model with more than one independent variable for <code>ovowel</code>). The smaller the AIC, the better the model describes the variance in the data. Since we only have the one model here, AIC is irrelevant for our purposes.</p>
</div>
<div id="fisher-scoring-iterations" class="section level4 hasAnchor" number="6.2.3.3">
<h4><span class="header-section-number">6.2.3.3</span> Fisher Scoring Iterations<a href="logistic-regression.html#fisher-scoring-iterations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In logistic regression, an iterative algorithm calculates the regression parameters, and the Fisher scoring iterations indicate how many iterations were required. This is also irrelevant for our purposes.</p>
<p><img src="img/glm3.png" style="width:60.0%" /></p>
</div>
<div id="deviance-residuals" class="section level4 hasAnchor" number="6.2.3.4">
<h4><span class="header-section-number">6.2.3.4</span> Deviance Residuals<a href="logistic-regression.html#deviance-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <strong>deviance residuals</strong> are the differences between the empirically observed and estimated values. They are not shown in the result of the logistic regression, but we can look at them using <code>resid()</code> and compute the usual summary statistics for them with <code>summary()</code>:</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="logistic-regression.html#cb375-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">resid</span>(lreg))</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -2.3755 -0.7566  0.3503  0.0586  0.7903  1.6677</code></pre>
</div>
</div>
<div id="the-chi2-test-reporting-results" class="section level3 hasAnchor" number="6.2.4">
<h3><span class="header-section-number">6.2.4</span> The <span class="math inline">\(\chi^2\)</span>-Test &amp; Reporting Results<a href="logistic-regression.html#the-chi2-test-reporting-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the linear regression, the <span class="math inline">\(F\)</span>-test was our test statistic. Instead, we perform a <strong><span class="math inline">\(\chi^2\)</span>-test</strong> for the logistic regression, which checks whether our model is a better fit for our data than an intercept-only model. You already know this test from the mixed models; it compares to models to each other. Since we already saw in the coefficients table above that the Wald test was significant for our independent variable, it is likely that our full model is better than a model without independent variables. We once again use the <code>anova()</code> function for this:</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="logistic-regression.html#cb377-1" tabindex="-1"></a><span class="fu">anova</span>(lreg)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: as.factor(vowel)
## 
## Terms added sequentially (first to last)
## 
## 
##      Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
## NULL                   219        291             
## year  1     61.1       218        230  5.4e-15 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The result of the <span class="math inline">\(\chi^2\)</span>-test has two rows: one for the null model (intercept-only) and the other for the model with the independent variable <code>year</code>. The degrees of freedom for null and residual deviation, which are reported in the column <code>Resid. Dev.</code>, are found in the column <code>Resid. Df</code>. These values were reported in abbreviated form in the summary of the logistic model. From the <span class="math inline">\(\chi^2\)</span>-test result, we are particularly interested in the value in the column <code>Pr(&gt;Chi)</code>, which contains the <span class="math inline">\(p\)</span>-value. If this value is less than 0.05 (see also the significance asterisks), then the model with the variable <code>year</code> is a better fit for the data than the null model.</p>
<p>Our initial research question was: <em>Is the pronunciation of the vowel (high vs. low) influenced by the year?</em></p>
<p>We can now report: <em>Year had a significant influence on the proportion of ‘lost’ with low/high vowel (<span class="math inline">\(\chi^2\)</span>[1] = 61.1, <span class="math inline">\(p\)</span> &lt; 0.001).</em></p>
</div>
</div>
<div id="the-sigmoid-function-and-tipping-point" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> The Sigmoid Function and Tipping Point<a href="logistic-regression.html#the-sigmoid-function-and-tipping-point" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We presented the results of a logistic regression above in the logit space. However, one can also use the y-intercept and the slope to plot <strong>proportions</strong> on the y-axis instead of log odds. In this case, the regression line is no longer straight, but <strong>sigmoidal</strong> (S-shaped). The formula for the sigmoid function is:</p>
<p><span class="math inline">\(f(x) = \frac{e^{bx+k}}{1 + e^{bx+k}}\)</span></p>
<p>In this formula, <span class="math inline">\(e\)</span> is the exponential function, <span class="math inline">\(b\)</span> and <span class="math inline">\(k\)</span> are the slope and the intercept, respectively. The larger the slope <span class="math inline">\(b\)</span> is (in the figure: 1, 0.5, 0.25), the steeper the sigmoid curve (black, red, green) becomes:</p>
<p><img src="img/sigmoid_b.png" /></p>
<p>When the slope is zero, the y-axis is a straight line with a y-value of 0.5. Changing the y-intercept <span class="math inline">\(k\)</span> (0, 1, -1 in the plot) when the slope is <span class="math inline">\(b = 0\)</span> shifts the straight horizontal line up or down (black, red, green):</p>
<p><img src="img/sigmoid_k.png" /></p>
<div id="the-tipping-point" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> The Tipping Point<a href="logistic-regression.html#the-tipping-point" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>tipping point</strong> is the point at which the sigmoid curve is <strong>steepest</strong>. At this point, the value on the y-axis (the proportion) is always 0.5 (shown below as a horizontal line). The x-value of the tipping point is calculated using <span class="math inline">\(\frac{-k}{b}\)</span>. For <span class="math inline">\(k = 4\)</span> and <span class="math inline">\(b = 0.8\)</span>, for example, this would be <span class="math inline">\(-4/0.8 = -5\)</span> (dashed line):</p>
<p><img src="img/sigmoid.png" /></p>
</div>
<div id="plotting-proportions" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Plotting Proportions<a href="logistic-regression.html#plotting-proportions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For our example above, we now want to plot proportions and then fit a sigmoidal regression curve to our data. We take our aggregated data frame <code>df</code> and calculate the proportion of <span class="math inline">\(P\)</span> (the proportion of “successes”) per year:</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="logistic-regression.html#cb379-1" tabindex="-1"></a>df<span class="sc">$</span>proportions <span class="ot">&lt;-</span> df<span class="sc">$</span>P <span class="sc">/</span> (df<span class="sc">$</span>P <span class="sc">+</span> df<span class="sc">$</span>Q)</span>
<span id="cb379-2"><a href="logistic-regression.html#cb379-2" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>## # A tibble: 6 × 5
##    year     P     Q log_odds proportions
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;       &lt;dbl&gt;
## 1  1950     5    30   -1.79        0.143
## 2  1960    21    18    0.154       0.538
## 3  1971    26    15    0.550       0.634
## 4  1980    20    13    0.431       0.606
## 5  1993    32     4    2.08        0.889
## 6  2005    34     2    2.83        0.944</code></pre>
<p>For the year 1950, the proportion of “successes” (where the vowel /o/ was produced low) is 14.3%, for the year 1960 it is already 53.8%, and so on. We can now plot these proportions stored in the newly created column <code>df$proportions</code> and then use <code>geom_smooth()</code> to draw a sigmoidal regression line through the data. To this end, we use the arguments <code>method = "glm"</code> (<em>generalized linear model</em>), <code>se = F</code> (don’t show standard error), and additionally <code>method.args = list(family = "quasibinomial")</code> so that the function knows that we’re plotting proportions.</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="logistic-regression.html#cb381-1" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb381-2"><a href="logistic-regression.html#cb381-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> year, <span class="at">y =</span> proportions) <span class="sc">+</span> </span>
<span id="cb381-3"><a href="logistic-regression.html#cb381-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb381-4"><a href="logistic-regression.html#cb381-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="at">se =</span> F,</span>
<span id="cb381-5"><a href="logistic-regression.html#cb381-5" tabindex="-1"></a>              <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">family =</span> <span class="st">&quot;quasibinomial&quot;</span>))</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-156-1.svg" width="672" /></p>
<p>In this plot, the complete “S” of the sigmoidal curve is not visible because our x-axis is limited. However, we can easily calculate further proportion values using <code>predict()</code>. As we saw earlier, <code>predict()</code> returns the log odds, not the proportions. We obtain the proportions by using the argument <code>type = "response"</code> in <code>predict()</code>:</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="logistic-regression.html#cb383-1" tabindex="-1"></a><span class="fu">predict</span>(lreg, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##      1      2      3      4      5      6      7 
## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 
##      8      9     10     11     12     13     14 
## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 
##     15     16     17     18     19     20     21 
## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 
##     22     23     24     25     26     27     28 
## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 
##     29     30     31     32     33     34     35 
## 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 0.2489 
##     36     37     38     39     40     41     42 
## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 
##     43     44     45     46     47     48     49 
## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 
##     50     51     52     53     54     55     56 
## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 
##     57     58     59     60     61     62     63 
## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 
##     64     65     66     67     68     69     70 
## 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 0.4009 
##     71     72     73     74     75     76     77 
## 0.4009 0.4009 0.4009 0.4009 0.5917 0.5917 0.5917 
##     78     79     80     81     82     83     84 
## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 
##     85     86     87     88     89     90     91 
## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 
##     92     93     94     95     96     97     98 
## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 
##     99    100    101    102    103    104    105 
## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 
##    106    107    108    109    110    111    112 
## 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 0.5917 
##    113    114    115    116    117    118    119 
## 0.5917 0.5917 0.5917 0.7318 0.7318 0.7318 0.7318 
##    120    121    122    123    124    125    126 
## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 
##    127    128    129    130    131    132    133 
## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 
##    134    135    136    137    138    139    140 
## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 
##    141    142    143    144    145    146    147 
## 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 0.7318 
##    148    149    150    151    152    153    154 
## 0.7318 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 
##    155    156    157    158    159    160    161 
## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 
##    162    163    164    165    166    167    168 
## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 
##    169    170    171    172    173    174    175 
## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 
##    176    177    178    179    180    181    182 
## 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 0.8718 
##    183    184    185    186    187    188    189 
## 0.8718 0.8718 0.9405 0.9405 0.9405 0.9405 0.9405 
##    190    191    192    193    194    195    196 
## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 
##    197    198    199    200    201    202    203 
## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 
##    204    205    206    207    208    209    210 
## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 
##    211    212    213    214    215    216    217 
## 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 0.9405 
##    218    219    220 
## 0.9405 0.9405 0.9405</code></pre>
<p>These are now the estimated values for all 220 observations in the original data frame. We now want some estimates for the years before 1950 and after 2010. So we also give the <code>predict()</code> function a data frame with the desired years:</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="logistic-regression.html#cb385-1" tabindex="-1"></a>more_props <span class="ot">&lt;-</span> <span class="fu">predict</span>(lreg, </span>
<span id="cb385-2"><a href="logistic-regression.html#cb385-2" tabindex="-1"></a>                      <span class="fu">data.frame</span>(<span class="at">year =</span> <span class="fu">c</span>(<span class="dv">1910</span>, <span class="dv">1920</span>, <span class="dv">1930</span>, <span class="dv">1940</span>, <span class="dv">2020</span>, <span class="dv">2030</span>)), </span>
<span id="cb385-3"><a href="logistic-regression.html#cb385-3" tabindex="-1"></a>                      <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span>
<span id="cb385-4"><a href="logistic-regression.html#cb385-4" tabindex="-1"></a>more_props</span></code></pre></div>
<pre><code>##       1       2       3       4       5       6 
## 0.01955 0.03871 0.07519 0.14101 0.97842 0.98919</code></pre>
<p>We will now create a data frame containing only the year and proportions, using the original data frame and the values just estimated:</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="logistic-regression.html#cb387-1" tabindex="-1"></a>df_new <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">year =</span> <span class="fu">c</span>(df<span class="sc">$</span>year, <span class="dv">1910</span>, <span class="dv">1920</span>, <span class="dv">1930</span>, <span class="dv">1940</span>, <span class="dv">2020</span>, <span class="dv">2030</span>),</span>
<span id="cb387-2"><a href="logistic-regression.html#cb387-2" tabindex="-1"></a>                     <span class="at">proportions =</span> <span class="fu">c</span>(df<span class="sc">$</span>proportions, more_props))</span>
<span id="cb387-3"><a href="logistic-regression.html#cb387-3" tabindex="-1"></a></span>
<span id="cb387-4"><a href="logistic-regression.html#cb387-4" tabindex="-1"></a><span class="fu">ggplot</span>(df_new) <span class="sc">+</span> </span>
<span id="cb387-5"><a href="logistic-regression.html#cb387-5" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> year, <span class="at">y =</span> proportions) <span class="sc">+</span> </span>
<span id="cb387-6"><a href="logistic-regression.html#cb387-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb387-7"><a href="logistic-regression.html#cb387-7" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="at">se =</span> F,</span>
<span id="cb387-8"><a href="logistic-regression.html#cb387-8" tabindex="-1"></a>              <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">family =</span> <span class="st">&quot;quasibinomial&quot;</span>))</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-159-1.svg" width="672" /></p>
<p>We can also calculate the tipping point for this data, using the <code>coefs</code> already stored above:</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="logistic-regression.html#cb389-1" tabindex="-1"></a><span class="sc">-</span>coefs[<span class="dv">1</span>] <span class="sc">/</span> coefs[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## (Intercept) 
##        1966</code></pre>
<p>According to our model, the year in which the pronunciation of /o/ in the <em>Received Pronunciation</em> changes from “high” to “low” is approximately 1965.</p>
</div>
</div>
<div id="tipping-points-in-perceptual-studies" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Tipping Points in Perceptual Studies<a href="logistic-regression.html#tipping-points-in-perceptual-studies" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tipping points are frequently used in perception tests, which are constructed as follows: We synthesized an 11-step continuum between /pUp/ and /pYp/. Phonetically, the difference between /U/ and /Y/ is the second formant, which is low for /U/ and high for /Y/. We gradually varied this F2 value in the continuum over 11 steps. The first and last tokens in this continuum sound very clearly like PUPP or PÜPP, but in between, it can be difficult for listeners to distinguish between PUPP and PÜPP. Each token from the continuum was then played to several German participants in randomized order, and the participant had to decide whether it was PUPP or PÜPP. We are interested to find out at which F2 value the participants’ perception switches from PUPP to PÜPP. In other words, we are interested in the tipping point.</p>
<p>We have stored data from such a perception experiment in the data frame <code>pvp</code>:</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="logistic-regression.html#cb391-1" tabindex="-1"></a><span class="fu">head</span>(pvp)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##   Vpn      F2 response
##   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;   
## 1 VP18   1239 Y       
## 2 VP18   1088 Y       
## 3 VP18    803 U       
## 4 VP18    956 U       
## 5 VP18   1328 Y       
## 6 VP18    861 U</code></pre>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="logistic-regression.html#cb393-1" tabindex="-1"></a><span class="fu">unique</span>(pvp<span class="sc">$</span>response)</span></code></pre></div>
<pre><code>## [1] &quot;Y&quot; &quot;U&quot;</code></pre>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="logistic-regression.html#cb395-1" tabindex="-1"></a><span class="fu">unique</span>(pvp<span class="sc">$</span>F2)</span></code></pre></div>
<pre><code>##  [1] 1239 1088  803  956 1328  861  989 1121  808 1310
## [11] 1436</code></pre>
<p>We expect that, with increasing F2 levels, subjects will be more likely to hear /Y/ than /U/, so we code the response /Y/ as success and /U/ as failure:</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="logistic-regression.html#cb397-1" tabindex="-1"></a>pvp <span class="sc">%&lt;&gt;%</span> </span>
<span id="cb397-2"><a href="logistic-regression.html#cb397-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">success =</span> <span class="fu">ifelse</span>(response <span class="sc">==</span> <span class="st">&quot;Y&quot;</span>, T, F),</span>
<span id="cb397-3"><a href="logistic-regression.html#cb397-3" tabindex="-1"></a>         <span class="at">failure =</span> <span class="sc">!</span>success)</span>
<span id="cb397-4"><a href="logistic-regression.html#cb397-4" tabindex="-1"></a><span class="fu">head</span>(pvp)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 5
##   Vpn      F2 response success failure
##   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt;  
## 1 VP18   1239 Y        TRUE    FALSE  
## 2 VP18   1088 Y        TRUE    FALSE  
## 3 VP18    803 U        FALSE   TRUE   
## 4 VP18    956 U        FALSE   TRUE   
## 5 VP18   1328 Y        TRUE    FALSE  
## 6 VP18    861 U        FALSE   TRUE</code></pre>
<p>For the steps of the F2 continuum, we calculate <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>:</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="logistic-regression.html#cb399-1" tabindex="-1"></a>df <span class="ot">&lt;-</span> pvp <span class="sc">%&gt;%</span></span>
<span id="cb399-2"><a href="logistic-regression.html#cb399-2" tabindex="-1"></a>  <span class="fu">group_by</span>(F2) <span class="sc">%&gt;%</span></span>
<span id="cb399-3"><a href="logistic-regression.html#cb399-3" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">P =</span> <span class="fu">sum</span>(success), <span class="at">Q =</span> <span class="fu">sum</span>(failure))</span>
<span id="cb399-4"><a href="logistic-regression.html#cb399-4" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>## # A tibble: 11 × 3
##       F2     P     Q
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1   803     0    10
##  2   808     0    10
##  3   861     0    10
##  4   956     0    10
##  5   989     0    10
##  6  1088     2     8
##  7  1121     4     6
##  8  1239     9     1
##  9  1310     9     1
## 10  1328    10     0
## 11  1436    10     0</code></pre>
<p>We then calculate the proportions of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> and plot the sigmoidal regression line in the data:</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="logistic-regression.html#cb401-1" tabindex="-1"></a>df<span class="sc">$</span>proportions <span class="ot">&lt;-</span> df<span class="sc">$</span>P <span class="sc">/</span> (df<span class="sc">$</span>P <span class="sc">+</span> df<span class="sc">$</span>Q)</span>
<span id="cb401-2"><a href="logistic-regression.html#cb401-2" tabindex="-1"></a>df</span></code></pre></div>
<pre><code>## # A tibble: 11 × 4
##       F2     P     Q proportions
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;
##  1   803     0    10         0  
##  2   808     0    10         0  
##  3   861     0    10         0  
##  4   956     0    10         0  
##  5   989     0    10         0  
##  6  1088     2     8         0.2
##  7  1121     4     6         0.4
##  8  1239     9     1         0.9
##  9  1310     9     1         0.9
## 10  1328    10     0         1  
## 11  1436    10     0         1</code></pre>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="logistic-regression.html#cb403-1" tabindex="-1"></a><span class="fu">ggplot</span>(df) <span class="sc">+</span> </span>
<span id="cb403-2"><a href="logistic-regression.html#cb403-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> F2, <span class="at">y =</span> proportions) <span class="sc">+</span> </span>
<span id="cb403-3"><a href="logistic-regression.html#cb403-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span> </span>
<span id="cb403-4"><a href="logistic-regression.html#cb403-4" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;glm&quot;</span>, <span class="at">se =</span> F, </span>
<span id="cb403-5"><a href="logistic-regression.html#cb403-5" tabindex="-1"></a>              <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">family =</span> <span class="st">&quot;quasibinomial&quot;</span>))</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-164-1.svg" width="672" /></p>
<p>To determine the tipping point of this sigmoid curve, we calculate the Generalized Linear Model:</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="logistic-regression.html#cb405-1" tabindex="-1"></a>pvp.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">as.factor</span>(response) <span class="sc">~</span> F2, <span class="at">family =</span> binomial, <span class="at">data =</span> pvp)</span></code></pre></div>
<p>Using the estimated regression coefficients, we can now calculate the subjects’ perceptual tipping point:</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="logistic-regression.html#cb406-1" tabindex="-1"></a>coefs <span class="ot">&lt;-</span> <span class="fu">coef</span>(pvp.glm)</span>
<span id="cb406-2"><a href="logistic-regression.html#cb406-2" tabindex="-1"></a><span class="sc">-</span>coefs[<span class="dv">1</span>] <span class="sc">/</span> coefs[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>## (Intercept) 
##        1151</code></pre>
<p>This means that above an F2 value of approximately 1151 Hz, the test subjects hear “PÜPP” rather than “PUPP”.</p>
<p>Finally, we want to determine whether the test subjects’ judgments were actually influenced by F2. For this, we use the <span class="math inline">\(\chi^2\)</span>-test.</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="logistic-regression.html#cb408-1" tabindex="-1"></a><span class="fu">anova</span>(pvp.glm)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: as.factor(response)
## 
## Terms added sequentially (first to last)
## 
## 
##      Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
## NULL                   109      148.1             
## F2    1      109       108       39.1   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We report: <em>The proportion of pUp/pYp responses was significantly influenced by F2 (<span class="math inline">\(\chi^2\)</span>[1] = 109.0, <span class="math inline">\(p\)</span> &lt; 0.001).</em></p>
</div>
<div id="categorical-independent-factor" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> Categorical Independent Factor<a href="logistic-regression.html#categorical-independent-factor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The logistic regression can be used in a similar way when the independent variable is categorical. The key difference is that no tipping point needs to be calculated and no sigmoid needs to be plotted.</p>
<p>In the data frame <code>sz</code>, we stored information about how 20 participants pronounced the word “Sonne” (sun): either with an initial [s] (voiceless) or an initial [z] (voiced). Of the 20 participants, 9 came from Bavaria and 11 from Schleswig-Holstein:</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="logistic-regression.html#cb410-1" tabindex="-1"></a><span class="fu">head</span>(sz)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##   fricative dialect Vpn  
##   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;
## 1 z         SH      S1   
## 2 z         SH      S2   
## 3 z         SH      S3   
## 4 z         SH      S4   
## 5 s         SH      S5   
## 6 s         SH      S6</code></pre>
<p>Our question is: <em>Is voicing (two levels: s, z) influenced by dialect (two levels: BY, SH)?</em></p>
<p>Since both variables are categorical in this case, we can create a bar plot to get an idea of the data:</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="logistic-regression.html#cb412-1" tabindex="-1"></a><span class="fu">ggplot</span>(sz) <span class="sc">+</span> </span>
<span id="cb412-2"><a href="logistic-regression.html#cb412-2" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">fill =</span> fricative, <span class="at">x =</span> dialect) <span class="sc">+</span> </span>
<span id="cb412-3"><a href="logistic-regression.html#cb412-3" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">position =</span> <span class="st">&quot;fill&quot;</span>)</span></code></pre></div>
<p><img src="statistik_r_files/figure-html/unnamed-chunk-169-1.svg" width="672" /></p>
<p>It appears that the initial fricative is produced significantly more often voiceless in Bavaria than in Schleswig-Holstein. Now, as before, we apply a logistic regression followed by a <span class="math inline">\(\chi^2\)</span>-test to the data:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="logistic-regression.html#cb413-1" tabindex="-1"></a>sz.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">as.factor</span>(fricative) <span class="sc">~</span> dialect, <span class="at">family =</span> binomial, <span class="at">data =</span> sz)</span>
<span id="cb413-2"><a href="logistic-regression.html#cb413-2" tabindex="-1"></a><span class="fu">anova</span>(sz.glm, <span class="at">test =</span> <span class="st">&quot;Chisq&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: as.factor(fricative)
## 
## Terms added sequentially (first to last)
## 
## 
##         Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)  
## NULL                       19       27.7           
## dialect  1      5.3        18       22.4    0.021 *
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(\chi^2\)</span>-test shows: <em>The distribution of voiced and voiceless /s/ in words like Sonne was significantly influenced by dialect (<span class="math inline">\(\chi^2\)</span>[1] = 5.3, <span class="math inline">\(p\)</span> &lt; 0.05).</em></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mixed-linear-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": null,
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": null,
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "none",
    "scroll_highlight": true
  },
  "toolbar": {
    "position": "fixed"
  },
  "info": false
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
